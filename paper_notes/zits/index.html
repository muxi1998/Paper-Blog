<!doctype html>





























<html
  class="not-ready lg:text-base"
  style="--bg: #faf8f1"
  lang="en-us"
>
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, shrink-to-fit=no"
  />

  
  <title>Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding - My Paper note site</title>

  
  <meta name="theme-color" />

  
  
  
  
  <meta name="description" content="Abstract 1. Introduction Inpainting的重點 coherent texture visually reasonable structures 過去的Inpainting方法有4大問題 感受野過小：CNN kernel的關係，就算是dilated CNN也會在在過大的毀損區域或高解析度的情境下降低效能 遺失整體的結構：若模型不了解整體的結構，則很難還原細節 計算量重：訓練GAN對大影像的還原是困難且耗費資源的，且效能會隨著影像解析度提高而降低 遮罩區域沒有位置資訊：模型容易在遺失區域產生偽影 2. Related work 3. Method 簡寫說明 ZITS : ZeroRA based Incremental Transformer Structure MPE : Masking Positional Encoding TSR : Transformer Structure Restorer FTR : Fourier CNN Texture Restoration Overview Structure restoration 輸入為$TSR(I_m, I_e, I_l, M)$ $I_m:$ 被遮罩的輸入影像 $I_e:$ 邊緣資訊 $I_l:$ 線條資訊 $M:$ 二元mask 輸出為草圖域 $[\tilde{I_e}, \tilde{I_l}]=TSR(I_m, I_e, I_l, M)$ Simple Structure upsample 在推論期間會使用SSU來將灰階的草圖上採樣至任意大小 Extract structure feature 在使用一個gated convolution based SFE來提取多尺度的特徵 $S_k=SFE(\tilde{I_e}, \tilde{I_l},M), {k=0,1,2,3}$ Add to to the fourier CNN texture restoration 將$S_k$資訊逐步加入(ZeroRA)相對應的FTR layer中 3." />
  <meta name="author" content="My Paper note site" />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="https://muxi1998.github.io/Paper-Blog/main.min.css" />

  
  
  
  
  
  <link rel="preload" as="image" href="https://muxi1998.github.io/Paper-Blog/theme.png" />

  
  
  
  
  

  
  
  

  
  
  <script
    defer
    src="https://muxi1998.github.io/Paper-Blog/highlight.min.js"
    onload="hljs.initHighlightingOnLoad();"
  ></script>
  

  
  
  

  
  <link rel="icon" href="https://muxi1998.github.io/Paper-Blog/favicon.ico" />
  <link rel="apple-touch-icon" href="https://muxi1998.github.io/Paper-Blog/apple-touch-icon.png" />

  
  <meta name="generator" content="Hugo 0.121.1">

  
  
  
  
  
  <meta itemprop="name" content="Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding">
<meta itemprop="description" content="Abstract 1. Introduction Inpainting的重點 coherent texture visually reasonable structures 過去的Inpainting方法有4大問題 感受野過小：CNN kernel的關係，就算是dilated CNN也會在在過大的毀損區域或高解析度的情境下降低效能 遺失整體的結構：若模型不了解整體的結構，則很難還原細節 計算量重：訓練GAN對大影像的還原是困難且耗費資源的，且效能會隨著影像解析度提高而降低 遮罩區域沒有位置資訊：模型容易在遺失區域產生偽影 2. Related work 3. Method 簡寫說明 ZITS : ZeroRA based Incremental Transformer Structure MPE : Masking Positional Encoding TSR : Transformer Structure Restorer FTR : Fourier CNN Texture Restoration Overview Structure restoration 輸入為$TSR(I_m, I_e, I_l, M)$ $I_m:$ 被遮罩的輸入影像 $I_e:$ 邊緣資訊 $I_l:$ 線條資訊 $M:$ 二元mask 輸出為草圖域 $[\tilde{I_e}, \tilde{I_l}]=TSR(I_m, I_e, I_l, M)$ Simple Structure upsample 在推論期間會使用SSU來將灰階的草圖上採樣至任意大小 Extract structure feature 在使用一個gated convolution based SFE來提取多尺度的特徵 $S_k=SFE(\tilde{I_e}, \tilde{I_l},M), {k=0,1,2,3}$ Add to to the fourier CNN texture restoration 將$S_k$資訊逐步加入(ZeroRA)相對應的FTR layer中 3.">

<meta itemprop="wordCount" content="345">
<meta itemprop="keywords" content="" />
  
  <meta property="og:title" content="Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding" />
<meta property="og:description" content="Abstract 1. Introduction Inpainting的重點 coherent texture visually reasonable structures 過去的Inpainting方法有4大問題 感受野過小：CNN kernel的關係，就算是dilated CNN也會在在過大的毀損區域或高解析度的情境下降低效能 遺失整體的結構：若模型不了解整體的結構，則很難還原細節 計算量重：訓練GAN對大影像的還原是困難且耗費資源的，且效能會隨著影像解析度提高而降低 遮罩區域沒有位置資訊：模型容易在遺失區域產生偽影 2. Related work 3. Method 簡寫說明 ZITS : ZeroRA based Incremental Transformer Structure MPE : Masking Positional Encoding TSR : Transformer Structure Restorer FTR : Fourier CNN Texture Restoration Overview Structure restoration 輸入為$TSR(I_m, I_e, I_l, M)$ $I_m:$ 被遮罩的輸入影像 $I_e:$ 邊緣資訊 $I_l:$ 線條資訊 $M:$ 二元mask 輸出為草圖域 $[\tilde{I_e}, \tilde{I_l}]=TSR(I_m, I_e, I_l, M)$ Simple Structure upsample 在推論期間會使用SSU來將灰階的草圖上採樣至任意大小 Extract structure feature 在使用一個gated convolution based SFE來提取多尺度的特徵 $S_k=SFE(\tilde{I_e}, \tilde{I_l},M), {k=0,1,2,3}$ Add to to the fourier CNN texture restoration 將$S_k$資訊逐步加入(ZeroRA)相對應的FTR layer中 3." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://muxi1998.github.io/Paper-Blog/paper_notes/zits/" /><meta property="article:section" content="paper_notes" />




  
  <meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding"/>
<meta name="twitter:description" content="Abstract 1. Introduction Inpainting的重點 coherent texture visually reasonable structures 過去的Inpainting方法有4大問題 感受野過小：CNN kernel的關係，就算是dilated CNN也會在在過大的毀損區域或高解析度的情境下降低效能 遺失整體的結構：若模型不了解整體的結構，則很難還原細節 計算量重：訓練GAN對大影像的還原是困難且耗費資源的，且效能會隨著影像解析度提高而降低 遮罩區域沒有位置資訊：模型容易在遺失區域產生偽影 2. Related work 3. Method 簡寫說明 ZITS : ZeroRA based Incremental Transformer Structure MPE : Masking Positional Encoding TSR : Transformer Structure Restorer FTR : Fourier CNN Texture Restoration Overview Structure restoration 輸入為$TSR(I_m, I_e, I_l, M)$ $I_m:$ 被遮罩的輸入影像 $I_e:$ 邊緣資訊 $I_l:$ 線條資訊 $M:$ 二元mask 輸出為草圖域 $[\tilde{I_e}, \tilde{I_l}]=TSR(I_m, I_e, I_l, M)$ Simple Structure upsample 在推論期間會使用SSU來將灰階的草圖上採樣至任意大小 Extract structure feature 在使用一個gated convolution based SFE來提取多尺度的特徵 $S_k=SFE(\tilde{I_e}, \tilde{I_l},M), {k=0,1,2,3}$ Add to to the fourier CNN texture restoration 將$S_k$資訊逐步加入(ZeroRA)相對應的FTR layer中 3."/>

  
  
  
  <link rel="canonical" href="https://muxi1998.github.io/Paper-Blog/paper_notes/zits/" />
  
  
</head>

  <body class="text-black duration-200 ease-out dark:text-white">
    <header class="mx-auto flex h-[4.5rem] max-w-3xl px-8 lg:justify-center">
  <div class="relative z-50 mr-auto flex items-center">
    <a
      class="-translate-x-[1px] -translate-y-[1px] text-2xl font-semibold"
      href="https://muxi1998.github.io/Paper-Blog/"
      >My Paper note site</a
    >
    <div
      class="btn-dark text-[0] ml-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.png)_left_center/_auto_theme('spacing.6')_no-repeat] [transition:_background-position_0.4s_steps(5)] dark:[background-position:right]"
      role="button"
      aria-label="Dark"
    ></div>
  </div>

  <div
    class="btn-menu relative z-50 -mr-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden"
    role="button"
    aria-label="Menu"
  ></div>

  

  <script>
    
    const htmlClass = document.documentElement.classList;
    setTimeout(() => {
      htmlClass.remove('not-ready');
    }, 10);

    
    const btnMenu = document.querySelector('.btn-menu');
    btnMenu.addEventListener('click', () => {
      htmlClass.toggle('open');
    });

    
    const metaTheme = document.querySelector('meta[name="theme-color"]');
    const lightBg = '#faf8f1'.replace(/"/g, '');
    const setDark = (isDark) => {
      metaTheme.setAttribute('content', isDark ? '#000' : lightBg);
      htmlClass[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark);
    };

    
    const darkScheme = window.matchMedia('(prefers-color-scheme: dark)');
    if (htmlClass.contains('dark')) {
      setDark(true);
    } else {
      const darkVal = localStorage.getItem('dark');
      setDark(darkVal ? darkVal === 'true' : darkScheme.matches);
    }

    
    darkScheme.addEventListener('change', (event) => {
      setDark(event.matches);
    });

    
    const btnDark = document.querySelector('.btn-dark');
    btnDark.addEventListener('click', () => {
      setDark(localStorage.getItem('dark') !== 'true');
    });
  </script>

  <div
    class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"
  >
    
    

    
  </div>
</header>


    <main
      class="prose prose-neutral relative mx-auto min-h-[calc(100%-9rem)] max-w-3xl px-8 pb-16 pt-12 dark:prose-invert"
    >
      

<article>
  <header class="mb-16">
    <h1 class="!my-0 pb-2.5">Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding</h1>

    
    <div class="text-sm antialiased opacity-60">
      
      
      
      
    </div>
    
  </header>

  <section><h1 id="abstract">Abstract</h1>
<h1 id="1-introduction">1. Introduction</h1>
<ul>
<li>Inpainting的重點
<ul>
<li>coherent texture</li>
<li>visually reasonable structures</li>
</ul>
</li>
<li>過去的Inpainting方法有4大問題
<ol>
<li>感受野過小：CNN kernel的關係，就算是dilated CNN也會在在過大的毀損區域或高解析度的情境下降低效能</li>
<li>遺失整體的結構：若模型不了解整體的結構，則很難還原細節</li>
<li>計算量重：訓練GAN對大影像的還原是困難且耗費資源的，且效能會隨著影像解析度提高而降低</li>
<li>遮罩區域沒有位置資訊：模型容易在遺失區域產生偽影</li>
</ol>
</li>
</ul>
<h1 id="2-related-work">2. Related work</h1>
<h1 id="3-method">3. Method</h1>
<p><img src="./ZITS/2022-08-22_3.32.31.png" alt="截圖 2022-08-22 下午3.32.31.png"></p>
<h3 id="簡寫說明">簡寫說明</h3>
<ul>
<li><strong>ZITS :</strong> ZeroRA based Incremental Transformer Structure</li>
<li><strong>MPE :</strong> Masking Positional Encoding</li>
<li><strong>TSR :</strong> Transformer Structure Restorer</li>
<li><strong>FTR :</strong> Fourier CNN Texture Restoration</li>
</ul>
<h3 id="overview">Overview</h3>
<ol>
<li>Structure restoration
<ol>
<li>輸入為$TSR(I_m, I_e, I_l, M)$
<ol>
<li>$I_m:$ 被遮罩的輸入影像</li>
<li>$I_e:$ 邊緣資訊</li>
<li>$I_l:$ 線條資訊</li>
<li>$M:$ 二元mask</li>
</ol>
</li>
<li>輸出為草圖域 $[\tilde{I_e}, \tilde{I_l}]=TSR(I_m, I_e, I_l, M)$</li>
</ol>
</li>
<li>Simple Structure upsample
<ol>
<li>在推論期間會使用SSU來將灰階的草圖上採樣至任意大小</li>
</ol>
</li>
<li>Extract structure feature
<ol>
<li>在使用一個gated convolution based SFE來提取多尺度的特徵
$S_k=SFE(\tilde{I_e}, \tilde{I_l},M), {k=0,1,2,3}$</li>
</ol>
</li>
<li>Add to to the fourier CNN texture restoration
<ol>
<li>將$S_k$資訊逐步加入(ZeroRA)相對應的FTR layer中</li>
</ol>
</li>
</ol>
<h2 id="31-transformer-structure-restoration">3.1 Transformer Structure Restoration</h2>
<ul>
<li>transformer具有回復全局資訊的能力</li>
<li>除了使用原始的attention module來提取全域相關性外，更提除了簡化計算的RPE based axial attention module</li>
</ul>
<h3 id="使用步驟">使用步驟</h3>
<ol>
<li>
<p>將原始大小為$256\times256的$輸入：遮罩後 影像$I_m$、邊線影像 $I_e$、線條影像$I_l$和遮罩$M$，透過三個卷積來downsampling</p>
</li>
<li>
<p>在得出的特徵上疊加上絕對位置的position embedding，$X\in \mathbb{R}^{h\times w\times c}$，$h,w=32$ and $c=256$</p>
</li>
<li>
<p>使用axial attention來取代原始attention的龐大計算，並且在每個axial attention 模組中也注入RPE(relative position encoding)來加強表示空間中的關係</p>
<p><img src="./ZITS/2022-08-31_9.22.19.png" alt="截圖 2022-08-31 下午9.22.19.png"></p>
<p>$W_{rq}, W_{rk}, W_{cq}, W_{ck}$是可訓練的參數，為attention之用的query和key
$R^{row}_{i,j}$是row $i$和$j$的RPE值</p>
</li>
<li>
<p>經上述一連串的attention計算後，在使用三個轉置卷積來回復成原始大小$256\times256$</p>
</li>
</ol>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><img src="./ZITS/2022-08-31_9.39.32.png" alt="截圖 2022-08-31 下午9.39.32.png"></p>
<h2 id="32-simple-structure-upsampler">3.2 Simple Structure Upsampler</h2>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><img src="./ZITS/2022-08-31_9.39.45.png" alt="截圖 2022-08-31 下午9.39.45.png"></p>
<ul>
<li>原始的插值法upsampling會產生鋸齒狀的問題，圖(f)-(i)</li>
<li>幸運的是草圖的上採樣可以透過學習的方法來達成
<ul>
<li>原本使用邊緣草圖和線條草圖一起訓練此upsampling模型，但發現邊緣草圖在不同的解析度下有不一樣的呈現方法，因此存在歧異，導致訓練出來的模型無法正確的upsample邊緣圖（如圖j）</li>
<li>但線條圖的特性相對清楚且在任意解析度下的樣子都一樣，因此作者發現只拿線條草圖來訓練upsample模型的效果較好，放大的草圖如圖(k)</li>
</ul>
</li>
</ul>
<h2 id="33-zerora-structure-enhanced-inpainting">3.3 ZeroRA Structure Enhanced Inpainting</h2>
<p><img src="./ZITS/2022-08-31_9.48.13.png" alt="截圖 2022-08-31 下午9.48.13.png"></p>
<h3 id="fourier-cnn-texture-restoration-ftr">Fourier CNN Texture Restoration (FTR)</h3>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<ul>
<li>FTR的關鍵在於FFC層（快速傅立葉卷積層），有兩個分支組成
<ol>
<li>局部分之使用常態卷積</li>
<li>全局分支在快速傅立葉變換後對特徵進行卷積</li>
</ol>
</li>
<li>在修復的過程中將兩個分支的資訊組合起來獲得更大的<strong>感受野</strong>與<strong>局部不變性</strong></li>
<li>此強大的方法雖然對恢復紋理很有用，但對於在<strong>重建整體結構是有弱勢的</strong>，因此此篇論文提出一系列新穎的組件來改善它</li>
</ul>
<p><a href="https://zhuanlan.zhihu.com/p/358187931">【multi-scale系列】频域卷积 Fast Fourier Convolution（NeurIPS 2020）</a></p>
<h3 id="structure-feature-encoder-sfe">Structure Feature Encoder (SFE)</h3>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<ul>
<li>需要FCN (Full convolution network)</li>
<li>此論問中的SFE是一個自動編碼器模型
<ul>
<li>三層的downsampling卷積→encoder</li>
<li>三層的residual blocks with dilated 卷積 → middle</li>
<li>三層的upsampling 卷積→decoder</li>
</ul>
</li>
<li>SFE中的encoder和decoder都是gated convolutions(GCs)
<ul>
<li>gated convolution通常是用來讓模型著重在重點區域的特徵，適用不規則形狀的遮罩</li>
<li>gated convolution的概念是另外在學一個sigmoid特徵，此特徵有點像weighting的概念，可以表達哪一區域的特徵較為重要並且保留</li>
</ul>
</li>
<li>在這部分SFE結束後會得到特徵空間下的邊緣圖和線條圖資訊，且已經經過過濾，只保留重點區域的資訊，因為灰階的草圖資訊很疏散，再送至FTR</li>
<li>最後會得到四個coarse-to-fine的特徵圖$S_k, k={0,1,2,3}$
<ul>
<li>最後一層middle加上三層decoder→此四張特徵圖代表轉換後的結構性特徵</li>
<li>$S_0, S_1, S_2, S_3=SFE(\tilde{I_e}, \tilde{I_l},M)$</li>
<li>$M$ 為resized的binary mask</li>
</ul>
</li>
</ul>
<h3 id="masking-positional-encoding-mpe">Masking Positional Encoding (MPE)</h3>
<ul>
<li>
<p><strong>為什麼需要masking positional encoding?</strong></p>
<ul>
<li>在CNN運算中，padding可以提中一些位置資訊，但只包含了spatial anchors，<strong>大部分只能識別角落或邊界的資訊</strong></li>
<li>靠近邊界的部分擁有較多的位置資訊，因此GAN在生成的時候能夠有比較明確的目標，反之靠近中心點的部分則容易產生偽影
→ GAN在資訊(specific position encoding)不足的區域容易產生重複性無意義的artifacts</li>
<li>在修復過程中，未屏蔽的區域不需要位置訊息，甚至可以說不用管他，因為模型永遠會知道為屏蔽區域的標準答案，但<strong>被屏蔽區域的位置資訊就很重要了</strong></li>
<li>受限於CNN的感受野，當mask過大時模型可能會失去方向和位置信息，因此如果有masking的positional encoding資訊輔助的話，就可能可以減少無意義的偽影產生</li>
<li>雖然FFC可以將特徵學習轉移到頻域上，無法明確區分屏蔽區域和未屏蔽區域，因此此篇論文在FFC上加上MPE的新模組來輔助提供遮蔽區域的位置資訊</li>
</ul>
</li>
<li>
<p><strong>MPE的運作邏輯</strong></p>
<ul>
<li>$P$代表masked and unmasked positional relations
<ul>
<li>$P_{dis}$ 代表masking distance</li>
<li>$P_{dir}$ 代表masking directions</li>
</ul>
</li>
<li>步驟
<ol>
<li>先給定一個inverse的$256\times256$的遮罩
<ol>
<li>1→為遮罩區域</li>
<li>0→屏蔽區域</li>
</ol>
</li>
<li>使用一個$3\times3$的all-one核心還計算各個位置的masking的距離</li>
<li>使用sinusoidal positional encoding來得到$P_{dis}\in\mathbb{R}^{256\times256\times d}$
此$d$ 對應到FTR的第一個卷積的深度(channel size)，因此FTR在運算時每個channel會再多一項$P_{dis}$的masking位置資訊(包含距離與方向)
<ul>
<li>此時的$P_{dis}$是絕對位置資訊，但可透過nearest interpolation到多樣的尺度來幫助訓練不同解析度下的相對位置關係</li>
</ul>
</li>
<li>在產生masking directions時，使用4個不同的二元核心來得到4-channel的one-hot vector $D_{dir}\in \mathbb{R}^{256\times256\times4}$
<ol>
<li>$D_{dir}$的值取決於<strong>哪一個kernel先覆蓋到</strong>masked region</li>
<li>masking direction是個multi-label vector，因為一個pixel可能有不只一個最短的direction，因此$D_{dir}$會在投影到一個 d 維度的特徵空間，其特徵空間的維度是一個可學習的變數
$W_{dir}\in \mathbb{R}^{4\times d}$，投影後的最終方向特徵為
$P_{dir}=D_{dir}\times W_{dir}\in\mathbb{R}^{256\times256\times d}$</li>
</ol>
</li>
<li>最後在MPE得到的$P_{dis}$和$P_{dir}$會注入至FTR的第一層</li>
</ol>
</li>
</ul>
</li>
</ul>
<p><img src="./ZITS/2022-09-01_1.06.12.png" alt="截圖 2022-09-01 上午1.06.12.png"></p>
<h3 id="zero-initialized-residual-addition-zerora">Zero-initialized Residual Addition (ZeroRA)</h3>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<ul>
<li>
<p><strong>動機</strong></p>
<ul>
<li>增量訓練（增加更多輔助訊息）可以靈活的改善圖像修復的成效，而在此論文中希望將整體結構的資訊視為一種輔助資訊，來進一步改善預訓練的修復模型</li>
<li>提出ZeroRA來取代transformer中的layer normalization計算</li>
</ul>
</li>
<li>
<p><strong>ZeroRA的概念</strong></p>
<ul>
<li>給定一個輸入特徵$x$，輸出原始輸入加上一個加權後的skip connection計算為$x&rsquo;$
$x&rsquo; =x+\alpha\ \cdot\ F(x)$</li>
<li>當訓練前期$\alpha$預設為0時，輸入和輸出的關係為相同，如此<strong>可以讓訓練變得穩定</strong></li>
</ul>
</li>
<li>
<p><strong>在此篇論文的應用</strong></p>
<ul>
<li>
<p>使用ZeroRA來漸進式的添加結構資訊進FTR中</p>
</li>
<li>
<p>4個zero-initialized $\alpha_k,k\in{0,1,2,3}$是用來fuse 4個相對應的feature maps $S_k$(來自SFE)</p>
<p><img src="./ZITS/2022-09-01_2.12.20.png" alt="截圖 2022-09-01 上午2.12.20.png"></p>
</li>
</ul>
</li>
<li>
<p><strong>ZeroRA的優點</strong></p>
<ul>
<li>在finetune時可以促使 model的輸出與pre-train model的輸出一致，可確保訓練時的穩定性</li>
</ul>
</li>
</ul>
<h2 id="34-loss-functions">3.4 Loss Functions</h2>
<p><img src="./ZITS/2022-09-01_2.28.53.png" alt="截圖 2022-09-01 上午2.28.53.png"></p>
<ul>
<li>
<p><strong>L1 Loss</strong></p>
<ul>
<li>只計算未遮蔽區域 → 確保有答案的地方不能壞掉，必須一模一樣</li>
<li>$L_{L1}=(1-M)\odot|\hat{I}-\tilde{I}|_1$</li>
</ul>
</li>
<li>
<p><strong>adversarial loss</strong></p>
<ul>
<li>
<p>包含discriminator loss $L_D$和generator loss $L_G$</p>
<p><img src="./ZITS/2022-09-01_2.21.14.png" alt="截圖 2022-09-01 上午2.21.14.png"></p>
<p>$L_{GP}$為gradient penalty用來保持GAN訓練中的穩定</p>
</li>
<li>
<p>僅將遮蔽區域的特徵視為假樣本</p>
</li>
<li>
<p>Discriminator: PatchGAN的discriminator</p>
<p><img src="./ZITS/2022-09-01_2.20.26.png" alt="截圖 2022-09-01 上午2.20.26.png"></p>
</li>
<li>
<p>Generator: FTR+SFE</p>
<p><img src="./ZITS/2022-09-01_2.20.40.png" alt="截圖 2022-09-01 上午2.20.40.png"></p>
</li>
</ul>
</li>
<li>
<p><strong>feature match loss</strong></p>
<ul>
<li>基於真假樣本間的discriminator feature的L1 loss</li>
<li>也是用來穩定GAN的訓練</li>
</ul>
</li>
<li>
<p><strong>high receptive field(HRF) perceptual loss</strong></p>
<p><img src="./ZITS/2022-09-01_2.25.22.png" alt="截圖 2022-09-01 上午2.25.22.png"></p>
<ul>
<li>在<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9707077">論文44</a>中提到相對於perceptual loss，HRP loss更能提升inpainting model的品質</li>
</ul>
</li>
</ul>
<h1 id="4-experiments">4. Experiments</h1>
<h2 id="41-datasets">4.1 Datasets</h2>
<h2 id="42-implementation-details">4.2 Implementation Details</h2>
<h2 id="43-comparison-methods">4.3 Comparison Methods</h2>
<h2 id="44-quantitative-comparisons">4.4 Quantitative Comparisons</h2>
<h2 id="45-qualitative-comparisons">4.5 Qualitative Comparisons</h2>
<h2 id="46-ablation-studies">4.6 Ablation Studies</h2>
<h2 id="47-results-of-high-resolusion-inpainting">4.7 Results of High-Resolusion Inpainting</h2>
<h1 id="5-conclusion">5. Conclusion</h1>
</section>

  
  

  
  
  
  
  <nav class="mt-24 flex rounded-lg bg-black/[3%] text-lg dark:bg-white/[8%]">
    
    <a
      class="flex w-1/2 items-center rounded-l-md p-6 pr-3 font-semibold no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]"
      href="https://muxi1998.github.io/Paper-Blog/paper_notes/temporal_group_fusion_network_dvi/"
      ><span class="mr-1.5">←</span><span></span></a
    >
    
    
  </nav>
  
  

  
  

  
  

  

  
</article>


    </main>

    <footer
  class="opaco mx-auto flex h-[4.5rem] max-w-3xl items-center px-8 text-[0.9em] opacity-60"
>
  <div class="mr-auto">
    &copy; 2024
    <a class="link" href="https://muxi1998.github.io/Paper-Blog/">My Paper note site</a>
  </div>
  <a class="link mx-6" href="https://gohugo.io/" rel="noopener" target="_blank"
    >Powered by Hugo️️</a
  >️
  <a
    class="link"
    href="https://github.com/nanxiaobei/hugo-paper"
    rel="noopener"
    target="_blank"
    >✎ Paper</a
  >
</footer>

  </body>
</html>
