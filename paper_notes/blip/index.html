<!doctype html>





























<html
  class="not-ready lg:text-base"
  style="--bg: #faf8f1"
  lang="en-us"
>
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, shrink-to-fit=no"
  />

  
  <title>BLIP - My Paper note site</title>

  
  <meta name="theme-color" />

  
  
  
  
  <meta name="description" content="Abstract ç›®å‰çš„VLPæ¨¡å‹æ€§èƒ½æå‡çš„æ–¹å¼ä¸»è¦æ˜¯é æ“´å¤§è³‡æ–™é›†â‡’ ç›®å‰æ˜¯ä»¥ç¶²è·¯ä¸Šçš„image-text pair è¨“ç·´ ç¶²è·¯ä¸Šçš„dataå¾ˆå¤§çš„å¯èƒ½å­˜åœ¨noise æ­¤è«–æ–‡æå‡ºä¸€å€‹æ¨™é¡Œç”¢ç”Ÿå™¨å’Œå»å™ªå™¨ä¾†è™•ç†ç¶²è·¯è³‡æ–™é›†æ‰€å­˜åœ¨çš„å™ªéŸ³å•é¡Œ 1. Introduction ğŸ¯ ç›®æ¨™ï¼š æå‡ºä¸€å€‹æ›´å¼·å¤§çš„VLPæ¶æ§‹
ğŸ•°ï¸ éå»æ–¹æ³•
åœ¨éå»çš„VLP(Vision -anguage Pretraining) æ–¹æ³•ä¸­æœ‰å…©å¤§å±¤é¢çš„é™åˆ¶ æ¨¡å‹å±¤é¢ é‡å°ä¸åŒçš„ä¸‹æ¸¸æ‡‰ç”¨ç›®å‰é‚„æ˜¯æœ‰å„è‡ªé©åˆçš„æ¶æ§‹ï¼Œé‚„æ²’æœ‰ä¸€å€‹èƒ½å®Œå…¨çµ±ä¸€å€‹çš„æ¨¡å‹ ç”Ÿæˆä»»å‹™ï¼ˆe.g. æ–‡å­—ç”Ÿæˆï¼‰â‡’ Encoder-Decoder ç†è§£ä»»å‹™ï¼ˆe.g. æª¢ç´¢ï¼‰â‡’ Encoder è³‡æ–™å±¤é¢ éå»æ–¹æ³•çš„è¨“ç·´è³‡æ–™å¤šä¾†è‡ªç¶²è·¯çˆ¬èŸ²æ‰€å¾—ï¼Œå­˜åœ¨noiseè³‡æ–™æœªè¢«æ¸…ç†ä¹¾æ·¨ï¼Œä¸”noiseå¸¶ä¾†çš„è² é¢å½±éŸ¿å°šæœªè¢«é©ç•¶è§£æ±º ğŸ’¡ æœ¬ç¯‡æ–¹æ³•
é‡å°ä¸Šè¿°æåˆ°çš„å…©å¤§å±¤é¢å•é¡Œé€²è¡Œç ”ç©¶ æ¨¡å‹å±¤é¢ æå‡ºä¸€å€‹å¤šæ¨¡æ…‹æ··åˆï¼ˆMultimodal mixtureï¼‰çš„Encoder-Decoderæ¶æ§‹ (MED) å¯ä»¥åœ¨å¾ŒçºŒæ‡‰ç”¨åœ¨æ›´å¤šçš„ä¸‹æ¸¸ä»»å‹™ä¸­ ä¿æŒé è¨“ç·´æ™‚çš„æ•ˆç‡ è³‡æ–™å±¤é¢ æå‡ºä¸€å€‹å¼•å°ï¼ˆBootstrappingï¼‰æ–¹æ³•ä¾†é¿å…noisy image-text pair Finetuneä¸€å€‹pre-trained MEDæˆå…©å€‹å­æ¨¡çµ„ Cap (Captioner) â‡’ ç”Ÿæˆåˆæˆå­—å¹• Filt (Filter)â‡’ éæ¿¾æ‰noisyå­—å¹• ğŸ”¥ ç ”ç©¶æˆæœ
å¼•å°å­—å¹•å¯ä»¥æå‡ä¸‹æ¸¸ä»»å‹™çš„æ•ˆèƒ½ï¼Œä¸”å­—å¹•å¤šæ¨£æ€§è¶Šé«˜è¶Šå¥½ BLIPä¸åƒ…åœ¨Vision-language tasksä¸­æœ‰SOTAçš„æ•ˆèƒ½ï¼Œåœ¨è½‰ç§»è‡³Video-language tasksä¸­ä¹Ÿé”åˆ°çš„SOTAä¸”zero-shotçš„æ•ˆèƒ½ 2. Related Work 2.1 Vision-language Pre-training éå»æ–¹æ³•çš„datasetä¾†æºå¤šæ˜¯ç¶²è·¯çˆ¬èŸ²ï¼Œå› æ­¤å­˜åœ¨å™ªéŸ³(noisy)å•é¡Œï¼Œä¸”å™ªéŸ³å•é¡Œè¢«æ¨¡å‹å¸¶ä¾†çš„æ•ˆæœæ©è”½ â‡’ å› æ­¤æå‡ºCapFilt ä¸åŒæ€§è³ªçš„ä»»å‹™èƒŒå¾Œçš„backboneæœƒä¸åŒ â‡’ æå‡ºå¤šæ¨¡æ…‹æ··åˆencoder-decoder understanding-base tasks â‡’ encoder generation-base tasks â‡’ encoder-decoder 2." />
  <meta name="author" content="My Paper note site" />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="https://muxi1998.github.io/Paper-Blog/main.min.css" />

  
  
  
  
  
  <link rel="preload" as="image" href="https://muxi1998.github.io/Paper-Blog/theme.png" />

  
  
  
  
  

  
  
  

  
  
  <script
    defer
    src="https://muxi1998.github.io/Paper-Blog/highlight.min.js"
    onload="hljs.initHighlightingOnLoad();"
  ></script>
  

  
  
  

  
  <link rel="icon" href="https://muxi1998.github.io/Paper-Blog/favicon.ico" />
  <link rel="apple-touch-icon" href="https://muxi1998.github.io/Paper-Blog/apple-touch-icon.png" />

  
  <meta name="generator" content="Hugo 0.121.1">

  
  
  
  
  
  <meta itemprop="name" content="BLIP">
<meta itemprop="description" content="Abstract ç›®å‰çš„VLPæ¨¡å‹æ€§èƒ½æå‡çš„æ–¹å¼ä¸»è¦æ˜¯é æ“´å¤§è³‡æ–™é›†â‡’ ç›®å‰æ˜¯ä»¥ç¶²è·¯ä¸Šçš„image-text pair è¨“ç·´ ç¶²è·¯ä¸Šçš„dataå¾ˆå¤§çš„å¯èƒ½å­˜åœ¨noise æ­¤è«–æ–‡æå‡ºä¸€å€‹æ¨™é¡Œç”¢ç”Ÿå™¨å’Œå»å™ªå™¨ä¾†è™•ç†ç¶²è·¯è³‡æ–™é›†æ‰€å­˜åœ¨çš„å™ªéŸ³å•é¡Œ 1. Introduction ğŸ¯ ç›®æ¨™ï¼š æå‡ºä¸€å€‹æ›´å¼·å¤§çš„VLPæ¶æ§‹
ğŸ•°ï¸ éå»æ–¹æ³•
åœ¨éå»çš„VLP(Vision -anguage Pretraining) æ–¹æ³•ä¸­æœ‰å…©å¤§å±¤é¢çš„é™åˆ¶ æ¨¡å‹å±¤é¢ é‡å°ä¸åŒçš„ä¸‹æ¸¸æ‡‰ç”¨ç›®å‰é‚„æ˜¯æœ‰å„è‡ªé©åˆçš„æ¶æ§‹ï¼Œé‚„æ²’æœ‰ä¸€å€‹èƒ½å®Œå…¨çµ±ä¸€å€‹çš„æ¨¡å‹ ç”Ÿæˆä»»å‹™ï¼ˆe.g. æ–‡å­—ç”Ÿæˆï¼‰â‡’ Encoder-Decoder ç†è§£ä»»å‹™ï¼ˆe.g. æª¢ç´¢ï¼‰â‡’ Encoder è³‡æ–™å±¤é¢ éå»æ–¹æ³•çš„è¨“ç·´è³‡æ–™å¤šä¾†è‡ªç¶²è·¯çˆ¬èŸ²æ‰€å¾—ï¼Œå­˜åœ¨noiseè³‡æ–™æœªè¢«æ¸…ç†ä¹¾æ·¨ï¼Œä¸”noiseå¸¶ä¾†çš„è² é¢å½±éŸ¿å°šæœªè¢«é©ç•¶è§£æ±º ğŸ’¡ æœ¬ç¯‡æ–¹æ³•
é‡å°ä¸Šè¿°æåˆ°çš„å…©å¤§å±¤é¢å•é¡Œé€²è¡Œç ”ç©¶ æ¨¡å‹å±¤é¢ æå‡ºä¸€å€‹å¤šæ¨¡æ…‹æ··åˆï¼ˆMultimodal mixtureï¼‰çš„Encoder-Decoderæ¶æ§‹ (MED) å¯ä»¥åœ¨å¾ŒçºŒæ‡‰ç”¨åœ¨æ›´å¤šçš„ä¸‹æ¸¸ä»»å‹™ä¸­ ä¿æŒé è¨“ç·´æ™‚çš„æ•ˆç‡ è³‡æ–™å±¤é¢ æå‡ºä¸€å€‹å¼•å°ï¼ˆBootstrappingï¼‰æ–¹æ³•ä¾†é¿å…noisy image-text pair Finetuneä¸€å€‹pre-trained MEDæˆå…©å€‹å­æ¨¡çµ„ Cap (Captioner) â‡’ ç”Ÿæˆåˆæˆå­—å¹• Filt (Filter)â‡’ éæ¿¾æ‰noisyå­—å¹• ğŸ”¥ ç ”ç©¶æˆæœ
å¼•å°å­—å¹•å¯ä»¥æå‡ä¸‹æ¸¸ä»»å‹™çš„æ•ˆèƒ½ï¼Œä¸”å­—å¹•å¤šæ¨£æ€§è¶Šé«˜è¶Šå¥½ BLIPä¸åƒ…åœ¨Vision-language tasksä¸­æœ‰SOTAçš„æ•ˆèƒ½ï¼Œåœ¨è½‰ç§»è‡³Video-language tasksä¸­ä¹Ÿé”åˆ°çš„SOTAä¸”zero-shotçš„æ•ˆèƒ½ 2. Related Work 2.1 Vision-language Pre-training éå»æ–¹æ³•çš„datasetä¾†æºå¤šæ˜¯ç¶²è·¯çˆ¬èŸ²ï¼Œå› æ­¤å­˜åœ¨å™ªéŸ³(noisy)å•é¡Œï¼Œä¸”å™ªéŸ³å•é¡Œè¢«æ¨¡å‹å¸¶ä¾†çš„æ•ˆæœæ©è”½ â‡’ å› æ­¤æå‡ºCapFilt ä¸åŒæ€§è³ªçš„ä»»å‹™èƒŒå¾Œçš„backboneæœƒä¸åŒ â‡’ æå‡ºå¤šæ¨¡æ…‹æ··åˆencoder-decoder understanding-base tasks â‡’ encoder generation-base tasks â‡’ encoder-decoder 2."><meta itemprop="datePublished" content="2024-01-06T00:00:00+00:00" />
<meta itemprop="dateModified" content="2024-01-06T00:00:00+00:00" />
<meta itemprop="wordCount" content="524">
<meta itemprop="keywords" content="" />
  
  <meta property="og:title" content="BLIP" />
<meta property="og:description" content="Abstract ç›®å‰çš„VLPæ¨¡å‹æ€§èƒ½æå‡çš„æ–¹å¼ä¸»è¦æ˜¯é æ“´å¤§è³‡æ–™é›†â‡’ ç›®å‰æ˜¯ä»¥ç¶²è·¯ä¸Šçš„image-text pair è¨“ç·´ ç¶²è·¯ä¸Šçš„dataå¾ˆå¤§çš„å¯èƒ½å­˜åœ¨noise æ­¤è«–æ–‡æå‡ºä¸€å€‹æ¨™é¡Œç”¢ç”Ÿå™¨å’Œå»å™ªå™¨ä¾†è™•ç†ç¶²è·¯è³‡æ–™é›†æ‰€å­˜åœ¨çš„å™ªéŸ³å•é¡Œ 1. Introduction ğŸ¯ ç›®æ¨™ï¼š æå‡ºä¸€å€‹æ›´å¼·å¤§çš„VLPæ¶æ§‹
ğŸ•°ï¸ éå»æ–¹æ³•
åœ¨éå»çš„VLP(Vision -anguage Pretraining) æ–¹æ³•ä¸­æœ‰å…©å¤§å±¤é¢çš„é™åˆ¶ æ¨¡å‹å±¤é¢ é‡å°ä¸åŒçš„ä¸‹æ¸¸æ‡‰ç”¨ç›®å‰é‚„æ˜¯æœ‰å„è‡ªé©åˆçš„æ¶æ§‹ï¼Œé‚„æ²’æœ‰ä¸€å€‹èƒ½å®Œå…¨çµ±ä¸€å€‹çš„æ¨¡å‹ ç”Ÿæˆä»»å‹™ï¼ˆe.g. æ–‡å­—ç”Ÿæˆï¼‰â‡’ Encoder-Decoder ç†è§£ä»»å‹™ï¼ˆe.g. æª¢ç´¢ï¼‰â‡’ Encoder è³‡æ–™å±¤é¢ éå»æ–¹æ³•çš„è¨“ç·´è³‡æ–™å¤šä¾†è‡ªç¶²è·¯çˆ¬èŸ²æ‰€å¾—ï¼Œå­˜åœ¨noiseè³‡æ–™æœªè¢«æ¸…ç†ä¹¾æ·¨ï¼Œä¸”noiseå¸¶ä¾†çš„è² é¢å½±éŸ¿å°šæœªè¢«é©ç•¶è§£æ±º ğŸ’¡ æœ¬ç¯‡æ–¹æ³•
é‡å°ä¸Šè¿°æåˆ°çš„å…©å¤§å±¤é¢å•é¡Œé€²è¡Œç ”ç©¶ æ¨¡å‹å±¤é¢ æå‡ºä¸€å€‹å¤šæ¨¡æ…‹æ··åˆï¼ˆMultimodal mixtureï¼‰çš„Encoder-Decoderæ¶æ§‹ (MED) å¯ä»¥åœ¨å¾ŒçºŒæ‡‰ç”¨åœ¨æ›´å¤šçš„ä¸‹æ¸¸ä»»å‹™ä¸­ ä¿æŒé è¨“ç·´æ™‚çš„æ•ˆç‡ è³‡æ–™å±¤é¢ æå‡ºä¸€å€‹å¼•å°ï¼ˆBootstrappingï¼‰æ–¹æ³•ä¾†é¿å…noisy image-text pair Finetuneä¸€å€‹pre-trained MEDæˆå…©å€‹å­æ¨¡çµ„ Cap (Captioner) â‡’ ç”Ÿæˆåˆæˆå­—å¹• Filt (Filter)â‡’ éæ¿¾æ‰noisyå­—å¹• ğŸ”¥ ç ”ç©¶æˆæœ
å¼•å°å­—å¹•å¯ä»¥æå‡ä¸‹æ¸¸ä»»å‹™çš„æ•ˆèƒ½ï¼Œä¸”å­—å¹•å¤šæ¨£æ€§è¶Šé«˜è¶Šå¥½ BLIPä¸åƒ…åœ¨Vision-language tasksä¸­æœ‰SOTAçš„æ•ˆèƒ½ï¼Œåœ¨è½‰ç§»è‡³Video-language tasksä¸­ä¹Ÿé”åˆ°çš„SOTAä¸”zero-shotçš„æ•ˆèƒ½ 2. Related Work 2.1 Vision-language Pre-training éå»æ–¹æ³•çš„datasetä¾†æºå¤šæ˜¯ç¶²è·¯çˆ¬èŸ²ï¼Œå› æ­¤å­˜åœ¨å™ªéŸ³(noisy)å•é¡Œï¼Œä¸”å™ªéŸ³å•é¡Œè¢«æ¨¡å‹å¸¶ä¾†çš„æ•ˆæœæ©è”½ â‡’ å› æ­¤æå‡ºCapFilt ä¸åŒæ€§è³ªçš„ä»»å‹™èƒŒå¾Œçš„backboneæœƒä¸åŒ â‡’ æå‡ºå¤šæ¨¡æ…‹æ··åˆencoder-decoder understanding-base tasks â‡’ encoder generation-base tasks â‡’ encoder-decoder 2." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://muxi1998.github.io/Paper-Blog/paper_notes/blip/" /><meta property="article:section" content="paper_notes" />
<meta property="article:published_time" content="2024-01-06T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-01-06T00:00:00+00:00" />


  
  <meta name="twitter:card" content="summary"/><meta name="twitter:title" content="BLIP"/>
<meta name="twitter:description" content="Abstract ç›®å‰çš„VLPæ¨¡å‹æ€§èƒ½æå‡çš„æ–¹å¼ä¸»è¦æ˜¯é æ“´å¤§è³‡æ–™é›†â‡’ ç›®å‰æ˜¯ä»¥ç¶²è·¯ä¸Šçš„image-text pair è¨“ç·´ ç¶²è·¯ä¸Šçš„dataå¾ˆå¤§çš„å¯èƒ½å­˜åœ¨noise æ­¤è«–æ–‡æå‡ºä¸€å€‹æ¨™é¡Œç”¢ç”Ÿå™¨å’Œå»å™ªå™¨ä¾†è™•ç†ç¶²è·¯è³‡æ–™é›†æ‰€å­˜åœ¨çš„å™ªéŸ³å•é¡Œ 1. Introduction ğŸ¯ ç›®æ¨™ï¼š æå‡ºä¸€å€‹æ›´å¼·å¤§çš„VLPæ¶æ§‹
ğŸ•°ï¸ éå»æ–¹æ³•
åœ¨éå»çš„VLP(Vision -anguage Pretraining) æ–¹æ³•ä¸­æœ‰å…©å¤§å±¤é¢çš„é™åˆ¶ æ¨¡å‹å±¤é¢ é‡å°ä¸åŒçš„ä¸‹æ¸¸æ‡‰ç”¨ç›®å‰é‚„æ˜¯æœ‰å„è‡ªé©åˆçš„æ¶æ§‹ï¼Œé‚„æ²’æœ‰ä¸€å€‹èƒ½å®Œå…¨çµ±ä¸€å€‹çš„æ¨¡å‹ ç”Ÿæˆä»»å‹™ï¼ˆe.g. æ–‡å­—ç”Ÿæˆï¼‰â‡’ Encoder-Decoder ç†è§£ä»»å‹™ï¼ˆe.g. æª¢ç´¢ï¼‰â‡’ Encoder è³‡æ–™å±¤é¢ éå»æ–¹æ³•çš„è¨“ç·´è³‡æ–™å¤šä¾†è‡ªç¶²è·¯çˆ¬èŸ²æ‰€å¾—ï¼Œå­˜åœ¨noiseè³‡æ–™æœªè¢«æ¸…ç†ä¹¾æ·¨ï¼Œä¸”noiseå¸¶ä¾†çš„è² é¢å½±éŸ¿å°šæœªè¢«é©ç•¶è§£æ±º ğŸ’¡ æœ¬ç¯‡æ–¹æ³•
é‡å°ä¸Šè¿°æåˆ°çš„å…©å¤§å±¤é¢å•é¡Œé€²è¡Œç ”ç©¶ æ¨¡å‹å±¤é¢ æå‡ºä¸€å€‹å¤šæ¨¡æ…‹æ··åˆï¼ˆMultimodal mixtureï¼‰çš„Encoder-Decoderæ¶æ§‹ (MED) å¯ä»¥åœ¨å¾ŒçºŒæ‡‰ç”¨åœ¨æ›´å¤šçš„ä¸‹æ¸¸ä»»å‹™ä¸­ ä¿æŒé è¨“ç·´æ™‚çš„æ•ˆç‡ è³‡æ–™å±¤é¢ æå‡ºä¸€å€‹å¼•å°ï¼ˆBootstrappingï¼‰æ–¹æ³•ä¾†é¿å…noisy image-text pair Finetuneä¸€å€‹pre-trained MEDæˆå…©å€‹å­æ¨¡çµ„ Cap (Captioner) â‡’ ç”Ÿæˆåˆæˆå­—å¹• Filt (Filter)â‡’ éæ¿¾æ‰noisyå­—å¹• ğŸ”¥ ç ”ç©¶æˆæœ
å¼•å°å­—å¹•å¯ä»¥æå‡ä¸‹æ¸¸ä»»å‹™çš„æ•ˆèƒ½ï¼Œä¸”å­—å¹•å¤šæ¨£æ€§è¶Šé«˜è¶Šå¥½ BLIPä¸åƒ…åœ¨Vision-language tasksä¸­æœ‰SOTAçš„æ•ˆèƒ½ï¼Œåœ¨è½‰ç§»è‡³Video-language tasksä¸­ä¹Ÿé”åˆ°çš„SOTAä¸”zero-shotçš„æ•ˆèƒ½ 2. Related Work 2.1 Vision-language Pre-training éå»æ–¹æ³•çš„datasetä¾†æºå¤šæ˜¯ç¶²è·¯çˆ¬èŸ²ï¼Œå› æ­¤å­˜åœ¨å™ªéŸ³(noisy)å•é¡Œï¼Œä¸”å™ªéŸ³å•é¡Œè¢«æ¨¡å‹å¸¶ä¾†çš„æ•ˆæœæ©è”½ â‡’ å› æ­¤æå‡ºCapFilt ä¸åŒæ€§è³ªçš„ä»»å‹™èƒŒå¾Œçš„backboneæœƒä¸åŒ â‡’ æå‡ºå¤šæ¨¡æ…‹æ··åˆencoder-decoder understanding-base tasks â‡’ encoder generation-base tasks â‡’ encoder-decoder 2."/>

  
  
  
  <link rel="canonical" href="https://muxi1998.github.io/Paper-Blog/paper_notes/blip/" />
  
  
</head>

  <body class="text-black duration-200 ease-out dark:text-white">
    <header class="mx-auto flex h-[4.5rem] max-w-3xl px-8 lg:justify-center">
  <div class="relative z-50 mr-auto flex items-center">
    <a
      class="-translate-x-[1px] -translate-y-[1px] text-2xl font-semibold"
      href="https://muxi1998.github.io/Paper-Blog/"
      >My Paper note site</a
    >
    <div
      class="btn-dark text-[0] ml-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.png)_left_center/_auto_theme('spacing.6')_no-repeat] [transition:_background-position_0.4s_steps(5)] dark:[background-position:right]"
      role="button"
      aria-label="Dark"
    ></div>
  </div>

  <div
    class="btn-menu relative z-50 -mr-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden"
    role="button"
    aria-label="Menu"
  ></div>

  

  <script>
    
    const htmlClass = document.documentElement.classList;
    setTimeout(() => {
      htmlClass.remove('not-ready');
    }, 10);

    
    const btnMenu = document.querySelector('.btn-menu');
    btnMenu.addEventListener('click', () => {
      htmlClass.toggle('open');
    });

    
    const metaTheme = document.querySelector('meta[name="theme-color"]');
    const lightBg = '#faf8f1'.replace(/"/g, '');
    const setDark = (isDark) => {
      metaTheme.setAttribute('content', isDark ? '#000' : lightBg);
      htmlClass[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark);
    };

    
    const darkScheme = window.matchMedia('(prefers-color-scheme: dark)');
    if (htmlClass.contains('dark')) {
      setDark(true);
    } else {
      const darkVal = localStorage.getItem('dark');
      setDark(darkVal ? darkVal === 'true' : darkScheme.matches);
    }

    
    darkScheme.addEventListener('change', (event) => {
      setDark(event.matches);
    });

    
    const btnDark = document.querySelector('.btn-dark');
    btnDark.addEventListener('click', () => {
      setDark(localStorage.getItem('dark') !== 'true');
    });
  </script>

  <div
    class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"
  >
    
    

    
  </div>
</header>


    <main
      class="prose prose-neutral relative mx-auto min-h-[calc(100%-9rem)] max-w-3xl px-8 pb-16 pt-12 dark:prose-invert"
    >
      

<article>
  <header class="mb-16">
    <h1 class="!my-0 pb-2.5">BLIP</h1>

    
    <div class="text-sm antialiased opacity-60">
      
      <time>Jan 6, 2024</time>
      
      
      
      
    </div>
    
  </header>

  <section><h1 id="abstract">Abstract</h1>
<ul>
<li>ç›®å‰çš„VLPæ¨¡å‹æ€§èƒ½æå‡çš„æ–¹å¼ä¸»è¦æ˜¯é æ“´å¤§è³‡æ–™é›†â‡’ ç›®å‰æ˜¯ä»¥ç¶²è·¯ä¸Šçš„image-text pair è¨“ç·´
<ul>
<li>ç¶²è·¯ä¸Šçš„dataå¾ˆå¤§çš„å¯èƒ½å­˜åœ¨noise</li>
<li>æ­¤è«–æ–‡æå‡ºä¸€å€‹<strong>æ¨™é¡Œç”¢ç”Ÿå™¨</strong>å’Œ<strong>å»å™ªå™¨</strong>ä¾†è™•ç†ç¶²è·¯è³‡æ–™é›†æ‰€å­˜åœ¨çš„å™ªéŸ³å•é¡Œ</li>
</ul>
</li>
</ul>
<h1 id="1-introduction">1. Introduction</h1>
<p>ğŸ¯ <strong>ç›®æ¨™ï¼š</strong> æå‡ºä¸€å€‹æ›´å¼·å¤§çš„VLPæ¶æ§‹</p>
<p>ğŸ•°ï¸ <strong>éå»æ–¹æ³•</strong></p>
<ul>
<li>åœ¨éå»çš„VLP(Vision -anguage Pretraining) æ–¹æ³•ä¸­æœ‰å…©å¤§å±¤é¢çš„é™åˆ¶
<ul>
<li>æ¨¡å‹å±¤é¢
<ul>
<li>é‡å°ä¸åŒçš„ä¸‹æ¸¸æ‡‰ç”¨ç›®å‰é‚„æ˜¯æœ‰å„è‡ªé©åˆçš„æ¶æ§‹ï¼Œé‚„æ²’æœ‰ä¸€å€‹èƒ½å®Œå…¨çµ±ä¸€å€‹çš„æ¨¡å‹
<ul>
<li>ç”Ÿæˆä»»å‹™ï¼ˆe.g. æ–‡å­—ç”Ÿæˆï¼‰â‡’ Encoder-Decoder</li>
<li>ç†è§£ä»»å‹™ï¼ˆe.g. æª¢ç´¢ï¼‰â‡’ Encoder</li>
</ul>
</li>
</ul>
</li>
<li>è³‡æ–™å±¤é¢
<ul>
<li>éå»æ–¹æ³•çš„è¨“ç·´è³‡æ–™å¤šä¾†è‡ªç¶²è·¯çˆ¬èŸ²æ‰€å¾—ï¼Œå­˜åœ¨noiseè³‡æ–™æœªè¢«æ¸…ç†ä¹¾æ·¨ï¼Œä¸”noiseå¸¶ä¾†çš„è² é¢å½±éŸ¿å°šæœªè¢«é©ç•¶è§£æ±º</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>ğŸ’¡ <strong>æœ¬ç¯‡æ–¹æ³•</strong></p>
<ul>
<li>é‡å°ä¸Šè¿°æåˆ°çš„å…©å¤§å±¤é¢å•é¡Œé€²è¡Œç ”ç©¶
<ul>
<li>æ¨¡å‹å±¤é¢
<ul>
<li>æå‡ºä¸€å€‹å¤šæ¨¡æ…‹æ··åˆï¼ˆMultimodal mixtureï¼‰çš„Encoder-Decoderæ¶æ§‹ (MED)
<ul>
<li>å¯ä»¥åœ¨å¾ŒçºŒæ‡‰ç”¨åœ¨æ›´å¤šçš„ä¸‹æ¸¸ä»»å‹™ä¸­</li>
<li>ä¿æŒé è¨“ç·´æ™‚çš„æ•ˆç‡</li>
</ul>
</li>
</ul>
</li>
<li>è³‡æ–™å±¤é¢
<ul>
<li>æå‡ºä¸€å€‹å¼•å°ï¼ˆBootstrappingï¼‰æ–¹æ³•ä¾†é¿å…noisy image-text pair</li>
<li>Finetuneä¸€å€‹pre-trained MEDæˆå…©å€‹å­æ¨¡çµ„
<ul>
<li>Cap (Captioner) â‡’ ç”Ÿæˆåˆæˆå­—å¹•</li>
<li>Filt (Filter)â‡’ éæ¿¾æ‰noisyå­—å¹•</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>ğŸ”¥ <strong>ç ”ç©¶æˆæœ</strong></p>
<ol>
<li>å¼•å°å­—å¹•å¯ä»¥æå‡ä¸‹æ¸¸ä»»å‹™çš„æ•ˆèƒ½ï¼Œä¸”å­—å¹•å¤šæ¨£æ€§è¶Šé«˜è¶Šå¥½</li>
<li>BLIPä¸åƒ…åœ¨Vision-language tasksä¸­æœ‰SOTAçš„æ•ˆèƒ½ï¼Œåœ¨è½‰ç§»è‡³Video-language tasksä¸­ä¹Ÿé”åˆ°çš„SOTAä¸”zero-shotçš„æ•ˆèƒ½</li>
</ol>
<h1 id="2-related-work">2. Related Work</h1>
<h2 id="21-vision-language-pre-training">2.1 Vision-language Pre-training</h2>
<ul>
<li>éå»æ–¹æ³•çš„datasetä¾†æºå¤šæ˜¯ç¶²è·¯çˆ¬èŸ²ï¼Œå› æ­¤å­˜åœ¨å™ªéŸ³(noisy)å•é¡Œï¼Œä¸”å™ªéŸ³å•é¡Œè¢«æ¨¡å‹å¸¶ä¾†çš„æ•ˆæœæ©è”½ â‡’ å› æ­¤æå‡º<strong>CapFilt</strong></li>
<li>ä¸åŒæ€§è³ªçš„ä»»å‹™èƒŒå¾Œçš„backboneæœƒä¸åŒ â‡’ æå‡º<strong>å¤šæ¨¡æ…‹æ··åˆencoder-decoder</strong>
<ul>
<li>understanding-base tasks â‡’ encoder</li>
<li>generation-base tasks â‡’ encoder-decoder</li>
</ul>
</li>
</ul>
<h2 id="22-knowledge-distillation">2.2 Knowledge Distillation</h2>
<ul>
<li>CapFiltæ¨¡çµ„é¡ä¼¼æ–¼student-teacheræ–¹æ³•ï¼ŒCaptioneré€éç”Ÿæˆç”¢ç”Ÿå­—å¹•ä¾†å­¸ç¿’èªæ„ï¼ŒFilteré€ééæ¿¾é›œè¨Šä¾†å­¸ç¿’èªæ„</li>
<li>CapFiltå¯ä»¥ç›¸è¼”ç›¸æˆ</li>
</ul>
<h2 id="23-data-augmentation">2.3 Data Augmentation</h2>
<ul>
<li>èªè¨€ä»»å‹™ä¸­çš„DA(Data Augmentation)ç›¸è¼ƒæ–¼vision tasksè¼ƒç‚ºå›°é›£</li>
<li>æœ¬ç¯‡ç ”ç©¶å±•ç¾äº†åˆæˆå­—å¹•å°æ–¼å¤§è¦æ¨¡çš„vision-language pre-trainingçš„æˆæ•ˆæ˜¯ä¸éŒ¯çš„</li>
</ul>
<h1 id="3-method">3. Method</h1>
<h2 id="31-model-architecture">3.1 Model Architecture</h2>
<p><strong>MED</strong> (Multimodal mixture encoder-decoder) æ˜¯ä¸€å€‹multi-task modalï¼Œä¸¦å¯ä»¥æä¾›ä»¥ä¸‹<!-- raw HTML omitted --><strong>ä¸‰ç¨®åŠŸèƒ½</strong><!-- raw HTML omitted --></p>
<ol>
<li>
<p>Unimodal encoder</p>
<ol>
<li>
<p>Image Encoder</p>
<ul>
<li>
<p>Transformer is better than the object detection model in feature extraction propose</p>
</li>
<li>
<!-- raw HTML omitted -->
<p><img src="./BLIP/model.png" alt="Vision Transformer Image"></p>
<!-- raw HTML omitted -->
</li>
</ul>
</li>
<li>
<p>Text Encoder</p>
<ul>
<li>
<!-- raw HTML omitted -->
<p><img src="./BLIP/BERT.png" alt="BERT Image"></p>
<!-- raw HTML omitted -->
</li>
</ul>
</li>
</ol>
</li>
<li>
<p>Image-grounded text encoder</p>
<ul>
<li>Inject <strong>visual information</strong> for  the cross-attention(CA)</li>
<li>Use specific task-specific <strong>token [Encode]</strong> appended to the input text to show the purpose is to generate the representation of the image-text pair</li>
</ul>
</li>
<li>
<p>Image-grounded text decoder</p>
<ul>
<li>Bi-directional attention â‡’ (change to) <strong>causal self-attention</strong></li>
<li>token [Decode] is used to signal the beginning of the sequence</li>
<li>end-of-sequence token is used to signal the end</li>
</ul>
</li>
</ol>
<h2 id="32-pre-training-objectives">3.2 Pre-training Objectives</h2>
<ul>
<li><strong>Three</strong> objectives
<ul>
<li>Understanding-based (x2)</li>
<li>Generation-based (x1)</li>
</ul>
</li>
<li>Computation flow
<ul>
<li>vision transformer (ViT): one-pass to save the computation loading</li>
<li>text transformer (BERT): three-pass</li>
</ul>
</li>
<li><strong>Three Losses</strong>
<ul>
<li>Image-Text Contrastive Loss (ITC) â‡’ Align the representation of vision and text
<ul>
<li>A positive image-text pair should have similar <strong>representation</strong> between image feature and text feature</li>
<li>Negative image-text pair should have more different representation</li>
</ul>
</li>
<li>Image-Text Matching Loss (ITM) â‡’ Distinguish whether the image-text pair is positive or negative
<ul>
<li>binary classification problem</li>
<li>Purpose is to check whether the image and text are matched</li>
</ul>
</li>
<li>Language Modeling Loss (LM) â‡’ generate textual description given an image
<ul>
<li>cross entropy loss: maximize the likelihood of the text in an autoregressive manner</li>
</ul>
</li>
</ul>
</li>
<li>Tricks of Minimizing the training computation
<ul>
<li>Share Weights between text encoder and text decoder, except <strong>SA layers</strong></li>
<li>Main components that makes the encoder and decoder different is the <!-- raw HTML omitted --></li>
</ul>
</li>
</ul>
<h2 id="33-capfilt">3.3 CapFilt</h2>
<ul>
<li><strong>Previous Problem:</strong> limited number of high-quality human annotated image-text pairs ${(I_h, T_h)}$
<ul>
<li>e.g. COCO dataset</li>
</ul>
</li>
<li><strong>Previous solution and limitation:</strong> Crawl image and alt-text pairs from the website
<ul>
<li>often do not accurately describe the visual content â‡’ <strong>noisy data</strong></li>
</ul>
</li>
<li><strong>Proposed solution:</strong> Finetune CapFilt on high-quality annotated image-text pair (e.g. COCO dataset)
<ul>
<li>Cap (Captioner)
<ul>
<li>finetune with <strong>LM loss</strong> to decode (synthesis) texts $T_s$ of given web images $I_w$</li>
<li>$Cap(I_w)=T_s$</li>
</ul>
</li>
<li>Filt (Filter)
<ul>
<li>finetune with <strong>ITC</strong> and <strong>ITM loss</strong> to learn whether a text matches an image</li>
<li>a text is considered to be noisy if the ITM predicts the input text and image pair is unmatched.</li>
</ul>
</li>
<li>Finally combine the filtered image-text pairs with the human-annotated pairs to form a new dataset</li>
</ul>
</li>
</ul>
<h1 id="4-experiments">4. Experiments</h1>
<ul>
<li>Dataset
<ul>
<li>14M images, includes</li>
<li><strong>two</strong> human-annotated dataset
<ul>
<li>COCO</li>
<li>Visual Genome</li>
</ul>
</li>
<li><strong>three</strong> web dataset
<ul>
<li>Conceptual Captions</li>
<li>Conceptual 12M</li>
<li>SBU captions</li>
</ul>
</li>
</ul>
</li>
<li>Extra experiment on LAION
<ul>
<li>115M images</li>
<li>More noisy texts</li>
</ul>
</li>
</ul>
<h2 id="41-pre-training-details">4.1 Pre-training Details</h2>
<h2 id="42-effect-of-capfilt">4.2 Effect of CapFilt</h2>
<h2 id="43-diversity-is-key-for-synthetic-captions">4.3 Diversity is Key for Synthetic Captions</h2>
<h2 id="44-parameter-sharing-and-decoupling">4.4 Parameter Sharing and Decoupling</h2>
<h1 id="5--comparison-with-state-of-the-arts">5.  Comparison with State-of-the-arts</h1>
<h2 id="51-image-text-retrieval">5.1 Image-Text Retrieval</h2>
<h2 id="52-image-captioning">5.2 Image Captioning</h2>
<h2 id="53-visal-question-answering-vqa">5.3 Visal Question Answering (VQA)</h2>
<h2 id="54-natural-language-visual-reasoning-nlvr2">5.4 Natural Language Visual Reasoning (NLVR2)</h2>
<h2 id="55-visual-dialog-visdial">5.5 Visual Dialog (VisDial)</h2>
<h2 id="56-zero-shot-transfer-to-video-language-tasks">5.6 Zero-shot Transfer to Video-Language Tasks</h2>
<h1 id="6-additional-ablation-study">6. Additional Ablation Study</h1>
<h1 id="7-conclusion">7. Conclusion</h1>
</section>

  
  

  
  
  
  
  <nav class="mt-24 flex rounded-lg bg-black/[3%] text-lg dark:bg-white/[8%]">
    
    <a
      class="flex w-1/2 items-center rounded-l-md p-6 pr-3 font-semibold no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]"
      href="https://muxi1998.github.io/Paper-Blog/paper_notes/giraffe-representing-scenes-as-compositional-generative-neural-feature-fields/"
      ><span class="mr-1.5">â†</span><span>GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields</span></a
    >
    
    
    <a
      class="ml-auto flex w-1/2 items-center justify-end rounded-r-md p-6 pl-3 font-semibold no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]"
      href="https://muxi1998.github.io/Paper-Blog/paper_notes/histogan/"
      ><span>HistoGAN: Controlling Colors of GAN-Generated and Real Images via Color Histograms</span><span class="ml-1.5">â†’</span></a
    >
    
  </nav>
  
  

  
  

  
  

  

  
</article>


    </main>

    <footer
  class="opaco mx-auto flex h-[4.5rem] max-w-3xl items-center px-8 text-[0.9em] opacity-60"
>
  <div class="mr-auto">
    &copy; 2024
    <a class="link" href="https://muxi1998.github.io/Paper-Blog/">My Paper note site</a>
  </div>
  <a class="link mx-6" href="https://gohugo.io/" rel="noopener" target="_blank"
    >Powered by Hugoï¸ï¸</a
  >ï¸
  <a
    class="link"
    href="https://github.com/nanxiaobei/hugo-paper"
    rel="noopener"
    target="_blank"
    >âœ Paper</a
  >
</footer>

  </body>
</html>
