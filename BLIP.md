# Abstract

# 1. Introduction

# 2. Related Work

## 2.1 Vision-language Pre-training

## 2.2 Knowledge Distillation

## 2.3 Data Augmentation

# 3. Method

## 3.1 Model Architecture

## 3.2 Pre-training Objectives

## 3.3 CapFilt

# 4. Experiments

## 4.1 Pre-training Details

## 4.2 Effect of CapFilt

## 4.3 Diversity is Key for Synthetic Captions

## 4.4 Parameter Sharing and Decoupling

# 5.  Comparison with State-of-the-arts

## 5.1 Image-Text Retrieval

## 5.2 Image Captioning

## 5.3 Visal Question Answering (VQA)

## 5.4 Natural Language Visual Reasoning (NLVR2)

## 5.5 Visual Dialog (VisDial)

## 5.6 Zero-shot Transfer to Video-Language Tasks

# 6. Additional Ablation Study

# 7. Conclusion