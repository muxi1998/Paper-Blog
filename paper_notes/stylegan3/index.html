<!doctype html>





























<html
  class="not-ready lg:text-base"
  style="--bg: #faf8f1"
  lang="en-us"
>
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, shrink-to-fit=no"
  />

  
  <title>Alias-Free Generative Adversarial Networks - My Paper note site</title>

  
  <meta name="theme-color" />

  
  
  
  
  <meta name="description" content="Abstract 問題發現 圖片合成的過程過度依賴像素座標，導致圖片細節可能黏在座標上而非描繪對象的表面
解決概念
將網路中的信息都是為連續的
解決方法
小幅度修改原本模型架構，達到普遍適用性高且能避免不需要知資訊洩漏至分層融合的過程
結果成效
對動態影像的合成有很大的幫助
1 Introduction 目前最大問題 在現實中不同尺度的細節往往是分層轉換的，而GAN並沒有做到自然的分層 細節特徵的位置應該繼承較上層粗略的特徵，彼此應該有繼承關係
猜測與發現 目前的網路會參考隱藏層中洩漏出的位置訊息來畫上細節，但如此的分層結構並不自然 image border
在進行卷積運算時常常會使用到zero-padding，而此舉動會不經意的洩漏出絕對位置的訊息
參考資訊 HOW MUCH POSITION INFORMATION DO CONVOLUTIONAL NEURAL NETWORKS ENCODE?
猜測在CNN各層卷積中隱含著絕對位置的資訊 zero-padding造成模型在無意間學習的特徵的絕對位置 per-pixel noise inputs
positional encoding
aliasing
可能造成混疊問題發生的原因 使用非理想採樣濾波器造成的，像素網格弱化（pixel位置資訊混淆？） Ex: nearest, bilinear, strided卷積 針對各個pixel使用非線性轉換 Ex: ReLU, swish 因為以上兩種可能導致網路會不自覺得將不同尺度的資訊都畫在basis上，而此basis就是螢幕中看到的平面座標 解決方法 去除所有可能的絕對位置參考來源 使得各層卷積的生成能夠更平等，而非受限於洩漏的絕對位置資訊 目前的上採樣filter並沒有很好的抑制混疊 使用low-pass filter來對原本的result再進一步處理來修正pointwise非線性造成的混疊問題 全面檢修StyleGAN2的信息處理 提出了一個更根本不同的圖片生成概念 Equivariance vs. Inquivariance Equivariance 等變性 對於輸入施加一些改變會反映在輸出上 $f(g(x))=g(f(x))$ $f(\cdot):$ 特徵函數 $g(\cdot):$ 變換函數 Inquivariance 不變性 對輸入施加一些改變不會影響輸出 CNN中的equivariant vs." />
  <meta name="author" content="My Paper note site" />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="https://muxi1998.github.io/Paper-Blog/main.min.css" />

  
  
  
  
  
  <link rel="preload" as="image" href="https://muxi1998.github.io/Paper-Blog/theme.png" />

  
  
  
  
  

  
  
  

  
  
  <script
    defer
    src="https://muxi1998.github.io/Paper-Blog/highlight.min.js"
    onload="hljs.initHighlightingOnLoad();"
  ></script>
  

  
  
  

  
  <link rel="icon" href="https://muxi1998.github.io/Paper-Blog/favicon.ico" />
  <link rel="apple-touch-icon" href="https://muxi1998.github.io/Paper-Blog/apple-touch-icon.png" />

  
  <meta name="generator" content="Hugo 0.121.1">

  
  
  
  
  
  <meta itemprop="name" content="Alias-Free Generative Adversarial Networks">
<meta itemprop="description" content="Abstract 問題發現 圖片合成的過程過度依賴像素座標，導致圖片細節可能黏在座標上而非描繪對象的表面
解決概念
將網路中的信息都是為連續的
解決方法
小幅度修改原本模型架構，達到普遍適用性高且能避免不需要知資訊洩漏至分層融合的過程
結果成效
對動態影像的合成有很大的幫助
1 Introduction 目前最大問題 在現實中不同尺度的細節往往是分層轉換的，而GAN並沒有做到自然的分層 細節特徵的位置應該繼承較上層粗略的特徵，彼此應該有繼承關係
猜測與發現 目前的網路會參考隱藏層中洩漏出的位置訊息來畫上細節，但如此的分層結構並不自然 image border
在進行卷積運算時常常會使用到zero-padding，而此舉動會不經意的洩漏出絕對位置的訊息
參考資訊 HOW MUCH POSITION INFORMATION DO CONVOLUTIONAL NEURAL NETWORKS ENCODE?
猜測在CNN各層卷積中隱含著絕對位置的資訊 zero-padding造成模型在無意間學習的特徵的絕對位置 per-pixel noise inputs
positional encoding
aliasing
可能造成混疊問題發生的原因 使用非理想採樣濾波器造成的，像素網格弱化（pixel位置資訊混淆？） Ex: nearest, bilinear, strided卷積 針對各個pixel使用非線性轉換 Ex: ReLU, swish 因為以上兩種可能導致網路會不自覺得將不同尺度的資訊都畫在basis上，而此basis就是螢幕中看到的平面座標 解決方法 去除所有可能的絕對位置參考來源 使得各層卷積的生成能夠更平等，而非受限於洩漏的絕對位置資訊 目前的上採樣filter並沒有很好的抑制混疊 使用low-pass filter來對原本的result再進一步處理來修正pointwise非線性造成的混疊問題 全面檢修StyleGAN2的信息處理 提出了一個更根本不同的圖片生成概念 Equivariance vs. Inquivariance Equivariance 等變性 對於輸入施加一些改變會反映在輸出上 $f(g(x))=g(f(x))$ $f(\cdot):$ 特徵函數 $g(\cdot):$ 變換函數 Inquivariance 不變性 對輸入施加一些改變不會影響輸出 CNN中的equivariant vs."><meta itemprop="datePublished" content="2021-11-24T00:00:00+00:00" />
<meta itemprop="dateModified" content="2021-11-24T00:00:00+00:00" />
<meta itemprop="wordCount" content="369">
<meta itemprop="keywords" content="" />
  
  <meta property="og:title" content="Alias-Free Generative Adversarial Networks" />
<meta property="og:description" content="Abstract 問題發現 圖片合成的過程過度依賴像素座標，導致圖片細節可能黏在座標上而非描繪對象的表面
解決概念
將網路中的信息都是為連續的
解決方法
小幅度修改原本模型架構，達到普遍適用性高且能避免不需要知資訊洩漏至分層融合的過程
結果成效
對動態影像的合成有很大的幫助
1 Introduction 目前最大問題 在現實中不同尺度的細節往往是分層轉換的，而GAN並沒有做到自然的分層 細節特徵的位置應該繼承較上層粗略的特徵，彼此應該有繼承關係
猜測與發現 目前的網路會參考隱藏層中洩漏出的位置訊息來畫上細節，但如此的分層結構並不自然 image border
在進行卷積運算時常常會使用到zero-padding，而此舉動會不經意的洩漏出絕對位置的訊息
參考資訊 HOW MUCH POSITION INFORMATION DO CONVOLUTIONAL NEURAL NETWORKS ENCODE?
猜測在CNN各層卷積中隱含著絕對位置的資訊 zero-padding造成模型在無意間學習的特徵的絕對位置 per-pixel noise inputs
positional encoding
aliasing
可能造成混疊問題發生的原因 使用非理想採樣濾波器造成的，像素網格弱化（pixel位置資訊混淆？） Ex: nearest, bilinear, strided卷積 針對各個pixel使用非線性轉換 Ex: ReLU, swish 因為以上兩種可能導致網路會不自覺得將不同尺度的資訊都畫在basis上，而此basis就是螢幕中看到的平面座標 解決方法 去除所有可能的絕對位置參考來源 使得各層卷積的生成能夠更平等，而非受限於洩漏的絕對位置資訊 目前的上採樣filter並沒有很好的抑制混疊 使用low-pass filter來對原本的result再進一步處理來修正pointwise非線性造成的混疊問題 全面檢修StyleGAN2的信息處理 提出了一個更根本不同的圖片生成概念 Equivariance vs. Inquivariance Equivariance 等變性 對於輸入施加一些改變會反映在輸出上 $f(g(x))=g(f(x))$ $f(\cdot):$ 特徵函數 $g(\cdot):$ 變換函數 Inquivariance 不變性 對輸入施加一些改變不會影響輸出 CNN中的equivariant vs." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://muxi1998.github.io/Paper-Blog/paper_notes/stylegan3/" /><meta property="article:section" content="paper_notes" />
<meta property="article:published_time" content="2021-11-24T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-11-24T00:00:00+00:00" />


  
  <meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Alias-Free Generative Adversarial Networks"/>
<meta name="twitter:description" content="Abstract 問題發現 圖片合成的過程過度依賴像素座標，導致圖片細節可能黏在座標上而非描繪對象的表面
解決概念
將網路中的信息都是為連續的
解決方法
小幅度修改原本模型架構，達到普遍適用性高且能避免不需要知資訊洩漏至分層融合的過程
結果成效
對動態影像的合成有很大的幫助
1 Introduction 目前最大問題 在現實中不同尺度的細節往往是分層轉換的，而GAN並沒有做到自然的分層 細節特徵的位置應該繼承較上層粗略的特徵，彼此應該有繼承關係
猜測與發現 目前的網路會參考隱藏層中洩漏出的位置訊息來畫上細節，但如此的分層結構並不自然 image border
在進行卷積運算時常常會使用到zero-padding，而此舉動會不經意的洩漏出絕對位置的訊息
參考資訊 HOW MUCH POSITION INFORMATION DO CONVOLUTIONAL NEURAL NETWORKS ENCODE?
猜測在CNN各層卷積中隱含著絕對位置的資訊 zero-padding造成模型在無意間學習的特徵的絕對位置 per-pixel noise inputs
positional encoding
aliasing
可能造成混疊問題發生的原因 使用非理想採樣濾波器造成的，像素網格弱化（pixel位置資訊混淆？） Ex: nearest, bilinear, strided卷積 針對各個pixel使用非線性轉換 Ex: ReLU, swish 因為以上兩種可能導致網路會不自覺得將不同尺度的資訊都畫在basis上，而此basis就是螢幕中看到的平面座標 解決方法 去除所有可能的絕對位置參考來源 使得各層卷積的生成能夠更平等，而非受限於洩漏的絕對位置資訊 目前的上採樣filter並沒有很好的抑制混疊 使用low-pass filter來對原本的result再進一步處理來修正pointwise非線性造成的混疊問題 全面檢修StyleGAN2的信息處理 提出了一個更根本不同的圖片生成概念 Equivariance vs. Inquivariance Equivariance 等變性 對於輸入施加一些改變會反映在輸出上 $f(g(x))=g(f(x))$ $f(\cdot):$ 特徵函數 $g(\cdot):$ 變換函數 Inquivariance 不變性 對輸入施加一些改變不會影響輸出 CNN中的equivariant vs."/>

  
  
  
  <link rel="canonical" href="https://muxi1998.github.io/Paper-Blog/paper_notes/stylegan3/" />
  
  
</head>

  <body class="text-black duration-200 ease-out dark:text-white">
    <header class="mx-auto flex h-[4.5rem] max-w-3xl px-8 lg:justify-center">
  <div class="relative z-50 mr-auto flex items-center">
    <a
      class="-translate-x-[1px] -translate-y-[1px] text-2xl font-semibold"
      href="https://muxi1998.github.io/Paper-Blog/"
      >My Paper note site</a
    >
    <div
      class="btn-dark text-[0] ml-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.png)_left_center/_auto_theme('spacing.6')_no-repeat] [transition:_background-position_0.4s_steps(5)] dark:[background-position:right]"
      role="button"
      aria-label="Dark"
    ></div>
  </div>

  <div
    class="btn-menu relative z-50 -mr-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden"
    role="button"
    aria-label="Menu"
  ></div>

  

  <script>
    
    const htmlClass = document.documentElement.classList;
    setTimeout(() => {
      htmlClass.remove('not-ready');
    }, 10);

    
    const btnMenu = document.querySelector('.btn-menu');
    btnMenu.addEventListener('click', () => {
      htmlClass.toggle('open');
    });

    
    const metaTheme = document.querySelector('meta[name="theme-color"]');
    const lightBg = '#faf8f1'.replace(/"/g, '');
    const setDark = (isDark) => {
      metaTheme.setAttribute('content', isDark ? '#000' : lightBg);
      htmlClass[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark);
    };

    
    const darkScheme = window.matchMedia('(prefers-color-scheme: dark)');
    if (htmlClass.contains('dark')) {
      setDark(true);
    } else {
      const darkVal = localStorage.getItem('dark');
      setDark(darkVal ? darkVal === 'true' : darkScheme.matches);
    }

    
    darkScheme.addEventListener('change', (event) => {
      setDark(event.matches);
    });

    
    const btnDark = document.querySelector('.btn-dark');
    btnDark.addEventListener('click', () => {
      setDark(localStorage.getItem('dark') !== 'true');
    });
  </script>

  <div
    class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"
  >
    
    

    
  </div>
</header>


    <main
      class="prose prose-neutral relative mx-auto min-h-[calc(100%-9rem)] max-w-3xl px-8 pb-16 pt-12 dark:prose-invert"
    >
      

<article>
  <header class="mb-16">
    <h1 class="!my-0 pb-2.5">Alias-Free Generative Adversarial Networks</h1>

    
    <div class="text-sm antialiased opacity-60">
      
      <time>Nov 24, 2021</time>
      
      
      
      
    </div>
    
  </header>

  <section><h1 id="abstract">Abstract</h1>
<p><strong>問題發現</strong>
圖片合成的過程過度依賴像素座標，導致圖片細節可能黏在座標上而非描繪對象的表面</p>
<p><strong>解決概念</strong></p>
<p>將網路中的信息都是為連續的</p>
<p><strong>解決方法</strong></p>
<p>小幅度修改原本模型架構，達到普遍適用性高且能避免不需要知資訊洩漏至分層融合的過程</p>
<p><strong>結果成效</strong></p>
<p>對動態影像的合成有很大的幫助</p>
<h1 id="1-introduction">1 Introduction</h1>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h3 id="目前最大問題">目前最大問題</h3>
<p>在現實中不同尺度的細節往往是分層轉換的，而GAN並沒有做到自然的分層
細節特徵的位置應該繼承較上層粗略的特徵，彼此應該有繼承關係</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h3 id="猜測與發現">猜測與發現</h3>
<ul>
<li>目前的網路會參考隱藏層中洩漏出的位置訊息來畫上細節，但如此的分層結構並不自然
<ul>
<li>
<p>image border</p>
<p>在進行卷積運算時常常會使用到zero-padding，而此舉動會不經意的洩漏出絕對位置的訊息</p>
<ul>
<li>參考資訊
<ul>
<li>
<p>HOW MUCH POSITION INFORMATION DO CONVOLUTIONAL NEURAL NETWORKS ENCODE?</p>
<ul>
<li>猜測在CNN各層卷積中隱含著絕對位置的資訊</li>
<li>zero-padding造成模型在無意間學習的特徵的絕對位置</li>
</ul>
<p><a href="https://arxiv.org/pdf/2001.08248.pdf"></a></p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>per-pixel noise inputs</p>
</li>
<li>
<p>positional encoding</p>
</li>
<li>
<p>aliasing</p>
</li>
</ul>
</li>
<li>可能造成混疊問題發生的原因
<ol>
<li>使用非理想採樣濾波器造成的，像素網格弱化（pixel位置資訊混淆？）
Ex: nearest, bilinear, strided卷積</li>
<li>針對各個pixel使用非線性轉換
Ex: ReLU, swish</li>
</ol>
<ul>
<li>因為以上兩種可能導致網路會不自覺得將不同尺度的資訊都畫在basis上，而此basis就是螢幕中看到的平面座標</li>
</ul>
</li>
</ul>
<h3 id="解決方法">解決方法</h3>
<ul>
<li>去除所有可能的絕對位置參考來源
使得各層卷積的生成能夠更平等，而非受限於洩漏的絕對位置資訊
<ol>
<li>目前的上採樣filter並沒有很好的抑制混疊</li>
<li>使用low-pass filter來對原本的result再進一步處理來修正pointwise非線性造成的混疊問題</li>
</ol>
</li>
<li>全面檢修StyleGAN2的信息處理</li>
<li>提出了一個更根本不同的圖片生成概念</li>
</ul>
<h3 id="equivariance-vs-inquivariance">Equivariance vs. Inquivariance</h3>
<ul>
<li><strong>Equivariance 等變性</strong>
<ul>
<li>對於輸入施加一些改變會反映在輸出上</li>
<li>$f(g(x))=g(f(x))$
$f(\cdot):$ 特徵函數
$g(\cdot):$ 變換函數</li>
</ul>
</li>
<li><strong>Inquivariance 不變性</strong>
<ul>
<li>對輸入施加一些改變不會影響輸出</li>
</ul>
</li>
</ul>
<p><a href="https://zhuanlan.zhihu.com/p/41682204">CNN中的equivariant vs. invariant</a></p>
<h1 id="2-equivariance-via-continuous-signal-interpretation">2 Equivariance via continuous signal interpretation</h1>
<h3 id="主旨">主旨</h3>
<ul>
<li>重新理解在神經網路中究竟傳遞的是什麼樣的訊息</li>
<li>在網路層中分別有離散和連續性的表達方式
<ul>
<li>實際的網路都是對離散的feature map進行操作</li>
<li>但離散和連續可以視為不同域的轉換，當我們實際對離散資料做處理時，可以類比到連續性資料</li>
<li>$f(z)=\phi_{s^{\prime}} \ *\ F(III_s \odot z)$
$F(Z)=III_{s^\prime} \odot f(\phi_s\ *\ Z)$
$III$: 採樣
$\phi$: 濾波
$s$: input
$s^{\prime}$: output</li>
</ul>
</li>
</ul>
<h3 id="各種補充背景">各種補充背景</h3>
<ul>
<li>
<p><strong>Nyquist-Shannon sampling therom</strong></p>
<ul>
<li>
<p>取樣是將一個訊號（時間或空間上連續的函式）轉換為數位序列（時間或空間上的離散的函式）</p>
</li>
<li>
<p>若$x(t)$不包含高於$B$ cps（次/秒）的頻率，那若取樣間隔時間小於$\frac{1}{2B}$秒，則$x(t)$的值會受到前一週期的影響</p>
</li>
<li>
<p>要使函式不受干擾，需要$2B$ 樣本/秒或更高的取樣率 （<strong>取樣頻率要夠高）</strong></p>
</li>
<li>
<p>給定取樣頻率$f_s$ 要完全重構的頻帶限制為$B\le \frac{f_s}{2}$，若$B$過高會造成混疊的現象 <strong>（原本的波形被取代）</strong></p>
<p><img src="./StyleGAN3/02.png" alt="02.png"></p>
</li>
</ul>
<p><a href="https://zh.wikipedia.org/wiki/%E9%87%87%E6%A0%B7%E5%AE%9A%E7%90%86">采样定理 - 维基百科，自由的百科全书</a></p>
<p><a href="http://wiki.csie.ncku.edu.tw/embedded/ADC/Sampling_Theorem">Sampling Theorem</a></p>
</li>
<li>
<p><strong>Dirac impulse</strong></p>
<ul>
<li>用於描述空間幾何點上的物理量 → 質點的質量分佈</li>
</ul>
<p><a href="https://baike.baidu.hk/item/%E8%84%88%E8%A1%9D%E5%87%BD%E6%95%B8/7666022">脈衝函數_百度百科</a></p>
</li>
<li>
<p><strong>Whittaker-Shannon interpolation</strong></p>
<ul>
<li>一種插值法，透過採樣點來還原連續性函數</li>
</ul>
<p><a href="https://zhuanlan.zhihu.com/p/112587699">这一切都从指数函数开始（4）&ndash;采样定理</a></p>
</li>
<li>
<p><strong>Fouier轉換</strong></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<ul>
<li>『人的耳朵是最好的傅立葉分析器』，耳朵可以透過聽到不同的頻率分辨出男女生</li>
<li>傅立葉的本質是時間的流動，讓我們找出一個在時間上流動的訊號</li>
<li>傅立葉的分析應用
<ul>
<li>濾波 （找到需要處理的頻率）</li>
<li>混頻 （將某訊號頻率移動到某個頻率）</li>
</ul>
</li>
</ul>
<p><a href="https://zhuanlan.zhihu.com/p/112587699">这一切都从指数函数开始（4）&ndash;采样定理</a></p>
</li>
</ul>
<h2 id="21-equivariant-network-layers">2.1 Equivariant network layers</h2>
<h3 id="主旨-1">主旨</h3>
<ul>
<li>在一個連續域的2D平面中，使用operation $f$ 對一個空間作轉換是具有等變性</li>
<li>此篇論文主要著重在兩種空間轉換
<ol>
<li>平移</li>
<li>旋轉</li>
</ol>
</li>
<li>在本節中會將convolution、upsampling、downsampling、nonlinearity帶入所說的operation $f$ 來探討</li>
</ul>
<h3 id="不同operation-f-的探討">不同operation $f$ 的探討</h3>
<ul>
<li><strong>Convolution</strong>
<ul>
<li>傳統的卷積為一個離散的kernel $K$，可以將$K$視為與input feature map位於同個網格中，採樣率為$s$</li>
<li>在離散域中的卷積運算 $F_{conv}(Z)=K\ *\ Z$</li>
<li>透過域轉換可以獲得在連續域中的計算為 $f_{conv}=\phi_s\ *\ (K\ *\ (III_s \odot z))=K\ *\ (\phi_s\ <em>\ (III_s \odot z))=K</em>z$</li>
<li>透過上述公式發現卷積並沒有帶給模型新的頻率資訊，因此在做卷積操作時可以滿足平移和旋轉所需要的頻帶寬</li>
<li>在平移中，卷積具有等變性，而旋轉需要是徑向對稱才能保有等變性 （發現$1\times 1$的kernek效果不錯）</li>
</ul>
</li>
<li><strong>Upsampling and downsampling</strong>
<ul>
<li>Upsampling
<ul>
<li>並沒有改變continuous representation而是提高輸出的採樣率 $s^\prime &gt; s$ <strong>用以增加額外資訊</strong></li>
</ul>
</li>
<li>Downsampling
<ul>
<li>套入ideal low-pass濾波器</li>
<li>丟掉一些資訊</li>
</ul>
</li>
</ul>
</li>
<li>N<strong>onlinearity</strong>
<ul>
<li>在連續域中，pointwise的計算與幾何轉換無關，但如何滿足頻帶寬會是一大問題</li>
<li>舉例：使用ReLU會為模型帶來無法再輸入看到的高頻資訊</li>
<li>為抑制高頻資訊的產生，可以透過另外再卷積一個低通濾波器</li>
<li>非線性是唯一會對模型加入新頻率的操作
我們可以透過加入reconstruction filter來精準控制希望每個layer實際產出多少新資訊</li>
</ul>
</li>
</ul>
<h1 id="3-practical-application-to-generator-network">3 Practical application to generator network</h1>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<ul>
<li>StyleGAN2包含兩大部分
<ol>
<li>mapping network</li>
<li>synthesis network</li>
</ol>
</li>
<li>目標是將合成網路中的連續性operation $g$ 對於轉換$t$ 具有等變性
$z_0:g(t[z_0];w)=t[g(z_0;w)]$</li>
<li>使用計算PSNR來比較微調各種模型架構對於等變性所造成的影響</li>
</ul>
<h2 id="31-fourier-features-and-baseline-simplificationsconfigs-b-d">3.1 Fourier features and baseline simplifications(configs B-D)</h2>
<h3 id="config-b">Config B</h3>
<ul>
<li>將合成網路最初的learned const轉換為<strong>傅立葉特徵 （可視為從離散域轉為連續域）</strong></li>
<li>無改善等變性，但可以計算等變性</li>
</ul>
<h3 id="config-c">Config C</h3>
<ul>
<li>移除外加的noise(增加隨機性的部分)</li>
<li>無明顯改進等變性，而noise和FID無明顯關係</li>
<li>移除noise同時也是因為和當前想看自然轉換的目標不同</li>
</ul>
<h3 id="config-d">Config D</h3>
<ul>
<li>精簡化原本的StyleGAN2架構</li>
<li>處理三個部分
<ul>
<li>降低mapping網路的深度</li>
<li>禁用各種正規化方法</li>
<li>移除skip connection</li>
</ul>
</li>
<li>以上三部分帶來的好處多和gradient有關，移除這些反而讓FID回復至StyleGAN2的水準，且稍微提升平移的等變性</li>
</ul>
<h2 id="32-step-by-step-redesign-motivated-by-continuous-interpretation">3.2 Step-by-step redesign motivated by continuous interpretation</h2>
<h3 id="config-e-boundaries-and-upsampling">Config E (Boundaries and upsampling)</h3>
<p><strong>邊界問題</strong></p>
<ul>
<li>模擬特徵圖為無邊際</li>
<li>維持一個固定大小（10pixel效果最好）的邊界在目標畫布周圍</li>
<li>在每層計算完後剪裁並貼至上述的畫布中 （避免原始的padding造成絕對位置的洩漏）</li>
</ul>
<p><strong>Upsampling</strong></p>
<ul>
<li>把bilinear 2x 上採樣filter改為理想低通濾波器（sinc）並同時使用大的Kaiser窗(n=6)</li>
<li>n=6代表上採樣時每個output像素會受到6個input像素影響
下採樣時每個input像素會影響6個output像素</li>
<li>Kaiser窗在此論文的目標中很有用，因為剛好能有效地控制transition band和衰減</li>
<li>在此配置下，FID退步，而平移的等變性稍微提升
<ul>
<li>FID退步可能是因為開始對feature map的內容有所限制</li>
</ul>
</li>
</ul>
<h3 id="config-f-filtered-nonlinearities">Config F (Filtered nonlinearities)</h3>
<ul>
<li>將非線性的計算濾波器化</li>
<li>因為頻帶寬的限制，得以將常規的<strong>2倍上採樣</strong>結合<strong>m倍的非線性上採樣</strong>變成2m倍的上採樣</li>
<li>在此配置下提升了平移的等變性</li>
</ul>
<h3 id="config-g-non-critical-sampling">Config G (Non-critical sampling)</h3>
<p><strong>critical sampling</strong></p>
<ul>
<li>將filter的cutoff設定剛好在憑帶寬
<ul>
<li>剛好避免混疊</li>
<li>盡量維持高頻率的資訊（高畫質細節的部分）</li>
</ul>
</li>
<li>在14層的前幾層中，高頻率的資訊其實不那麼重要（前面還不需要那麼細節）</li>
</ul>
<p><strong>抑制混疊的方法</strong></p>
<ul>
<li>降低cutoff的頻率至$f_c=\frac{s}{2}-f_h$</li>
<li>如此可以確保所有可能導致混疊的頻率都停留在stopband</li>
<li>在實際應用中只在較低解析度的layers選用較低的$f_c$ ，因為高解析度的layer還是需要維持高畫質來產生清晰的圖片</li>
<li>此配置使平移的等變性提升，同時使FID相較於StyleGAN2更好</li>
</ul>
<h3 id="config-h-transformed-fourier-features">Config H (Transformed Fourier features)</h3>
<ul>
<li>具等變性的生成器的其中一個特性在於，若中間特徵$z_i$有幾何變換都能傳遞至最終的圖像$z_N$</li>
<li>未能產生方向不同的圖片，本篇論文在模型中加入一個learned affine layer來給傅立葉特徵全局平移和旋轉的參數</li>
<li>此affine layer一開始會以身份轉換初始化</li>
<li>此配置稍微使FID進步</li>
</ul>
<h3 id="config-t-flexible-layer-specifications">Config T (Flexible layer specifications)</h3>
<h3 id="config-r-rotation-equivariance">Config R (Rotation equivariance)</h3>
<h1 id="4-results">4 Results</h1>
<p><img src="./StyleGAN3/%E6%88%AA%E5%9C%96_2021-12-11_%E4%B8%8A%E5%8D%8811.56.49.png" alt="截圖 2021-12-11 上午11.56.49.png"></p>
<h3 id="主要結果分析">主要結果分析</h3>
<ul>
<li>在6個資料集下測試</li>
<li>比較三個模型
<ul>
<li>StyleGAN2 (config B)</li>
<li>StyleGAN3-T</li>
<li>StyleGAN-R (config R)</li>
</ul>
</li>
<li><strong>結果分析</strong>
<ul>
<li>三個模型的FID平分秋色都不錯</li>
<li>而StyleGAN3在等變性上有很好的表現</li>
<li>只有StyleGAN3-R有旋轉的等變性</li>
<li>三者的訓練參數量
<ul>
<li>30.0M</li>
<li>22.3M</li>
<li>15.8M</li>
</ul>
</li>
<li>三者的訓練時間
<ul>
<li>1106 h</li>
<li>1576 h</li>
<li>2248 h</li>
</ul>
</li>
<li><strong>在動態轉換上StyleGAN3更為連貫，並提供了類似於3D場景的錯覺</strong></li>
</ul>
</li>
</ul>
<h3 id="ablation-and-comparisons">Ablation and comparisons</h3>
<h3 id="internal-representation">Internal representation</h3>
<ul>
<li>從feature map看出兩種模型所傳遞的訊息意義不同
<ul>
<li>StyleGAN2傳遞的是訊號強度</li>
<li>StyleGAN3傳遞的資訊包含相位</li>
</ul>
</li>
<li>由StyleGAN3的feature map包含相位資訊可以猜測，要使模型將細節合成在surface上，勢必會需要製造出一個坐標系</li>
</ul>
<p><img src="./StyleGAN3/%E6%88%AA%E5%9C%96_2021-12-11_%E4%B8%8B%E5%8D%8812.16.13.png" alt="截圖 2021-12-11 下午12.16.13.png"></p>
<h1 id="5-limitations-discussion-and-future-work">5 Limitations, discussion, and future work</h1>
<ul>
<li>目前都是修改生成器的部分，未來可以<strong>使判別器也具有等變性</strong>，提升合成結果</li>
</ul>
</section>

  
  

  
  
  
  
  <nav class="mt-24 flex rounded-lg bg-black/[3%] text-lg dark:bg-white/[8%]">
    
    <a
      class="flex w-1/2 items-center rounded-l-md p-6 pr-3 font-semibold no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]"
      href="https://muxi1998.github.io/Paper-Blog/paper_notes/blip/"
      ><span class="mr-1.5">←</span><span>BLIP</span></a
    >
    
    
    <a
      class="ml-auto flex w-1/2 items-center justify-end rounded-r-md p-6 pl-3 font-semibold no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]"
      href="https://muxi1998.github.io/Paper-Blog/paper_notes/histogan/"
      ><span>HistoGAN: Controlling Colors of GAN-Generated and Real Images via Color Histograms</span><span class="ml-1.5">→</span></a
    >
    
  </nav>
  
  

  
  

  
  

  

  
</article>


    </main>

    <footer
  class="opaco mx-auto flex h-[4.5rem] max-w-3xl items-center px-8 text-[0.9em] opacity-60"
>
  <div class="mr-auto">
    &copy; 2024
    <a class="link" href="https://muxi1998.github.io/Paper-Blog/">My Paper note site</a>
  </div>
  <a class="link mx-6" href="https://gohugo.io/" rel="noopener" target="_blank"
    >Powered by Hugo️️</a
  >️
  <a
    class="link"
    href="https://github.com/nanxiaobei/hugo-paper"
    rel="noopener"
    target="_blank"
    >✎ Paper</a
  >
</footer>

  </body>
</html>
