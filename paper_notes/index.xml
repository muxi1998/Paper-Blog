<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Paper_notes on My Paper note site</title>
    <link>https://muxi1998.github.io/Paper-Blog/paper_notes/</link>
    <description>Recent content in Paper_notes on My Paper note site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 06 Jan 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://muxi1998.github.io/Paper-Blog/paper_notes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>BLIP</title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/blip/</link>
      <pubDate>Sat, 06 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/blip/</guid>
      <description>Abstract 目前的VLP模型性能提升的方式主要是靠擴大資料集⇒ 目前是以網路上的image-text pair 訓練 網路上的data很大的可能存在noise 此論文提出一個標題產生器和去噪器來處理網路資料集所存在的噪音問題 1. Introduction 🎯 目標： 提出一個更強大的VLP架構&#xA;🕰️ 過去方法&#xA;在過去的VLP(Vision -anguage Pretraining) 方法中有兩大層面的限制 模型層面 針對不同的下游應用目前還是有各自適合的架構，還沒有一個能完全統一個的模型 生成任務（e.g. 文字生成）⇒ Encoder-Decoder 理解任務（e.g. 檢索）⇒ Encoder 資料層面 過去方法的訓練資料多來自網路爬蟲所得，存在noise資料未被清理乾淨，且noise帶來的負面影響尚未被適當解決 💡 本篇方法&#xA;針對上述提到的兩大層面問題進行研究 模型層面 提出一個多模態混合（Multimodal mixture）的Encoder-Decoder架構 (MED) 可以在後續應用在更多的下游任務中 保持預訓練時的效率 資料層面 提出一個引導（Bootstrapping）方法來避免noisy image-text pair Finetune一個pre-trained MED成兩個子模組 Cap (Captioner) ⇒ 生成合成字幕 Filt (Filter)⇒ 過濾掉noisy字幕 🔥 研究成果&#xA;引導字幕可以提升下游任務的效能，且字幕多樣性越高越好 BLIP不僅在Vision-language tasks中有SOTA的效能，在轉移至Video-language tasks中也達到的SOTA且zero-shot的效能 2. Related Work 2.1 Vision-language Pre-training 過去方法的dataset來源多是網路爬蟲，因此存在噪音(noisy)問題，且噪音問題被模型帶來的效果掩蔽 ⇒ 因此提出CapFilt 不同性質的任務背後的backbone會不同 ⇒ 提出多模態混合encoder-decoder understanding-base tasks ⇒ encoder generation-base tasks ⇒ encoder-decoder 2.</description>
    </item>
    <item>
      <title>GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields</title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/giraffe-representing-scenes-as-compositional-generative-neural-feature-fields/</link>
      <pubDate>Mon, 10 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/giraffe-representing-scenes-as-compositional-generative-neural-feature-fields/</guid>
      <description>Abstract 問題情境&#xA;除了能更真實的合成(生成)圖片，還要能讓圖片內容是可以控制的&#xA;過去解法&#xA;研究latent code的解耦合，想辦法透過不同的factor來控制data的變異性 問題發現&#xA;多數研究都專注於2D空間，而忘記我們的世界是3D空間 只有少數的研究著重於場景的構成性質(c nature of scenes) 目前僅能在單個物件場景下才能有較好結果 背景過於複雜或逼真，會導致模型的效果不穩定 提出方法&#xA;讓模型基於一個組合式的3D場景表示來生成物件，來達到圖片合成的可控性。 用神經特徵域來描述場景可以幫助對多物件的解耦合(物件的形狀或外觀) 不需要額外的監督即可透過unstructured和unposed的圖片集合來訓練整個網路 講此3D場景表示法與神經渲染管道結合，可以生成快速且逼真的圖像合成模型 達到成果&#xA;能夠將圖片中的物件個別的解耦合出來，並對該物件進行平移或旋轉，改變相機視角&#xA;1. Introduction 前情提要 生成和操控逼真的圖像內容是電腦視覺領域一直在努力的目標 若要處理3D資訊，會花費大量硬體資源，及人力創建 GAN在近期促進高度逼真圖像合成的進步，可以合成1024*1024高畫質的圖片合成 合成逼真的2D圖片並非唯一目標，對於合成過程也要有簡單、一致的控制方法 許多方法研究如何在未明確監督下，從數據中的學習解耦表示 解耦合的解釋各式各樣，通常是指控制感興趣的屬性(物件形狀、大小、姿勢)，且不影響其他屬性 大多數的方法都沒有考慮到場景的組合性質，都在2D圖片中操作，而忘記我們的世界是3D空間 會導致糾纏的控制機制不是內建的，而是後來才在潛在空間中發現的 在許多應用場景中，3D的合成是很關鍵的，能達到更細緻的合成結果，因此有些研究開始著重於在3D空間上的操作 voxels primitives radiance fields Contribution 提出GIRAFFE，從原始非結構化圖像集訓練出一個可控且逼真的場景生成方法&#xA;提出兩個主要見解：&#xA;將3D場景表示直接合併到生成模型中，可以得到一個更可控的圖像合成 將3D表示法與神經渲染管道相結合，可以實現更快的推理，與生成更逼真的圖像 將場景視為一連串的神經特徵場的組合&#xA;將場景體積渲染到一個較低畫素的特徵圖片 透過神經渲染過程來處理特徵圖，最後輸出最終渲染圖&#xA;2. Related work GAN-based Image Synthesis 目前情況&#xA;已被證實可以成功在高解析度(1024$\times$1024)圖片下進行合成 多個研究如何在不給予額外監督下，也能對不同的factor進行解耦合 修改訓練目標(training objective) 改變網路架構 研究潛在空間 遇到挑戰&#xA;傳統的GAN方法都沒有明確的模擬場景的構成性質，目前開始研究如何對object-level進行控制 創新改變&#xA;撇開在2D空間的思維，將所有運算轉成在3D空間進行 發現能夠達到更好的解耦合且更可控的合成 Implicit Functions 目前情況&#xA;重塑3D幾何結構，目前也使用重構場景 傳統3D的重構是需要大量的人力監督來完成，因此有人提出微分渲染 有人提出重要的NeRFs，可以達到在複雜場景下仍能以不同視角的合成 隱涵神經網路模型 體渲染技術 遇到挑戰</description>
    </item>
    <item>
      <title>Alias-Free Generative Adversarial Networks</title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/stylegan3/</link>
      <pubDate>Wed, 24 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/stylegan3/</guid>
      <description>Abstract 問題發現 圖片合成的過程過度依賴像素座標，導致圖片細節可能黏在座標上而非描繪對象的表面&#xA;解決概念&#xA;將網路中的信息都是為連續的&#xA;解決方法&#xA;小幅度修改原本模型架構，達到普遍適用性高且能避免不需要知資訊洩漏至分層融合的過程&#xA;結果成效&#xA;對動態影像的合成有很大的幫助&#xA;1 Introduction 目前最大問題 在現實中不同尺度的細節往往是分層轉換的，而GAN並沒有做到自然的分層 細節特徵的位置應該繼承較上層粗略的特徵，彼此應該有繼承關係&#xA;猜測與發現 目前的網路會參考隱藏層中洩漏出的位置訊息來畫上細節，但如此的分層結構並不自然 image border&#xA;在進行卷積運算時常常會使用到zero-padding，而此舉動會不經意的洩漏出絕對位置的訊息&#xA;參考資訊 HOW MUCH POSITION INFORMATION DO CONVOLUTIONAL NEURAL NETWORKS ENCODE?&#xA;猜測在CNN各層卷積中隱含著絕對位置的資訊 zero-padding造成模型在無意間學習的特徵的絕對位置 per-pixel noise inputs&#xA;positional encoding&#xA;aliasing&#xA;可能造成混疊問題發生的原因 使用非理想採樣濾波器造成的，像素網格弱化（pixel位置資訊混淆？） Ex: nearest, bilinear, strided卷積 針對各個pixel使用非線性轉換 Ex: ReLU, swish 因為以上兩種可能導致網路會不自覺得將不同尺度的資訊都畫在basis上，而此basis就是螢幕中看到的平面座標 解決方法 去除所有可能的絕對位置參考來源 使得各層卷積的生成能夠更平等，而非受限於洩漏的絕對位置資訊 目前的上採樣filter並沒有很好的抑制混疊 使用low-pass filter來對原本的result再進一步處理來修正pointwise非線性造成的混疊問題 全面檢修StyleGAN2的信息處理 提出了一個更根本不同的圖片生成概念 Equivariance vs. Inquivariance Equivariance 等變性 對於輸入施加一些改變會反映在輸出上 $f(g(x))=g(f(x))$ $f(\cdot):$ 特徵函數 $g(\cdot):$ 變換函數 Inquivariance 不變性 對輸入施加一些改變不會影響輸出 CNN中的equivariant vs.</description>
    </item>
    <item>
      <title>Anomaly Detection Neural Network with Dual Auto-Encoders GAN and Its Industrial Inspection Applications</title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/anomaly-detection-neural-network-with-dual-auto-encoders-gan-and-its-industrial-inspection-applications/</link>
      <pubDate>Wed, 20 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/anomaly-detection-neural-network-with-dual-auto-encoders-gan-and-its-industrial-inspection-applications/</guid>
      <description>Abstract 現在越來越多研究著重於使用深度學習來進行工業上的自動光學檢測，而在使用深度學習方法的過程中遇到其中一個大挑戰是樣本不平均的問題&#xA;因此，此篇論文提出的異常偵測神經網路架構，dual auto-encoder generative adversarial network (DAGAN)，有很好的圖片生成能力以及訓練穩定性。而使用到的三個資料集&#xA;MVTec AD mobile phone screen glass wood defect detection dataset 用來驗證DAGAN的檢測能力，提出skip-connection以及dual auto-encoder架構，並且展現了優秀的圖片重構能力以及訓練穩定度&#xA;結果 在所有測試資料集中的17個類別，其中13個類別都由此論文提出的模型勝出，尤其是在變異性高的資料集中效果更好&#xA;其他優勢&#xA;比U-net有更好的偵測能力 驗證discriminator的重要性 在訓練集少的情況下也有很好的結果 Introduction 使用情境&#xA;近期CNN發展的蓬勃，也協助提升AOI上的發展，在許多影像偵測的應用中CNN也做出許多貢獻，因此現在許多工業上的應用也都借助於CNN的能力來提升AOI的效能&#xA;遇到困難&#xA;在瑕疵偵測的相關應用中很常會碰到不正常樣本不足的情況，儘管目前有多樣的data augmentation方法，但CNN在少樣本下的訓練效果仍有限&#xA;過去方法&#xA;樣本數問題&#xA;使用GAN來生成與原始資料集分佈相似的不正常樣本，因此開始有了以下幾種GAN&#xA;AnoGAN GANomaly Skip-GANomaly GAN生成圖的能力問題 BEGAN 1. 2. CNN 和batch normalization 3. Wassertein loss 4. dual auto-encoder&#xA;提出方法&#xA;GAN-based雙自動編碼器的異常檢測神經網路&#xA;使用公用的工業檢測資料集MCTec AD來與過去的方法比較 使用真實產線中的兩個資料集(Surface glass of mobile phone和wood defect capability)來驗證DCGAN的檢測能力 使用較少的資料集來驗證DAGAN的檢測能力是否被受影響 Related Works 2.1. Generative Adversarial Network (GAN) GAN是一個非監督式學習的神經網路，目標是學習找出一個相似於訓練資料集的機率分佈，來從此分佈中生成不存在的圖，利用generator和discriminator互相競爭的方式來訓練模型</description>
    </item>
    <item>
      <title>HistoGAN: Controlling Colors of GAN-Generated and Real Images via Color Histograms</title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/histogan/</link>
      <pubDate>Wed, 06 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/histogan/</guid>
      <description>方法不局限於同個domain下的顏色轉換，而是單純就color histogram來轉換顏色，而內容保留&#xA;Main idea Scheme 在做圖片的風格轉換時，特別關注於對顏色的控制（可以視為Style transfer的一個sub-category）&#xA;Previous work Problems (motivation) 基於一張風格目標圖做轉換 可能會影響到被轉換的內容細節（紋理、色調） 轉換的品質好壞非常依賴input和target圖片間語意相似性（是否在同一個domain） 想要有好的風格轉換效果就必須在同一個domain下 Method 只藉由color histogram來協助deep network的效果 由此就可以從任意的domain中提取色彩 Result Abstract HistoGAN based on StyleGAN architecture control the GAN-generated images specified by a target color histogram feature ReHistoGAN expend HistoGAN to recolor the generated image encoder unsupervised keep the original images content while changing the colors based on the given target histogram 1. Motivation and Related work 顏色直方圖是個很好用來表達圖片顏色的方法，有許多種不同的顏色直方圖來表達顏色分佈&#xA;3D histogram 2D histogram color palette color triad 動機</description>
    </item>
    <item>
      <title>Style-based Encoder Pre-training for Multi-modal Image Synthesis</title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/step/</link>
      <pubDate>Wed, 06 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/step/</guid>
      <description>Main idea Scheme 對圖片作一對多的樣態轉換&#xA;Problems (motivation) 為了讓模型能將輸入轉成不同樣態，可能採用複雜的模型來記住輸入與不同樣態間的關聯性 （暴力法） Mode collapse問題 遇到沒看過的風格可能轉換的效果就不如預期（模型轉換不同樣態的成果好壞取決於訓練時的訓練資料集） Relative work VAE BicycleGAN MUNIT-p Method 預訓練一個不錯的風格encoder來將輸入的風格encode成latent code 調整並尋找適合的loss function (loss terms越少越好) Result 合成的結果最好 模型成效不依賴目標訓練資料集（泛用度更廣） 更有效更有能力表達風格特徵（latent code），對風格的保真度提高 簡化訓練目標並加快訓練速度 Abstract 過去I2I遇到的問題 輸入輸出的一對多對應關係 Mode collapses problem 過去解決方法 針對多種樣態的輸出使用複雜的模型進行訓練，來因應多樣的輸出範疇 看著結果來訓練 暴力法 本論文的解決方法 強化圖像Encoder的能力（對風格作更進階的分析）來學習更潛在的空間特徵 對輸入本質訓練 本論文的方法概念 將 I2I 轉換的行為分成兩個工作 pre-trained generic style encoder ( proxy task ) 學習圖片的嵌入資訊 任意domain的image → 低維度的風格空間資訊 圖片合成 本論文的優勢 模型不依賴目標訓練資料集（泛用度更廣） 更有效更有能力表達空間特徵，對風格的保真度提高 簡化訓練目標並加快訓練速度 不同loss term對multi-modal I2I的影響 提出VAEs的替代方案來對未受限制的空間特徵進行採樣 跟六個對照組相比結果最好 1. Introduction Image-to-image (I2I) 是一個將圖片從一個domain轉到另外一個domain的任務 e.</description>
    </item>
    <item>
      <title></title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/fuseformer-fusing-fine-grained-information-in-transformers-for-video-inpainting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/fuseformer-fusing-fine-grained-information-in-transformers-for-video-inpainting/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/selective-unsupervised-learning-based-wi-fi-fingerprint-system-using-autoencoder-and-gan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/selective-unsupervised-learning-based-wi-fi-fingerprint-system-using-autoencoder-and-gan/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/self-attention-temporal-convolutional-network-for-long-term-daily-living-activity-detection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/self-attention-temporal-convolutional-network-for-long-term-daily-living-activity-detection/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/sttn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/sttn/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/stylegan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/stylegan/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/temporal_group_fusion_network_dvi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/temporal_group_fusion_network_dvi/</guid>
      <description></description>
    </item>
    <item>
      <title>Deep Image Compositing</title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/deep-image-compositing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/deep-image-compositing/</guid>
      <description>Question List&#xA;Abstract 研究情境&#xA;合成照片(常使用於替換背景)，將一個肖像圖合成在其他背景中 並預期做到合成品質更好（邊界不模糊）的圖&#xA;過去問題&#xA;耗時，為獲得高品質的合成結果，在使用複雜的工具時通常要經過許多步驟才能將照片合成，門檻高 分割 去背 前景去汙 光暈 (合成邊界明顯) 前景汙染 提出方法&#xA;不需要其他使用者輸入資料即可生成高品質的合成照片 進行End-to-end訓練，優化上下文及顏色信息的使用 引入自學策略，由易至難逐步訓練，減輕資料集不足對訓練的影響 [[Note]Laplacian pyramid blending](https://www.notion.so/Note-Laplacian-pyramid-blending-6938543340bb46218cafc61d63d79079?pvs=21)&#xA;實驗結果&#xA;可自動生成高品質的合成照片&#xA;比已知的所有方法擁有更好的質與量&#xA;質哪裡比較好?&#xA;邊界幾乎沒有偽影，細節也比較清楚，從PSNR測試及使用者測試中可以得到品質比較好的結論&#xA;量哪裡比較好?&#xA;訓練集的量因為有自己發明的data augmentation演算法，因此訓練資料較多 1.Introduction 研究動機&#xA;過去在合成圖片時，若想得到高品質的合成結果通常耗時耗力且需要一定的經驗與技術（門檻高），因此此研究提出一個全自動且高品質的合成機制&#xA;過去經驗&#xA;採用salient object segmentation model分割前景，會有偽影的問題&#xA;Why偽影？&#xA;因為在邊界融合時使用較低階的融合方法 e.g.Poisson融合，Laplacian融合，羽化&#xA;直覺的copy-past不再適用（邊界問題）&#xA;GT mask 概念&#xA;因此有人提出從前景中提取物件遮罩的概念，著重在alpha channel（透明度），即為Ground truth matte&#xA;利用GT matte訓練一個可以預測image matte的模型&#xA;若模型預測極盡正確的物件遮罩，就可以協助合成圖片&#xA;缺點&#xA;需要人為準備training data（trimap: 前景、後景、不確定區域） 儘管有高品質的物件遮罩，在合成圖片還是有光暈問題 Harmonization 概念 改善前後景融合的交界顏色 缺點 需要使用者先提供無光暈的完美物件遮罩才能進行 提出解法&#xA;全自動的end-to-end深度學習模型&#xA;新的multi-stream fusion模型，可以融合不同尺度的圖片&#xA;在提取肖像遮罩時使用兩個network&#xA;Foreground segmentation network refinement network 易至難的data augmentation來建立自學合成的機制</description>
    </item>
    <item>
      <title>Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding</title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/zits/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/zits/</guid>
      <description>Abstract 1. Introduction Inpainting的重點 coherent texture visually reasonable structures 過去的Inpainting方法有4大問題 感受野過小：CNN kernel的關係，就算是dilated CNN也會在在過大的毀損區域或高解析度的情境下降低效能 遺失整體的結構：若模型不了解整體的結構，則很難還原細節 計算量重：訓練GAN對大影像的還原是困難且耗費資源的，且效能會隨著影像解析度提高而降低 遮罩區域沒有位置資訊：模型容易在遺失區域產生偽影 2. Related work 3. Method 簡寫說明 ZITS : ZeroRA based Incremental Transformer Structure MPE : Masking Positional Encoding TSR : Transformer Structure Restorer FTR : Fourier CNN Texture Restoration Overview Structure restoration 輸入為$TSR(I_m, I_e, I_l, M)$ $I_m:$ 被遮罩的輸入影像 $I_e:$ 邊緣資訊 $I_l:$ 線條資訊 $M:$ 二元mask 輸出為草圖域 $[\tilde{I_e}, \tilde{I_l}]=TSR(I_m, I_e, I_l, M)$ Simple Structure upsample 在推論期間會使用SSU來將灰階的草圖上採樣至任意大小 Extract structure feature 在使用一個gated convolution based SFE來提取多尺度的特徵 $S_k=SFE(\tilde{I_e}, \tilde{I_l},M), {k=0,1,2,3}$ Add to to the fourier CNN texture restoration 將$S_k$資訊逐步加入(ZeroRA)相對應的FTR layer中 3.</description>
    </item>
  </channel>
</rss>
