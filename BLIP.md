<!-- # Outline
- [Abstract](#abstract)
- [1. Introduction](#1-introduction)
- [2. Related Work](#2-related-work)
  - [2.1 Vision-language Pre-training](#21-vision-language-pre-training)
  - [2.2 Knowledge Distillation](#22-knowledge-distillation)
  - [2.3 Data Augmentation](#23-data-augmentation)
- [3. Method](#3-method)
  - [3.1 Model Architecture](#31-model-architecture)
  - [3.2 Pre-training Objectives](#32-pre-training-objectives)
  - [3.3 CapFilt](#33-capfilt)
- [4. Experiments](#4-experiments)
  - [4.1 Pre-training Details](#41-pre-training-details)
  - [4.2 Effect of CapFilt](#42-effect-of-capfilt)
  - [4.3 Diversity is Key for Synthetic Captions](#43-diversity-is-key-for-synthetic-captions)
  - [4.4 Parameter Sharing and Decoupling](#44-parameter-sharing-and-decoupling)
- [5. Comparison with State-of-the-arts](#5--comparison-with-state-of-the-arts)
  - [5.1 Image-Text Retrieval](#51-image-text-retrieval)
  - [5.2 Image Captioning](#52-image-captioning)
  - [5.3 Visual Question Answering (VQA)](#53-visal-question-answering-vqa)
  - [5.4 Natural Language Visual Reasoning (NLVR2)](#54-natural-language-visual-reasoning-nlvr2)
  - [5.5 Visual Dialog (VisDial)](#55-visual-dialog-visdial)
  - [5.6 Zero-shot Transfer to Video-Language Tasks](#56-zero-shot-transfer-to-video-language-tasks)
- [6. Additional Ablation Study](#6-additional-ablation-study)
- [7. Conclusion](#7-conclusion)
---
--- -->

# Abstract
- ç›®å‰çš„VLPæ¨¡å‹æ€§èƒ½æå‡çš„æ–¹å¼ä¸»è¦æ˜¯é æ“´å¤§è³‡æ–™é›†â‡’ ç›®å‰æ˜¯ä»¥ç¶²è·¯ä¸Šçš„image-text pair è¨“ç·´
    - ç¶²è·¯ä¸Šçš„dataå¾ˆå¤§çš„å¯èƒ½å­˜åœ¨noise
    - æ­¤è«–æ–‡æå‡ºä¸€å€‹**æ¨™é¡Œç”¢ç”Ÿå™¨**å’Œ**å»å™ªå™¨**ä¾†è™•ç†ç¶²è·¯è³‡æ–™é›†æ‰€å­˜åœ¨çš„å™ªéŸ³å•é¡Œ

# 1. Introduction

ğŸ¯ **ç›®æ¨™ï¼š** æå‡ºä¸€å€‹æ›´å¼·å¤§çš„VLPæ¶æ§‹

ğŸ•°ï¸ **éå»æ–¹æ³•**
- åœ¨éå»çš„VLP(Vision -anguage Pretraining) æ–¹æ³•ä¸­æœ‰å…©å¤§å±¤é¢çš„é™åˆ¶
    - æ¨¡å‹å±¤é¢
        - é‡å°ä¸åŒçš„ä¸‹æ¸¸æ‡‰ç”¨ç›®å‰é‚„æ˜¯æœ‰å„è‡ªé©åˆçš„æ¶æ§‹ï¼Œé‚„æ²’æœ‰ä¸€å€‹èƒ½å®Œå…¨çµ±ä¸€å€‹çš„æ¨¡å‹
            - ç”Ÿæˆä»»å‹™ï¼ˆe.g. æ–‡å­—ç”Ÿæˆï¼‰â‡’ Encoder-Decoder
            - ç†è§£ä»»å‹™ï¼ˆe.g. æª¢ç´¢ï¼‰â‡’ Encoder
    - è³‡æ–™å±¤é¢
        - éå»æ–¹æ³•çš„è¨“ç·´è³‡æ–™å¤šä¾†è‡ªç¶²è·¯çˆ¬èŸ²æ‰€å¾—ï¼Œå­˜åœ¨noiseè³‡æ–™æœªè¢«æ¸…ç†ä¹¾æ·¨ï¼Œä¸”noiseå¸¶ä¾†çš„è² é¢å½±éŸ¿å°šæœªè¢«é©ç•¶è§£æ±º

ğŸ’¡ **æœ¬ç¯‡æ–¹æ³•**

- é‡å°ä¸Šè¿°æåˆ°çš„å…©å¤§å±¤é¢å•é¡Œé€²è¡Œç ”ç©¶
    - æ¨¡å‹å±¤é¢
        - æå‡ºä¸€å€‹å¤šæ¨¡æ…‹æ··åˆï¼ˆMultimodal mixtureï¼‰çš„Encoder-Decoderæ¶æ§‹ (MED)
            - å¯ä»¥åœ¨å¾ŒçºŒæ‡‰ç”¨åœ¨æ›´å¤šçš„ä¸‹æ¸¸ä»»å‹™ä¸­
            - ä¿æŒé è¨“ç·´æ™‚çš„æ•ˆç‡
    - è³‡æ–™å±¤é¢
        - æå‡ºä¸€å€‹å¼•å°ï¼ˆBootstrappingï¼‰æ–¹æ³•ä¾†é¿å…noisy image-text pair
        - Finetuneä¸€å€‹pre-trained MEDæˆå…©å€‹å­æ¨¡çµ„
            - Cap (Captioner) â‡’ ç”Ÿæˆåˆæˆå­—å¹•
            - Filt (Filter)â‡’ éæ¿¾æ‰noisyå­—å¹•

ğŸ”¥ **ç ”ç©¶æˆæœ**

1. å¼•å°å­—å¹•å¯ä»¥æå‡ä¸‹æ¸¸ä»»å‹™çš„æ•ˆèƒ½ï¼Œä¸”å­—å¹•å¤šæ¨£æ€§è¶Šé«˜è¶Šå¥½
2. BLIPä¸åƒ…åœ¨Vision-language tasksä¸­æœ‰SOTAçš„æ•ˆèƒ½ï¼Œåœ¨è½‰ç§»è‡³Video-language tasksä¸­ä¹Ÿé”åˆ°çš„SOTAä¸”zero-shotçš„æ•ˆèƒ½

# 2. Related Work

## 2.1 Vision-language Pre-training
- éå»æ–¹æ³•çš„datasetä¾†æºå¤šæ˜¯ç¶²è·¯çˆ¬èŸ²ï¼Œå› æ­¤å­˜åœ¨å™ªéŸ³(noisy)å•é¡Œï¼Œä¸”å™ªéŸ³å•é¡Œè¢«æ¨¡å‹å¸¶ä¾†çš„æ•ˆæœæ©è”½ â‡’ å› æ­¤æå‡º**CapFilt**
- ä¸åŒæ€§è³ªçš„ä»»å‹™èƒŒå¾Œçš„backboneæœƒä¸åŒ â‡’ æå‡º**å¤šæ¨¡æ…‹æ··åˆencoder-decoder**
    - understanding-base tasks â‡’ encoder
    - generation-base tasks â‡’ encoder-decoder

## 2.2 Knowledge Distillation
- CapFiltæ¨¡çµ„é¡ä¼¼æ–¼student-teacheræ–¹æ³•ï¼ŒCaptioneré€éç”Ÿæˆç”¢ç”Ÿå­—å¹•ä¾†å­¸ç¿’èªæ„ï¼ŒFilteré€ééæ¿¾é›œè¨Šä¾†å­¸ç¿’èªæ„
- CapFiltå¯ä»¥ç›¸è¼”ç›¸æˆ

## 2.3 Data Augmentation
- èªè¨€ä»»å‹™ä¸­çš„DA(Data Augmentation)ç›¸è¼ƒæ–¼vision tasksè¼ƒç‚ºå›°é›£
- æœ¬ç¯‡ç ”ç©¶å±•ç¾äº†åˆæˆå­—å¹•å°æ–¼å¤§è¦æ¨¡çš„vision-language pre-trainingçš„æˆæ•ˆæ˜¯ä¸éŒ¯çš„

# 3. Method

## 3.1 Model Architecture
**MED** (Multimodal mixture encoder-decoder) æ˜¯ä¸€å€‹multi-task modalï¼Œä¸¦å¯ä»¥æä¾›ä»¥ä¸‹<span style="color:red">**ä¸‰ç¨®åŠŸèƒ½**</span>

1. Unimodal encoder
    1. Image Encoder
        - Transformer is better than the object detection model in feature extraction propose
        - <details>
            <summary>employ <b>Vision Transformer</b></summary>

            ![Vision Transformer Image](./BLIP/model.png)
            </details>

            
    2. Text Encoder
        - <details>
            <summary>employ <b>BERT</b></summary>

            ![BERT Image](./BLIP/BERT.png)
            </details>
            
2. Image-grounded text encoder
    - Inject **visual information** for  the cross-attention(CA)
    - Use specific task-specific **token [Encode]** appended to the input text to show the purpose is to generate the representation of the image-text pair
3. Image-grounded text decoder
    - Bi-directional attention â‡’ (change to) **causal self-attention**
    - token [Decode] is used to signal the beginning of the sequence
    - end-of-sequence token is used to signal the end

## 3.2 Pre-training Objectives
- **Three** objectives
    - Understanding-based (x2)
    - Generation-based (x1)
- Computation flow
    - vision transformer (ViT): one-pass to save the computation loading
    - text transformer (BERT): three-pass
- **Three Losses**
    - Image-Text Contrastive Loss (ITC) â‡’ Align the representation of vision and text
        - A positive image-text pair should have similar **representation** between image feature and text feature
        - Negative image-text pair should have more different representation
    - Image-Text Matching Loss (ITM) â‡’ Distinguish whether the image-text pair is positive or negative
        - binary classification problem
        - Purpose is to check whether the image and text are matched
    - Language Modeling Loss (LM) â‡’ generate textual description given an image
        - cross entropy loss: maximize the likelihood of the text in an autoregressive manner
- Tricks of Minimizing the training computation
    - Share Weights between text encoder and text decoder, except **SA layers**
    - Main components that makes the encoder and decoder different is the <attention layer>

## 3.3 CapFilt
- **Previous Problem:** limited number of high-quality human annotated image-text pairs $\{(I_h, T_h)\}$
    - e.g. COCO dataset
- **Previous solution and limitation:** Crawl image and alt-text pairs from the website
    - often do not accurately describe the visual content â‡’ **noisy data**
- **Proposed solution:** Finetune CapFilt on high-quality annotated image-text pair (e.g. COCO dataset)
    - Cap (Captioner)
        - finetune with **LM loss** to decode (synthesis) texts $T_s$ of given web images $I_w$
        - $Cap(I_w)=T_s$
    - Filt (Filter)
        - finetune with **ITC** and **ITM loss** to learn whether a text matches an image
        - a text is considered to be noisy if the ITM predicts the input text and image pair is unmatched.
    - Finally combine the filtered image-text pairs with the human-annotated pairs to form a new dataset

# 4. Experiments

## 4.1 Pre-training Details

## 4.2 Effect of CapFilt

## 4.3 Diversity is Key for Synthetic Captions

## 4.4 Parameter Sharing and Decoupling

# 5.  Comparison with State-of-the-arts

## 5.1 Image-Text Retrieval

## 5.2 Image Captioning

## 5.3 Visal Question Answering (VQA)

## 5.4 Natural Language Visual Reasoning (NLVR2)

## 5.5 Visual Dialog (VisDial)

## 5.6 Zero-shot Transfer to Video-Language Tasks

# 6. Additional Ablation Study

# 7. Conclusion