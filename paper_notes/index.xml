<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Paper_notes on My Paper note site</title>
    <link>https://muxi1998.github.io/Paper-Blog/paper_notes/</link>
    <description>Recent content in Paper_notes on My Paper note site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 06 Jan 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://muxi1998.github.io/Paper-Blog/paper_notes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>BLIP</title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/blip/</link>
      <pubDate>Sat, 06 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/blip/</guid>
      <description>Abstract 目前的VLP模型性能提升的方式主要是靠擴大資料集⇒ 目前是以網路上的image-text pair 訓練 網路上的data很大的可能存在noise 此論文提出一個標題產生器和去噪器來處理網路資料集所存在的噪音問題 1. Introduction 🎯 目標： 提出一個更強大的VLP架構&#xA;🕰️ 過去方法&#xA;在過去的VLP(Vision -anguage Pretraining) 方法中有兩大層面的限制 模型層面 針對不同的下游應用目前還是有各自適合的架構，還沒有一個能完全統一個的模型 生成任務（e.g. 文字生成）⇒ Encoder-Decoder 理解任務（e.g. 檢索）⇒ Encoder 資料層面 過去方法的訓練資料多來自網路爬蟲所得，存在noise資料未被清理乾淨，且noise帶來的負面影響尚未被適當解決 💡 本篇方法&#xA;針對上述提到的兩大層面問題進行研究 模型層面 提出一個多模態混合（Multimodal mixture）的Encoder-Decoder架構 (MED) 可以在後續應用在更多的下游任務中 保持預訓練時的效率 資料層面 提出一個引導（Bootstrapping）方法來避免noisy image-text pair Finetune一個pre-trained MED成兩個子模組 Cap (Captioner) ⇒ 生成合成字幕 Filt (Filter)⇒ 過濾掉noisy字幕 🔥 研究成果&#xA;引導字幕可以提升下游任務的效能，且字幕多樣性越高越好 BLIP不僅在Vision-language tasks中有SOTA的效能，在轉移至Video-language tasks中也達到的SOTA且zero-shot的效能 2. Related Work 2.1 Vision-language Pre-training 過去方法的dataset來源多是網路爬蟲，因此存在噪音(noisy)問題，且噪音問題被模型帶來的效果掩蔽 ⇒ 因此提出CapFilt 不同性質的任務背後的backbone會不同 ⇒ 提出多模態混合encoder-decoder understanding-base tasks ⇒ encoder generation-base tasks ⇒ encoder-decoder 2.</description>
    </item>
    <item>
      <title>GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields</title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/giraffe-representing-scenes-as-compositional-generative-neural-feature-fields/</link>
      <pubDate>Mon, 10 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/giraffe-representing-scenes-as-compositional-generative-neural-feature-fields/</guid>
      <description>Abstract 問題情境&#xA;除了能更真實的合成(生成)圖片，還要能讓圖片內容是可以控制的&#xA;過去解法&#xA;研究latent code的解耦合，想辦法透過不同的factor來控制data的變異性 問題發現&#xA;多數研究都專注於2D空間，而忘記我們的世界是3D空間 只有少數的研究著重於場景的構成性質(c nature of scenes) 目前僅能在單個物件場景下才能有較好結果 背景過於複雜或逼真，會導致模型的效果不穩定 提出方法&#xA;讓模型基於一個組合式的3D場景表示來生成物件，來達到圖片合成的可控性。 用神經特徵域來描述場景可以幫助對多物件的解耦合(物件的形狀或外觀) 不需要額外的監督即可透過unstructured和unposed的圖片集合來訓練整個網路 講此3D場景表示法與神經渲染管道結合，可以生成快速且逼真的圖像合成模型 達到成果&#xA;能夠將圖片中的物件個別的解耦合出來，並對該物件進行平移或旋轉，改變相機視角&#xA;1. Introduction 前情提要 生成和操控逼真的圖像內容是電腦視覺領域一直在努力的目標 若要處理3D資訊，會花費大量硬體資源，及人力創建 GAN在近期促進高度逼真圖像合成的進步，可以合成1024*1024高畫質的圖片合成 合成逼真的2D圖片並非唯一目標，對於合成過程也要有簡單、一致的控制方法 許多方法研究如何在未明確監督下，從數據中的學習解耦表示 解耦合的解釋各式各樣，通常是指控制感興趣的屬性(物件形狀、大小、姿勢)，且不影響其他屬性 大多數的方法都沒有考慮到場景的組合性質，都在2D圖片中操作，而忘記我們的世界是3D空間 會導致糾纏的控制機制不是內建的，而是後來才在潛在空間中發現的 在許多應用場景中，3D的合成是很關鍵的，能達到更細緻的合成結果，因此有些研究開始著重於在3D空間上的操作 voxels primitives radiance fields Contribution 提出GIRAFFE，從原始非結構化圖像集訓練出一個可控且逼真的場景生成方法&#xA;提出兩個主要見解：&#xA;將3D場景表示直接合併到生成模型中，可以得到一個更可控的圖像合成 將3D表示法與神經渲染管道相結合，可以實現更快的推理，與生成更逼真的圖像 將場景視為一連串的神經特徵場的組合&#xA;將場景體積渲染到一個較低畫素的特徵圖片 透過神經渲染過程來處理特徵圖，最後輸出最終渲染圖&#xA;2. Related work GAN-based Image Synthesis 目前情況&#xA;已被證實可以成功在高解析度(1024$\times$1024)圖片下進行合成 多個研究如何在不給予額外監督下，也能對不同的factor進行解耦合 修改訓練目標(training objective) 改變網路架構 研究潛在空間 遇到挑戰&#xA;傳統的GAN方法都沒有明確的模擬場景的構成性質，目前開始研究如何對object-level進行控制 創新改變&#xA;撇開在2D空間的思維，將所有運算轉成在3D空間進行 發現能夠達到更好的解耦合且更可控的合成 Implicit Functions 目前情況&#xA;重塑3D幾何結構，目前也使用重構場景 傳統3D的重構是需要大量的人力監督來完成，因此有人提出微分渲染 有人提出重要的NeRFs，可以達到在複雜場景下仍能以不同視角的合成 隱涵神經網路模型 體渲染技術 遇到挑戰</description>
    </item>
    <item>
      <title>Alias-Free Generative Adversarial Networks</title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/stylegan3/</link>
      <pubDate>Wed, 24 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/stylegan3/</guid>
      <description>Abstract 問題發現 圖片合成的過程過度依賴像素座標，導致圖片細節可能黏在座標上而非描繪對象的表面&#xA;解決概念&#xA;將網路中的信息都是為連續的&#xA;解決方法&#xA;小幅度修改原本模型架構，達到普遍適用性高且能避免不需要知資訊洩漏至分層融合的過程&#xA;結果成效&#xA;對動態影像的合成有很大的幫助&#xA;1 Introduction 目前最大問題 在現實中不同尺度的細節往往是分層轉換的，而GAN並沒有做到自然的分層 細節特徵的位置應該繼承較上層粗略的特徵，彼此應該有繼承關係&#xA;猜測與發現 目前的網路會參考隱藏層中洩漏出的位置訊息來畫上細節，但如此的分層結構並不自然 image border&#xA;在進行卷積運算時常常會使用到zero-padding，而此舉動會不經意的洩漏出絕對位置的訊息&#xA;參考資訊 HOW MUCH POSITION INFORMATION DO CONVOLUTIONAL NEURAL NETWORKS ENCODE?&#xA;猜測在CNN各層卷積中隱含著絕對位置的資訊 zero-padding造成模型在無意間學習的特徵的絕對位置 per-pixel noise inputs&#xA;positional encoding&#xA;aliasing&#xA;可能造成混疊問題發生的原因 使用非理想採樣濾波器造成的，像素網格弱化（pixel位置資訊混淆？） Ex: nearest, bilinear, strided卷積 針對各個pixel使用非線性轉換 Ex: ReLU, swish 因為以上兩種可能導致網路會不自覺得將不同尺度的資訊都畫在basis上，而此basis就是螢幕中看到的平面座標 解決方法 去除所有可能的絕對位置參考來源 使得各層卷積的生成能夠更平等，而非受限於洩漏的絕對位置資訊 目前的上採樣filter並沒有很好的抑制混疊 使用low-pass filter來對原本的result再進一步處理來修正pointwise非線性造成的混疊問題 全面檢修StyleGAN2的信息處理 提出了一個更根本不同的圖片生成概念 Equivariance vs. Inquivariance Equivariance 等變性 對於輸入施加一些改變會反映在輸出上 $f(g(x))=g(f(x))$ $f(\cdot):$ 特徵函數 $g(\cdot):$ 變換函數 Inquivariance 不變性 對輸入施加一些改變不會影響輸出 CNN中的equivariant vs.</description>
    </item>
    <item>
      <title>A Style-Based Generator Architecture for Generative Adversarial Networks</title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/stylegan/</link>
      <pubDate>Wed, 10 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/stylegan/</guid>
      <description>Abstract 此篇論文提出一個新的GAN架構，可以達到以下目的 自動且非監督式學習將高階的特徵（動作和身份）分開 利用隨機噪聲達到生成圖片的隨機變化(stochastic variation) Ex: 雀斑位置、頭髮文理 更直觀、特定尺度的合成 其他成果： disentangle latent factors，達到更好的interpolation 提供兩種量化糾纏基準的指標 提供一個高畫質且高多樣性的人臉資料集 1. Introduction 過去在使用GAN生成圖偏食會面臨到以下問題 整體過程像是黑盒子，對合成過程不了解（隨機特徵的起源） 對於latent space的特性不熟悉 沒有一個量化基準來比較不同GAN生成圖片的interpolation 提出方法： 先進的方式來控制圖片合成的流程，從Style的latent code修改各層卷積的資訊，達到在不同尺度下修改風格，同時另外加入噪聲增加隨機性 透過將input latent code映射到intermediate latent space，可以相對有效的disentanglement 提出兩個量化disentanglement的基準 （幫助比較不同GAN的成效） perceptual path length linear separability 其他貢獻：&#xA;FFHQ資料集（Flickr-Faces-HQ） 高品質 多樣性廣 2. Style-based generator 與過往的GAN不同，用一個learned const來取代傳統的input layer&#xA;過去的latent code只有出現在輸入層，而StyleGAN的input latent code可以輸入至各層卷積&#xA;StyleGAN將input latent code映射到中介latent space $W$&#xA;$W$透過AdaIN來控制各層卷積&#xA;AdaIN 笔记&#xA;在卷積運算後會加入高斯雜訊&#xA;架構概述&#xA;A 是一個預先學習好的仿射轉換 B 是一個預先學習好的noise scaling機制 mapping網路 8層全連接卷積 合成網路 $g$ 18層 $4^2-1024^2$的解析度各兩層 輸出層是用$1\times1$卷積來將圖片轉換成RGB 比起其他風格轉換方法，風格資訊只從輸入層放入，StyleGAN是將紑格的latent code $w$轉換到各層卷積</description>
    </item>
    <item>
      <title>Anomaly Detection Neural Network with Dual Auto-Encoders GAN and Its Industrial Inspection Applications</title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/anomaly-detection-neural-network-with-dual-auto-encoders-gan-and-its-industrial-inspection-applications/</link>
      <pubDate>Wed, 20 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/anomaly-detection-neural-network-with-dual-auto-encoders-gan-and-its-industrial-inspection-applications/</guid>
      <description>Abstract 現在越來越多研究著重於使用深度學習來進行工業上的自動光學檢測，而在使用深度學習方法的過程中遇到其中一個大挑戰是樣本不平均的問題&#xA;因此，此篇論文提出的異常偵測神經網路架構，dual auto-encoder generative adversarial network (DAGAN)，有很好的圖片生成能力以及訓練穩定性。而使用到的三個資料集&#xA;MVTec AD mobile phone screen glass wood defect detection dataset 用來驗證DAGAN的檢測能力，提出skip-connection以及dual auto-encoder架構，並且展現了優秀的圖片重構能力以及訓練穩定度&#xA;結果 在所有測試資料集中的17個類別，其中13個類別都由此論文提出的模型勝出，尤其是在變異性高的資料集中效果更好&#xA;其他優勢&#xA;比U-net有更好的偵測能力 驗證discriminator的重要性 在訓練集少的情況下也有很好的結果 Introduction 使用情境&#xA;近期CNN發展的蓬勃，也協助提升AOI上的發展，在許多影像偵測的應用中CNN也做出許多貢獻，因此現在許多工業上的應用也都借助於CNN的能力來提升AOI的效能&#xA;遇到困難&#xA;在瑕疵偵測的相關應用中很常會碰到不正常樣本不足的情況，儘管目前有多樣的data augmentation方法，但CNN在少樣本下的訓練效果仍有限&#xA;過去方法&#xA;樣本數問題&#xA;使用GAN來生成與原始資料集分佈相似的不正常樣本，因此開始有了以下幾種GAN&#xA;AnoGAN GANomaly Skip-GANomaly GAN生成圖的能力問題 BEGAN 1. 2. CNN 和batch normalization 3. Wassertein loss 4. dual auto-encoder&#xA;提出方法&#xA;GAN-based雙自動編碼器的異常檢測神經網路&#xA;使用公用的工業檢測資料集MCTec AD來與過去的方法比較 使用真實產線中的兩個資料集(Surface glass of mobile phone和wood defect capability)來驗證DCGAN的檢測能力 使用較少的資料集來驗證DAGAN的檢測能力是否被受影響 Related Works 2.1. Generative Adversarial Network (GAN) GAN是一個非監督式學習的神經網路，目標是學習找出一個相似於訓練資料集的機率分佈，來從此分佈中生成不存在的圖，利用generator和discriminator互相競爭的方式來訓練模型</description>
    </item>
    <item>
      <title>HistoGAN: Controlling Colors of GAN-Generated and Real Images via Color Histograms</title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/histogan/</link>
      <pubDate>Wed, 06 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/histogan/</guid>
      <description>方法不局限於同個domain下的顏色轉換，而是單純就color histogram來轉換顏色，而內容保留&#xA;Main idea Scheme 在做圖片的風格轉換時，特別關注於對顏色的控制（可以視為Style transfer的一個sub-category）&#xA;Previous work Problems (motivation) 基於一張風格目標圖做轉換 可能會影響到被轉換的內容細節（紋理、色調） 轉換的品質好壞非常依賴input和target圖片間語意相似性（是否在同一個domain） 想要有好的風格轉換效果就必須在同一個domain下 Method 只藉由color histogram來協助deep network的效果 由此就可以從任意的domain中提取色彩 Result Abstract HistoGAN based on StyleGAN architecture control the GAN-generated images specified by a target color histogram feature ReHistoGAN expend HistoGAN to recolor the generated image encoder unsupervised keep the original images content while changing the colors based on the given target histogram 1. Motivation and Related work 顏色直方圖是個很好用來表達圖片顏色的方法，有許多種不同的顏色直方圖來表達顏色分佈&#xA;3D histogram 2D histogram color palette color triad 動機</description>
    </item>
    <item>
      <title>Style-based Encoder Pre-training for Multi-modal Image Synthesis</title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/step/</link>
      <pubDate>Wed, 06 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/step/</guid>
      <description>Main idea Scheme 對圖片作一對多的樣態轉換&#xA;Problems (motivation) 為了讓模型能將輸入轉成不同樣態，可能採用複雜的模型來記住輸入與不同樣態間的關聯性 （暴力法） Mode collapse問題 遇到沒看過的風格可能轉換的效果就不如預期（模型轉換不同樣態的成果好壞取決於訓練時的訓練資料集） Relative work VAE BicycleGAN MUNIT-p Method 預訓練一個不錯的風格encoder來將輸入的風格encode成latent code 調整並尋找適合的loss function (loss terms越少越好) Result 合成的結果最好 模型成效不依賴目標訓練資料集（泛用度更廣） 更有效更有能力表達風格特徵（latent code），對風格的保真度提高 簡化訓練目標並加快訓練速度 Abstract 過去I2I遇到的問題 輸入輸出的一對多對應關係 Mode collapses problem 過去解決方法 針對多種樣態的輸出使用複雜的模型進行訓練，來因應多樣的輸出範疇 看著結果來訓練 暴力法 本論文的解決方法 強化圖像Encoder的能力（對風格作更進階的分析）來學習更潛在的空間特徵 對輸入本質訓練 本論文的方法概念 將 I2I 轉換的行為分成兩個工作 pre-trained generic style encoder ( proxy task ) 學習圖片的嵌入資訊 任意domain的image → 低維度的風格空間資訊 圖片合成 本論文的優勢 模型不依賴目標訓練資料集（泛用度更廣） 更有效更有能力表達空間特徵，對風格的保真度提高 簡化訓練目標並加快訓練速度 不同loss term對multi-modal I2I的影響 提出VAEs的替代方案來對未受限制的空間特徵進行採樣 跟六個對照組相比結果最好 1. Introduction Image-to-image (I2I) 是一個將圖片從一個domain轉到另外一個domain的任務 e.</description>
    </item>
    <item>
      <title></title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/fuseformer-fusing-fine-grained-information-in-transformers-for-video-inpainting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/fuseformer-fusing-fine-grained-information-in-transformers-for-video-inpainting/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/self-attention-temporal-convolutional-network-for-long-term-daily-living-activity-detection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/self-attention-temporal-convolutional-network-for-long-term-daily-living-activity-detection/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/sttn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/sttn/</guid>
      <description></description>
    </item>
    <item>
      <title>Deep Image Compositing</title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/deep-image-compositing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/deep-image-compositing/</guid>
      <description>Question List&#xA;Abstract 研究情境&#xA;合成照片(常使用於替換背景)，將一個肖像圖合成在其他背景中 並預期做到合成品質更好（邊界不模糊）的圖&#xA;過去問題&#xA;耗時，為獲得高品質的合成結果，在使用複雜的工具時通常要經過許多步驟才能將照片合成，門檻高 分割 去背 前景去汙 光暈 (合成邊界明顯) 前景汙染 提出方法&#xA;不需要其他使用者輸入資料即可生成高品質的合成照片 進行End-to-end訓練，優化上下文及顏色信息的使用 引入自學策略，由易至難逐步訓練，減輕資料集不足對訓練的影響 [[Note]Laplacian pyramid blending](https://www.notion.so/Note-Laplacian-pyramid-blending-6938543340bb46218cafc61d63d79079?pvs=21)&#xA;實驗結果&#xA;可自動生成高品質的合成照片&#xA;比已知的所有方法擁有更好的質與量&#xA;質哪裡比較好?&#xA;邊界幾乎沒有偽影，細節也比較清楚，從PSNR測試及使用者測試中可以得到品質比較好的結論&#xA;量哪裡比較好?&#xA;訓練集的量因為有自己發明的data augmentation演算法，因此訓練資料較多 1.Introduction 研究動機&#xA;過去在合成圖片時，若想得到高品質的合成結果通常耗時耗力且需要一定的經驗與技術（門檻高），因此此研究提出一個全自動且高品質的合成機制&#xA;過去經驗&#xA;採用salient object segmentation model分割前景，會有偽影的問題&#xA;Why偽影？&#xA;因為在邊界融合時使用較低階的融合方法 e.g.Poisson融合，Laplacian融合，羽化&#xA;直覺的copy-past不再適用（邊界問題）&#xA;GT mask 概念&#xA;因此有人提出從前景中提取物件遮罩的概念，著重在alpha channel（透明度），即為Ground truth matte&#xA;利用GT matte訓練一個可以預測image matte的模型&#xA;若模型預測極盡正確的物件遮罩，就可以協助合成圖片&#xA;缺點&#xA;需要人為準備training data（trimap: 前景、後景、不確定區域） 儘管有高品質的物件遮罩，在合成圖片還是有光暈問題 Harmonization 概念 改善前後景融合的交界顏色 缺點 需要使用者先提供無光暈的完美物件遮罩才能進行 提出解法&#xA;全自動的end-to-end深度學習模型&#xA;新的multi-stream fusion模型，可以融合不同尺度的圖片&#xA;在提取肖像遮罩時使用兩個network&#xA;Foreground segmentation network refinement network 易至難的data augmentation來建立自學合成的機制</description>
    </item>
    <item>
      <title>Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding</title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/zits/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/zits/</guid>
      <description>Abstract 1. Introduction Inpainting的重點 coherent texture visually reasonable structures 過去的Inpainting方法有4大問題 感受野過小：CNN kernel的關係，就算是dilated CNN也會在在過大的毀損區域或高解析度的情境下降低效能 遺失整體的結構：若模型不了解整體的結構，則很難還原細節 計算量重：訓練GAN對大影像的還原是困難且耗費資源的，且效能會隨著影像解析度提高而降低 遮罩區域沒有位置資訊：模型容易在遺失區域產生偽影 2. Related work 3. Method 簡寫說明 ZITS : ZeroRA based Incremental Transformer Structure MPE : Masking Positional Encoding TSR : Transformer Structure Restorer FTR : Fourier CNN Texture Restoration Overview Structure restoration 輸入為$TSR(I_m, I_e, I_l, M)$ $I_m:$ 被遮罩的輸入影像 $I_e:$ 邊緣資訊 $I_l:$ 線條資訊 $M:$ 二元mask 輸出為草圖域 $[\tilde{I_e}, \tilde{I_l}]=TSR(I_m, I_e, I_l, M)$ Simple Structure upsample 在推論期間會使用SSU來將灰階的草圖上採樣至任意大小 Extract structure feature 在使用一個gated convolution based SFE來提取多尺度的特徵 $S_k=SFE(\tilde{I_e}, \tilde{I_l},M), {k=0,1,2,3}$ Add to to the fourier CNN texture restoration 將$S_k$資訊逐步加入(ZeroRA)相對應的FTR layer中 3.</description>
    </item>
    <item>
      <title>Selective Unsupervised Learning-Based Wi-Fi Fingerprint System Using Autoencoder and GAN</title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/selective-unsupervised-learning-based-wi-fi-fingerprint-system-using-autoencoder-and-gan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/selective-unsupervised-learning-based-wi-fi-fingerprint-system-using-autoencoder-and-gan/</guid>
      <description>Abstract 預期目標&#xA;建立一個基於神經網路的自動化且低時耗的多樓層Wi-Fi指紋建立系統 過去方法&#xA;TOA (time-ofr-arrival) 提出方法&#xA;使用UDRM演算法→Unsupervised dual radio mapping (UDRM) 選定一個主要參考的樓層建立最初的radio map 其他樓層則觀察與參考樓層的結構差異來分別選擇autoencoder或GAN來建立自己的radio map 適用於室內環境 基於MDLP最小描述長度原則與RMF無線電地圖反饋機制來同時優化並更新radio map 不需要label data 呈現成果&#xA;1. Introduction 情境說明 GPS定位的應用非常廣泛，但在室內應用中，GPS的訊號會受限，因此許多研究在找尋替代GPS的室內定位系統 TOA (time-of-arrival) 計算發收端和接收端的訊號抵達時間來估算兩者間的距離 fingerprint 計算AP訓好的強度來估算在室內的定位 相較TOA更為穩定，使用Wi-Fi，藍牙等 僅需要universal device而不需要到處部署發送端和接收端 Fingerprint說明 分為兩個階段 training phase 計算APs的RSSIs來建立radio map，以RP(reference point)為單位 → RP間隔為2-3m 利用preprocessing演算法來用RP建立radio map deterministic或機率模型 眾包 (crowdsourcing) 分類模型 positioning phase 即時的定位結果是利用將使用者當下的RSSI值與已建立好的radio map進行比對來找到在radio map中最接近的RSSI分佈，將該RP作為定位位置 使用SVM或k-nearest neighbor來找到最相似的RP 此論文方法重點 提出一個automatic Wi-Fi fingerprint system based on unsupervised learning，主要包含兩大演算法： UDRM → unsupervised dual radio mapping 演算法 MDLP-based RMF 演算法 MDLP → minimum description length principle RMF → radio map feedback 如何建立其他樓層的初始radio map?</description>
    </item>
    <item>
      <title>Temporal Group Fusion Network for Deep Video Inpainting</title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/temporal_group_fusion_network_dvi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/temporal_group_fusion_network_dvi/</guid>
      <description>Abstract 問題情境&#xA;對一段影片中缺失的區塊進行內容的填補&#xA;過去方法及問題&#xA;過去使用的DL方式多使用align的方式將參考frame與目標frame的估計動作的資訊整合在一起 問題是上述方法會讓performance高度依賴frame與frame間的align精准度 提出方法&#xA;提出一個Temporal Group Fusion Network (TGF-Net) 來透過兩階段的fusion策略來有效的整合時序上的資訊 階段零：在最初會使用一個粗略的alignment模型來處理large motion&#xA;階段一：&#xA;輸入的frame會先被分到不同的group中 分類完後使用intra-group fusion module來整合group內部的資訊 階段二：&#xA;使用一個temporal attention model來整合不同group間的資訊 實驗結果&#xA;利用兩階段的fusion策略來整合時序上的資訊，可以避免過度依賴align的結果 明顯的提高視覺上的品質以及時序間的相依性 1. Introduction Video inpainting是一個很基礎的task，重點在於合成一個視覺上貌似真實且在缺失的部分維持時序上的相依性&#xA;應用一、修復損毀的影片 應用二、移除不想要的物件 應用三、影片重訂向(video retargeting) video inpainting和image inpainting的差異在於video多了時序上的資訊，可以利用連續性的資料來對遺失部分進行更好的填補，因為缺失的部分可能有在其他frame中出現過，例如以下影片中的frame1~frame4中缺失的黃色部份可以在frame30中找到&#xA;儘管可以在其他frame中找的相關的信息，但因為鏡頭或各種因素，很難直接將該資訊直接應用在填補缺失上，因此要如何妥善利用這樣的資訊是很重要的問題&#xA;過去CNN方法著&#xA;重於alignment在進行fusion 使用3D-CNN來提取特徵（如此會受限receptive field） 本篇的特色 提出一個novel的end-to-end模型Temporal Group Fusion Network 兩階段的fusion策略 先整合group間的資訊 使用3D卷積來對各個group間的資訊進行整合 在各個group中都有一份target frame，此target frame是用來引導網路提取有意義的時序資訊 在整合不同group間彼此的資訊（善用不同group frame間互補的資訊） 不同group分別提供了不同(互補)的資訊來幫助填補缺口 使用temporal attention model來整合不同group間的資訊 此attention model會學習哪些特徵是有助於修補缺口，並且忽略無效區域 兩階段的fusion策略的確讓修補的效果看起來更為真實且保持時序上的連貫相依性，且避免了過度依賴alignment 此篇論文的方式仍有限制，感受野上(receptive field)的限制 若影片中具有大規模的動作時會有問題 因此在最初處理時還是會用個粗略的alignlment模型來做一些初步的補償，如此來減少target frame和reference frame間的差異，幫助該論文的模型能專注於object motion 2.</description>
    </item>
  </channel>
</rss>
