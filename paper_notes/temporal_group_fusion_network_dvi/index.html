<!doctype html>





























<html
  class="not-ready lg:text-base"
  style="--bg: #faf8f1"
  lang="en-us"
>
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, shrink-to-fit=no"
  />

  
  <title>Temporal Group Fusion Network for Deep Video Inpainting - My Paper note site</title>

  
  <meta name="theme-color" />

  
  
  
  
  <meta name="description" content="Abstract 問題情境
對一段影片中缺失的區塊進行內容的填補
過去方法及問題
過去使用的DL方式多使用align的方式將參考frame與目標frame的估計動作的資訊整合在一起 問題是上述方法會讓performance高度依賴frame與frame間的align精准度 提出方法
提出一個Temporal Group Fusion Network (TGF-Net) 來透過兩階段的fusion策略來有效的整合時序上的資訊 階段零：在最初會使用一個粗略的alignment模型來處理large motion
階段一：
輸入的frame會先被分到不同的group中 分類完後使用intra-group fusion module來整合group內部的資訊 階段二：
使用一個temporal attention model來整合不同group間的資訊 實驗結果
利用兩階段的fusion策略來整合時序上的資訊，可以避免過度依賴align的結果 明顯的提高視覺上的品質以及時序間的相依性 1. Introduction Video inpainting是一個很基礎的task，重點在於合成一個視覺上貌似真實且在缺失的部分維持時序上的相依性
應用一、修復損毀的影片 應用二、移除不想要的物件 應用三、影片重訂向(video retargeting) video inpainting和image inpainting的差異在於video多了時序上的資訊，可以利用連續性的資料來對遺失部分進行更好的填補，因為缺失的部分可能有在其他frame中出現過，例如以下影片中的frame1~frame4中缺失的黃色部份可以在frame30中找到
儘管可以在其他frame中找的相關的信息，但因為鏡頭或各種因素，很難直接將該資訊直接應用在填補缺失上，因此要如何妥善利用這樣的資訊是很重要的問題
過去CNN方法著
重於alignment在進行fusion 使用3D-CNN來提取特徵（如此會受限receptive field） 本篇的特色 提出一個novel的end-to-end模型Temporal Group Fusion Network 兩階段的fusion策略 先整合group間的資訊 使用3D卷積來對各個group間的資訊進行整合 在各個group中都有一份target frame，此target frame是用來引導網路提取有意義的時序資訊 在整合不同group間彼此的資訊（善用不同group frame間互補的資訊） 不同group分別提供了不同(互補)的資訊來幫助填補缺口 使用temporal attention model來整合不同group間的資訊 此attention model會學習哪些特徵是有助於修補缺口，並且忽略無效區域 兩階段的fusion策略的確讓修補的效果看起來更為真實且保持時序上的連貫相依性，且避免了過度依賴alignment 此篇論文的方式仍有限制，感受野上(receptive field)的限制 若影片中具有大規模的動作時會有問題 因此在最初處理時還是會用個粗略的alignlment模型來做一些初步的補償，如此來減少target frame和reference frame間的差異，幫助該論文的模型能專注於object motion 2." />
  <meta name="author" content="My Paper note site" />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="https://muxi1998.github.io/Paper-Blog/main.min.css" />

  
  
  
  
  
  <link rel="preload" as="image" href="https://muxi1998.github.io/Paper-Blog/theme.png" />

  
  
  
  
  

  
  
  

  
  
  <script
    defer
    src="https://muxi1998.github.io/Paper-Blog/highlight.min.js"
    onload="hljs.initHighlightingOnLoad();"
  ></script>
  

  
  
  

  
  <link rel="icon" href="https://muxi1998.github.io/Paper-Blog/favicon.ico" />
  <link rel="apple-touch-icon" href="https://muxi1998.github.io/Paper-Blog/apple-touch-icon.png" />

  
  <meta name="generator" content="Hugo 0.121.1">

  
  
  
  
  
  <meta itemprop="name" content="Temporal Group Fusion Network for Deep Video Inpainting">
<meta itemprop="description" content="Abstract 問題情境
對一段影片中缺失的區塊進行內容的填補
過去方法及問題
過去使用的DL方式多使用align的方式將參考frame與目標frame的估計動作的資訊整合在一起 問題是上述方法會讓performance高度依賴frame與frame間的align精准度 提出方法
提出一個Temporal Group Fusion Network (TGF-Net) 來透過兩階段的fusion策略來有效的整合時序上的資訊 階段零：在最初會使用一個粗略的alignment模型來處理large motion
階段一：
輸入的frame會先被分到不同的group中 分類完後使用intra-group fusion module來整合group內部的資訊 階段二：
使用一個temporal attention model來整合不同group間的資訊 實驗結果
利用兩階段的fusion策略來整合時序上的資訊，可以避免過度依賴align的結果 明顯的提高視覺上的品質以及時序間的相依性 1. Introduction Video inpainting是一個很基礎的task，重點在於合成一個視覺上貌似真實且在缺失的部分維持時序上的相依性
應用一、修復損毀的影片 應用二、移除不想要的物件 應用三、影片重訂向(video retargeting) video inpainting和image inpainting的差異在於video多了時序上的資訊，可以利用連續性的資料來對遺失部分進行更好的填補，因為缺失的部分可能有在其他frame中出現過，例如以下影片中的frame1~frame4中缺失的黃色部份可以在frame30中找到
儘管可以在其他frame中找的相關的信息，但因為鏡頭或各種因素，很難直接將該資訊直接應用在填補缺失上，因此要如何妥善利用這樣的資訊是很重要的問題
過去CNN方法著
重於alignment在進行fusion 使用3D-CNN來提取特徵（如此會受限receptive field） 本篇的特色 提出一個novel的end-to-end模型Temporal Group Fusion Network 兩階段的fusion策略 先整合group間的資訊 使用3D卷積來對各個group間的資訊進行整合 在各個group中都有一份target frame，此target frame是用來引導網路提取有意義的時序資訊 在整合不同group間彼此的資訊（善用不同group frame間互補的資訊） 不同group分別提供了不同(互補)的資訊來幫助填補缺口 使用temporal attention model來整合不同group間的資訊 此attention model會學習哪些特徵是有助於修補缺口，並且忽略無效區域 兩階段的fusion策略的確讓修補的效果看起來更為真實且保持時序上的連貫相依性，且避免了過度依賴alignment 此篇論文的方式仍有限制，感受野上(receptive field)的限制 若影片中具有大規模的動作時會有問題 因此在最初處理時還是會用個粗略的alignlment模型來做一些初步的補償，如此來減少target frame和reference frame間的差異，幫助該論文的模型能專注於object motion 2.">

<meta itemprop="wordCount" content="535">
<meta itemprop="keywords" content="" />
  
  <meta property="og:title" content="Temporal Group Fusion Network for Deep Video Inpainting" />
<meta property="og:description" content="Abstract 問題情境
對一段影片中缺失的區塊進行內容的填補
過去方法及問題
過去使用的DL方式多使用align的方式將參考frame與目標frame的估計動作的資訊整合在一起 問題是上述方法會讓performance高度依賴frame與frame間的align精准度 提出方法
提出一個Temporal Group Fusion Network (TGF-Net) 來透過兩階段的fusion策略來有效的整合時序上的資訊 階段零：在最初會使用一個粗略的alignment模型來處理large motion
階段一：
輸入的frame會先被分到不同的group中 分類完後使用intra-group fusion module來整合group內部的資訊 階段二：
使用一個temporal attention model來整合不同group間的資訊 實驗結果
利用兩階段的fusion策略來整合時序上的資訊，可以避免過度依賴align的結果 明顯的提高視覺上的品質以及時序間的相依性 1. Introduction Video inpainting是一個很基礎的task，重點在於合成一個視覺上貌似真實且在缺失的部分維持時序上的相依性
應用一、修復損毀的影片 應用二、移除不想要的物件 應用三、影片重訂向(video retargeting) video inpainting和image inpainting的差異在於video多了時序上的資訊，可以利用連續性的資料來對遺失部分進行更好的填補，因為缺失的部分可能有在其他frame中出現過，例如以下影片中的frame1~frame4中缺失的黃色部份可以在frame30中找到
儘管可以在其他frame中找的相關的信息，但因為鏡頭或各種因素，很難直接將該資訊直接應用在填補缺失上，因此要如何妥善利用這樣的資訊是很重要的問題
過去CNN方法著
重於alignment在進行fusion 使用3D-CNN來提取特徵（如此會受限receptive field） 本篇的特色 提出一個novel的end-to-end模型Temporal Group Fusion Network 兩階段的fusion策略 先整合group間的資訊 使用3D卷積來對各個group間的資訊進行整合 在各個group中都有一份target frame，此target frame是用來引導網路提取有意義的時序資訊 在整合不同group間彼此的資訊（善用不同group frame間互補的資訊） 不同group分別提供了不同(互補)的資訊來幫助填補缺口 使用temporal attention model來整合不同group間的資訊 此attention model會學習哪些特徵是有助於修補缺口，並且忽略無效區域 兩階段的fusion策略的確讓修補的效果看起來更為真實且保持時序上的連貫相依性，且避免了過度依賴alignment 此篇論文的方式仍有限制，感受野上(receptive field)的限制 若影片中具有大規模的動作時會有問題 因此在最初處理時還是會用個粗略的alignlment模型來做一些初步的補償，如此來減少target frame和reference frame間的差異，幫助該論文的模型能專注於object motion 2." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://muxi1998.github.io/Paper-Blog/paper_notes/temporal_group_fusion_network_dvi/" /><meta property="article:section" content="paper_notes" />




  
  <meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Temporal Group Fusion Network for Deep Video Inpainting"/>
<meta name="twitter:description" content="Abstract 問題情境
對一段影片中缺失的區塊進行內容的填補
過去方法及問題
過去使用的DL方式多使用align的方式將參考frame與目標frame的估計動作的資訊整合在一起 問題是上述方法會讓performance高度依賴frame與frame間的align精准度 提出方法
提出一個Temporal Group Fusion Network (TGF-Net) 來透過兩階段的fusion策略來有效的整合時序上的資訊 階段零：在最初會使用一個粗略的alignment模型來處理large motion
階段一：
輸入的frame會先被分到不同的group中 分類完後使用intra-group fusion module來整合group內部的資訊 階段二：
使用一個temporal attention model來整合不同group間的資訊 實驗結果
利用兩階段的fusion策略來整合時序上的資訊，可以避免過度依賴align的結果 明顯的提高視覺上的品質以及時序間的相依性 1. Introduction Video inpainting是一個很基礎的task，重點在於合成一個視覺上貌似真實且在缺失的部分維持時序上的相依性
應用一、修復損毀的影片 應用二、移除不想要的物件 應用三、影片重訂向(video retargeting) video inpainting和image inpainting的差異在於video多了時序上的資訊，可以利用連續性的資料來對遺失部分進行更好的填補，因為缺失的部分可能有在其他frame中出現過，例如以下影片中的frame1~frame4中缺失的黃色部份可以在frame30中找到
儘管可以在其他frame中找的相關的信息，但因為鏡頭或各種因素，很難直接將該資訊直接應用在填補缺失上，因此要如何妥善利用這樣的資訊是很重要的問題
過去CNN方法著
重於alignment在進行fusion 使用3D-CNN來提取特徵（如此會受限receptive field） 本篇的特色 提出一個novel的end-to-end模型Temporal Group Fusion Network 兩階段的fusion策略 先整合group間的資訊 使用3D卷積來對各個group間的資訊進行整合 在各個group中都有一份target frame，此target frame是用來引導網路提取有意義的時序資訊 在整合不同group間彼此的資訊（善用不同group frame間互補的資訊） 不同group分別提供了不同(互補)的資訊來幫助填補缺口 使用temporal attention model來整合不同group間的資訊 此attention model會學習哪些特徵是有助於修補缺口，並且忽略無效區域 兩階段的fusion策略的確讓修補的效果看起來更為真實且保持時序上的連貫相依性，且避免了過度依賴alignment 此篇論文的方式仍有限制，感受野上(receptive field)的限制 若影片中具有大規模的動作時會有問題 因此在最初處理時還是會用個粗略的alignlment模型來做一些初步的補償，如此來減少target frame和reference frame間的差異，幫助該論文的模型能專注於object motion 2."/>

  
  
  
  <link rel="canonical" href="https://muxi1998.github.io/Paper-Blog/paper_notes/temporal_group_fusion_network_dvi/" />
  
  
</head>

  <body class="text-black duration-200 ease-out dark:text-white">
    <header class="mx-auto flex h-[4.5rem] max-w-3xl px-8 lg:justify-center">
  <div class="relative z-50 mr-auto flex items-center">
    <a
      class="-translate-x-[1px] -translate-y-[1px] text-2xl font-semibold"
      href="https://muxi1998.github.io/Paper-Blog/"
      >My Paper note site</a
    >
    <div
      class="btn-dark text-[0] ml-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.png)_left_center/_auto_theme('spacing.6')_no-repeat] [transition:_background-position_0.4s_steps(5)] dark:[background-position:right]"
      role="button"
      aria-label="Dark"
    ></div>
  </div>

  <div
    class="btn-menu relative z-50 -mr-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden"
    role="button"
    aria-label="Menu"
  ></div>

  

  <script>
    
    const htmlClass = document.documentElement.classList;
    setTimeout(() => {
      htmlClass.remove('not-ready');
    }, 10);

    
    const btnMenu = document.querySelector('.btn-menu');
    btnMenu.addEventListener('click', () => {
      htmlClass.toggle('open');
    });

    
    const metaTheme = document.querySelector('meta[name="theme-color"]');
    const lightBg = '#faf8f1'.replace(/"/g, '');
    const setDark = (isDark) => {
      metaTheme.setAttribute('content', isDark ? '#000' : lightBg);
      htmlClass[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark);
    };

    
    const darkScheme = window.matchMedia('(prefers-color-scheme: dark)');
    if (htmlClass.contains('dark')) {
      setDark(true);
    } else {
      const darkVal = localStorage.getItem('dark');
      setDark(darkVal ? darkVal === 'true' : darkScheme.matches);
    }

    
    darkScheme.addEventListener('change', (event) => {
      setDark(event.matches);
    });

    
    const btnDark = document.querySelector('.btn-dark');
    btnDark.addEventListener('click', () => {
      setDark(localStorage.getItem('dark') !== 'true');
    });
  </script>

  <div
    class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"
  >
    
    

    
  </div>
</header>


    <main
      class="prose prose-neutral relative mx-auto min-h-[calc(100%-9rem)] max-w-3xl px-8 pb-16 pt-12 dark:prose-invert"
    >
      

<article>
  <header class="mb-16">
    <h1 class="!my-0 pb-2.5">Temporal Group Fusion Network for Deep Video Inpainting</h1>

    
    <div class="text-sm antialiased opacity-60">
      
      
      
      
    </div>
    
  </header>

  <section><h1 id="abstract">Abstract</h1>
<p><strong>問題情境</strong></p>
<p>對一段影片中缺失的區塊進行內容的填補</p>
<p><strong>過去方法及問題</strong></p>
<ul>
<li>過去使用的DL方式多使用align的方式將參考frame與目標frame的估計動作的資訊整合在一起</li>
<li>問題是上述方法會讓performance高度依賴frame與frame間的align精准度</li>
</ul>
<p><strong>提出方法</strong></p>
<ul>
<li>提出一個Temporal Group Fusion Network (TGF-Net) 來透過兩階段的fusion策略來有效的整合時序上的資訊
<ul>
<li>
<p>階段零：在最初會使用一個粗略的alignment模型來處理large motion</p>
</li>
<li>
<p>階段一：</p>
<ul>
<li>輸入的frame會先被分到不同的group中</li>
<li>分類完後使用intra-group fusion module來整合group內部的資訊</li>
</ul>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
</li>
<li>
<p>階段二：</p>
<ul>
<li>使用一個temporal attention model來整合不同group間的資訊</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>實驗結果</strong></p>
<ul>
<li>利用兩階段的fusion策略來整合時序上的資訊，可以避免過度依賴align的結果</li>
<li>明顯的提高視覺上的品質以及時序間的相依性</li>
</ul>
<h1 id="1-introduction">1. Introduction</h1>
<ul>
<li>
<p>Video inpainting是一個很基礎的task，重點在於合成一個視覺上貌似真實且在缺失的部分維持時序上的相依性</p>
<ul>
<li>應用一、修復損毀的影片</li>
<li>應用二、移除不想要的物件</li>
<li>應用三、影片重訂向(video retargeting)</li>
</ul>
</li>
<li>
<p>video inpainting和image inpainting的差異在於video多了時序上的資訊，可以利用連續性的資料來對遺失部分進行更好的填補，因為缺失的部分可能有在其他frame中出現過，例如以下影片中的frame1~frame4中缺失的黃色部份可以在frame30中找到</p>
<p><img src="../paper_resources/Temporal_Group_Fusion_Network_DVI/%E6%88%AA%E5%9C%96_2022-06-02_%E4%B8%8B%E5%8D%881.59.32.png" alt="截圖 2022-06-02 下午1.59.32.png"></p>
</li>
<li>
<p>儘管可以在其他frame中找的相關的信息，但因為鏡頭或各種因素，很難直接將該資訊直接應用在填補缺失上，因此要如何妥善利用這樣的資訊是很重要的問題</p>
</li>
<li>
<p>過去CNN方法著</p>
<ul>
<li>重於alignment在進行fusion</li>
<li>使用3D-CNN來提取特徵（如此會受限receptive field）</li>
</ul>
</li>
</ul>
<h3 id="本篇的特色">本篇的特色</h3>
<ul>
<li>提出一個novel的end-to-end模型Temporal Group Fusion Network
<ul>
<li>兩階段的fusion策略
<ul>
<li>先整合group間的資訊
<ul>
<li>使用3D卷積來對各個group間的資訊進行整合</li>
<li>在各個group中都有一份target frame，此target frame是用來引導網路提取有意義的時序資訊</li>
</ul>
</li>
<li>在整合不同group間彼此的資訊（善用不同group frame間互補的資訊）
<ul>
<li>不同group分別提供了不同(互補)的資訊來幫助填補缺口</li>
<li>使用temporal attention model來整合不同group間的資訊</li>
<li>此attention model會學習哪些特徵是有助於修補缺口，並且忽略無效區域</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>兩階段的fusion策略的確讓修補的效果看起來更為真實且保持時序上的連貫相依性，且避免了過度依賴alignment</li>
<li>此篇論文的方式仍有限制，感受野上(receptive field)的限制
<ul>
<li>若影片中具有大規模的動作時會有問題</li>
<li>因此在最初處理時還是會用個粗略的alignlment模型來做一些初步的補償，如此來減少target frame和reference frame間的差異，幫助該論文的模型能專注於object motion</li>
</ul>
</li>
</ul>
<h1 id="2-related-work">2. Related Work</h1>
<h2 id="a-single-image-inpainting">A. Single Image inpainting</h2>
<h2 id="b-video-inpainting">B. Video inpainting</h2>
<h1 id="3-proposed-method">3. Proposed Method</h1>
<h2 id="a-problem-statement">A. Problem Statement</h2>
<ul>
<li>$X:={X_1, X_2, \cdots, X_T}$ 是一系列的毀損影片frames，其高為$H$，寬為$W$，frame的長度為$T$</li>
<li>$M:={M_1, M_2, \cdots, M_T}$ 是上述一系列毀損影片frame中毀損的區域，0為正常的pixel，1為毀損的地方</li>
<li>$\hat{Y}:={\hat{Y_1}, \hat{Y_2}, \cdots, \hat{Y_T}}$ 為此網路將masked video $X$ mapping到回復輸出的結果，且此結果需要盡可能接近GT $Y:={Y_1, Y_2, \cdots, Y_T}$</li>
</ul>
<h2 id="b-overview">B. Overview$</h2>
<h3 id="主要概念">主要概念</h3>
<ul>
<li>在影像修復的任務中，物件的移動和視角的變化關係，當前frame中缺失的區域通常可能在其他frame中顯示出來</li>
<li>若過去和未來都沒有可以參考修復的frame，則此網路也應該有能力合成未知區域的能力</li>
<li>為達到上述的成效，此論文建構編碼器－解碼器架構的網路，用來整合其他frame的時間信息並同時完成單frame的修復</li>
</ul>
<h3 id="提出方法概念">提出方法概念</h3>
<ul>
<li>將修復任務視為“多對單”問題
<ul>
<li>
<p>按時間逐幀處理影片</p>
</li>
<li>
<p>提出時間組融合網路TGF-Net</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<ul>
<li>Step1. 粗略的對齊</li>
<li>Step2. 特徵提取</li>
<li>Step3. 時間信息融合和目標frame恢復</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="tgf-net概要說明">TGF-Net概要說明</h3>
<p><img src="../paper_resources/Temporal_Group_Fusion_Network_DVI/%E6%88%AA%E5%9C%96_2022-06-03_%E4%B8%8B%E5%8D%8811.08.18.png" alt="截圖 2022-06-03 下午11.08.18.png"></p>
<ul>
<li>
<p>輸入為</p>
<ul>
<li>目標frame $X_t$ (有標註缺失的區域)</li>
<li>$X_t$ 對應的參考frame ${X_m, \cdots, X_p,\cdots, X_n}$</li>
</ul>
</li>
<li>
<p>流程步驟說明</p>
<ol>
<li>先將參考frame各自對$X_t$做alignment對齊得到${CX_m, \cdots, CX_p, \cdots, CX_n}$</li>
<li>將$X_t$和對齊過後的參考frame進行encode 得到特徵$F^s_t$和${CF^s_m, \cdots, CF^s_p, \cdots, CF^s_n}$
<ul>
<li>$s$ 為特徵的尺度 (1/2, 1/4, 1/8)</li>
</ul>
</li>
<li>利用時間資訊融合模組TIF module來合成不同尺度的特徵，再近一步upsample融合特徵$FF^s_t$到下一個尺度，來漸進式的整合不同尺度的資訊
<ul>
<li>TIF module使用兩階段的fusion策略
<ul>
<li>階段一、整合group中的資訊</li>
<li>階段二、整合不同group間的資訊</li>
</ul>
</li>
</ul>
</li>
<li>最後TGF-Net輸出完整的frame $\hat{Y_t}$</li>
</ol>
</li>
</ul>
<p><img src="../paper_resources/Temporal_Group_Fusion_Network_DVI/%E6%88%AA%E5%9C%96_2022-06-03_%E4%B8%8B%E5%8D%8811.30.36.png" alt="截圖 2022-06-03 下午11.30.36.png"></p>
<h2 id="c-pyramid-frame-encoder">C. Pyramid frame encoder</h2>
<h3 id="目標提升時序資訊整合的效率">目標：提升時序資訊整合的效率</h3>
<h3 id="概念">概念</h3>
<ul>
<li>金字塔編碼器由low-level特徵到high-level特徵，過程中取得不同尺度下的特徵</li>
</ul>
<h3 id="運作流程">運作流程</h3>
<ul>
<li>拿frame與其對應的binary mask為輸入，並在進入第一層前會先將input在通道的軸上進行concat</li>
<li>Encoder的結構如下
<ul>
<li>七卷積層及ReLU激活函數</li>
<li>卷積層使用stride 2來幫助降低解析度 1/2, 1/4, 1/8</li>
</ul>
</li>
</ul>
<h2 id="d-temporal-information-fusion-tif-module">D. Temporal information fusion (TIF) module</h2>
<p><img src="../paper_resources/Temporal_Group_Fusion_Network_DVI/%E6%88%AA%E5%9C%96_2022-06-03_%E4%B8%8B%E5%8D%8811.57.52.png" alt="截圖 2022-06-03 下午11.57.52.png"></p>
<h3 id="目標整合參考frame的資訊">目標：整合參考frame的資訊</h3>
<h3 id="概念-1">概念</h3>
<ul>
<li>過去的方法直接使用3D卷積來處理整個影片序列，但如此就會有視野限制，遠距離的參考frame無法有效的被目標frame明確引導，導致時間信息融合不足</li>
<li>為避免上述問題，使用TIF模組來將目標frame與各個參考frame group起來
<ul>
<li>確保目標frame都能確實地引導參考frame該保留哪些有用的資訊</li>
<li>兩階段融合擺脫對於alignment操作的依賴，大大提高修復結果的質量</li>
</ul>
</li>
</ul>
<h3 id="流程步驟">流程步驟：</h3>
<ol>
<li>
<p>目標frame和參考frame會先被分在同一個群組中，新的grouped序列表示為  $G={G_m, \cdots, G_p, \cdots, G_n}$，$G_i={CF_i, F_t}$</p>
</li>
<li>
<p>在各個group中會使用intra-group fusion module來整合group中的資訊，使用3D CNN</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
</li>
<li>
<p>temporal attention fusion module會用來整合事先透過intra-group fusion的特徵$G={GF_m, \cdots, GF_p, \cdots, GF_n}$，整合後得到最終特徵$FF_t$wit</p>
</li>
</ol>
<h3 id="intra-group-fusion-module">Intra-group fusion module</h3>
<p><img src="../paper_resources/Temporal_Group_Fusion_Network_DVI/%E6%88%AA%E5%9C%96_2022-06-04_%E4%B8%8A%E5%8D%8812.28.35.png" alt="截圖 2022-06-04 上午12.28.35.png"></p>
<ul>
<li>
<p>先將$F_t$和$CF_i$堆疊在一起</p>
</li>
<li>
<p>把堆疊後的結果送到multi-scale fusion block來生成intra-group fusion feature $GF_i$</p>
</li>
<li>
<p>multi-scale fusion block中的各個stream (不同的scale)包含了兩種結構</p>
<ul>
<li>
<p>一個3D 卷積層 kernel size為$ks\times ks\times 2 \ (H\times W\times T)$</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
</li>
<li>
<p>五個2D卷積層 kernel size為$ks\times ks$ ，$ks$會依stream不同而異</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
</li>
</ul>
</li>
<li>
<p>最後整合分別來自三個不同stream的特徵</p>
<ul>
<li>concat來自三個尺度的特徵並使用$1\times 1$卷積來map到原始的input大小</li>
</ul>
</li>
<li>
<p>此設計可聚合多尺度的上下文信息，使網路能夠處理不同的物件大小並獲得更精確的完整結果</p>
</li>
</ul>
<h3 id="temporal-attention-fusion-module">Temporal attention fusion module</h3>
<h3 id="目標動態的整合跨組間的信息找到有用的信息避免無用的資訊">目標：動態的整合跨組間的信息，找到有用的信息，避免無用的資訊</h3>
<h3 id="概念-2">概念</h3>
<ul>
<li>解決目標frame與參考frame間的遮擋與視差、相機移動的問題，導致不同組的特徵圖信息量不均等</li>
<li>利用時間注意力機制來學習每個組的貢獻，以整合來自不同組的信息</li>
</ul>
<h3 id="運作流程-1">運作流程</h3>
<ul>
<li>使用一個$3\times3$卷積層來對intra-group特徵$GF_i$做下一步的特徵提取，得到一個通道的group-wise特徵圖$WF_i$</li>
<li>將透過$3\times3$卷積得到的新的序列$WF={WF_m, \cdots, WF_p, \cdots, WF_n}$沿著時間維度concat在一起</li>
<li>再套上一個softmax函式來計算各個位置在時序通道上的attention maps 並以$GA={GA_m, \cdots, GA_p, \cdots, GA_n}$ 表示
$GA_i(x,y)=\frac{e^{WF_i(x,y)}}{\sum^N_{j=1}e^{WF_j(x,y)}}$
<ul>
<li>$N$ 代表group-wise特徵圖的總數</li>
<li>$GA_i(x,y)$ 表示attention map $GA_i$在位置$(x,y)$的值</li>
</ul>
</li>
<li>最後的融合特徵$FF_t$是由將最初的group fusion特徵${GF_m, \cdots, GF_p,\cdots, GF_n}$與attention maps ${GA_m, \cdots, GA_p, \cdots, GA_n}$相乘，再加總
$FF_t=\sum^N_{i=1}GA_i \odot GF_i$</li>
<li>在尺度$s$下得到$FF^s_t$，將其使用一個$3\times 3$的卷積來使此$FF^s_t$能被傳遞到下一個尺度做使用
$\tilde{F_t^{s+1}}=(1-M^{s+1}_t)\odot F^{s+1}_t+M^{s+1}_t\odot \tilde{FF}^s_t$</li>
</ul>
<p><img src="../paper_resources/Temporal_Group_Fusion_Network_DVI/%E6%88%AA%E5%9C%96_2022-06-03_%E4%B8%8B%E5%8D%8811.08.18.png" alt="截圖 2022-06-03 下午11.08.18.png"></p>
<h2 id="e-coarse-alignment-module">E. Coarse Alignment Module</h2>
<h3 id="目標解決大運動影像時存在的問題">目標：解決大運動影像時存在的問題</h3>
<h3 id="概念-3">概念</h3>
<ul>
<li>在使用TGF-Net之前先使用一個粗略的alignment模組來初步的對目標frame和參考frame做初步的簡單對齊</li>
</ul>
<h3 id="運作流程-2">運作流程</h3>
<ul>
<li>採用自監督對齊損失函式來end-to-end訓練粗略對齊模組</li>
<li>alignment loss計算目標frame與參考frame之間的L1距離，且計算時不包含毀損區域
$L_{align}=\sum_i\parallel (1-M_t) \odot(CX_i-X_t) \parallel_1$</li>
</ul>
<h2 id="f-loss-function">F. Loss Function</h2>
<h3 id="主要訓練目標">主要訓練目標</h3>
<ol>
<li>pixel-wise的reconstruction</li>
<li>知覺相似度 (perceptual similarity)</li>
<li>時空相關性 (spatio-temporal coherence)</li>
</ol>
<h3 id="pixel-wise-reconstruction-loss">Pixel-wise reconstruction loss</h3>
<ul>
<li>限制restored target frame必須在各個尺度下都近似於GT frame，用以漸進式的refine細節</li>
<li>將缺失部分與無缺失部分分開計算</li>
</ul>
<p>$$
L_{py}=\sum_{s=1}^{S-1}\parallel (1-M^s_t)\ \odot \ (Y^s-h(FF^s_t)) \parallel_1+\lambda_{hole}\parallel M^s_t \ \odot \ (Y^s-h(FF^s_t)) \parallel_1
$$</p>
<ul>
<li>$h$ 代表一個$1\times 1$的卷積來將特徵圖$FF^s_t$轉換成一個RGB圖片</li>
<li>$Y^s$ 表示resized成和$FF^s_t$尺度一樣的GT</li>
</ul>
<h3 id="perceptual-loss">Perceptual loss</h3>
<ul>
<li>抓到高階語意以及模擬人類感知下的影像品質</li>
<li>使用ImageNet-pretrained VGG-16的架構來計算</li>
</ul>
<p>$$
L{prec}=\mathbb{E}[\sum_i \frac{1}{N_i}\parallel \phi_i(\hat{Y}_{comp}-\phi_i(Y)) \parallel_1]
$$</p>
<ul>
<li>$\hat{Y}_{comp}$ 表示網路輸出$\hat{Y}$的缺失部分與輸入$X$無缺失部分的結合</li>
<li>$\phi_i$ 為pretrained VGG-16的activation map
<ul>
<li>relu1_1, relu2_1, relu3_1, relu4_1, relu5_1</li>
</ul>
</li>
</ul>
<h3 id="style-loss-and-total-variation-loss">Style loss and total variation loss</h3>
<ul>
<li>用來對恢復frame的視覺品質做更近一步的改善</li>
<li>給定一個大小為$C_j\times H_j\times W_j$的特徵圖，其style loss的計算公式如下</li>
</ul>
<p>$$
L_{style}=\mathbb{E}<em>j[\parallel G^\phi_j(\hat{Y}</em>{comp})-G^\phi_j(Y) \parallel_1]
$$</p>
<ul>
<li>$G^\phi_j$ 是一個$G_j\times G_j$的gram matrix建構於所選的activation map</li>
</ul>
<h3 id="整體loss函式">整體loss函式</h3>
<p>$$
L_{total}=L_{py}+\lambda_1L_{prec}+\lambda_2L_{style}+\lambda_3L_{tv}+\lambda_4 L_{align}
$$</p>
<ul>
<li>$\lambda_1=0.01, \lambda_2=24, \lambda_3=0.1, \lambda_4=2, \lambda_{hole}=3$</li>
</ul>
<h2 id="g-testing">G. Testing</h2>
<ul>
<li>在測試的過程，影片是依照時序順序逐frame放入模型中</li>
<li>要完成 t-th frame要拿$X_t, { X_m, \cdots, X_p , \cdots, X_n}$以及其對應的masks當作輸入</li>
<li>最後輸入完成的t-th frame $\hat{Y}_t$，並更新到輸入的影片序列中，來提供更多的資訊來幫助接下來的frame進行修復，並得到更好得時序相依性</li>
</ul>
<h2 id="h-temporal-consistency">H. Temporal Consistency</h2>
<ul>
<li>
<p>維持時序一致性的方法為在回復完各個frame後都會重新更新回原本的輸入序列中</p>
</li>
<li>
<p>被還原的frame可以做為之後的參考frame，提供更多的資訊給接下來的frame</p>
</li>
<li>
<p>3D卷積可以抓出時空資訊</p>
</li>
<li>
<p>此篇論文提出的TIF module有兩階段的fusion策略</p>
<ul>
<li>首先使用3D卷積來整合group內的資訊</li>
<li>再來使用temporal attention mechanism來整合不同group間的資訊</li>
</ul>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
</li>
<li>
<p>未能讓TGF-Net發揮更大的作用，在整個網路運作前會先使用粗略的alignment module來做簡單的對齊</p>
</li>
</ul>
<h1 id="4-experiments">4. Experiments</h1>
<h2 id="a-datasets">A. Datasets</h2>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h3 id="youtube-vos">YouTube-VOS</h3>
<ul>
<li>4,453個youtube片段，包含多樣的場景 (適合建立inpainting模型→能夠抓到許多自然場景的分佈)
<ul>
<li>街道</li>
<li>野外</li>
</ul>
</li>
<li>每個clip差不多約3~6秒</li>
<li>所有clip平均包含140個frame</li>
<li>實際使用
<ul>
<li>training set: 3,471</li>
<li>testing set: 508</li>
</ul>
</li>
</ul>
<p><img src="../paper_resources/Temporal_Group_Fusion_Network_DVI/%E6%88%AA%E5%9C%96_2022-06-05_%E4%B8%8B%E5%8D%883.38.14.png" alt="截圖 2022-06-05 下午3.38.14.png"></p>
<ul>
<li>large-scale video object segmentation dataset</li>
</ul>
<h3 id="davis-dataset">DAVIS Dataset</h3>
<ul>
<li>包含150個高品質的Full HD影片序列</li>
</ul>
<h3 id="資料集使用方式">資料集使用方式</h3>
<ul>
<li>有兩種mask的方法
<ul>
<li>移除特定物件</li>
<li>移除特定區域（模擬移除浮水印 ）</li>
</ul>
</li>
</ul>
<p><img src="../paper_resources/Temporal_Group_Fusion_Network_DVI/%E6%88%AA%E5%9C%96_2022-06-05_%E4%B8%8B%E5%8D%883.51.45.png" alt="截圖 2022-06-05 下午3.51.45.png"></p>
<h2 id="b-implementation-details">B. Implementation Details</h2>
<p>（略）</p>
<h2 id="c-baselinesal">C. Baselinesal</h2>
<h3 id="主要比較方法">主要比較方法</h3>
<ul>
<li>VINet
<ul>
<li>recurrent encoder-decoder network</li>
<li>flow warping-based context aggregation</li>
</ul>
</li>
<li>DFVI
<ul>
<li>將video inpainting task視為pixel propagation問題</li>
<li>用dense flow field來傳遞其他frame的內容</li>
</ul>
</li>
<li>CPNet
<ul>
<li>預測不同frame間的轉換matrics，來對齊frame</li>
<li>用weighting summing來聚集aligned frames的資訊</li>
</ul>
</li>
<li>STTN
<ul>
<li>使用multi-scale patch-based self-attention策略來填補遺失的區域</li>
</ul>
</li>
</ul>
<h3 id="比較設定">比較設定</h3>
<ul>
<li>使用作者的pre-train模型</li>
<li>256 x 384 frames</li>
</ul>
<h2 id="d-evaluation-metrics">D. Evaluation Metrics</h2>
<h3 id="psnr">PSNR</h3>
<ul>
<li>評估回復圖片的失真率</li>
</ul>
<h3 id="ssim">SSIM</h3>
<ul>
<li>比較回復圖片與原始GT圖片結構的相似性</li>
</ul>
<h3 id="flow-warping-error">flow warping error</h3>
<ul>
<li>上述兩個指標常被用來評價品質，但無法有效的評估temporal consistency</li>
<li>因此使用flow warping error來計算兩個frame間的temporal stability</li>
</ul>
<h2 id="e-qualitative-evaluation">E. Qualitative Evaluation</h2>
<p><img src="../paper_resources/Temporal_Group_Fusion_Network_DVI/%E6%88%AA%E5%9C%96_2022-06-05_%E4%B8%8B%E5%8D%884.59.50.png" alt="截圖 2022-06-05 下午4.59.50.png"></p>
<p><img src="../paper_resources/Temporal_Group_Fusion_Network_DVI/%E6%88%AA%E5%9C%96_2022-06-05_%E4%B8%8B%E5%8D%885.00.18.png" alt="截圖 2022-06-05 下午5.00.18.png"></p>
<ul>
<li>圖5是測試移除特定區塊</li>
<li>圖6是測試移除特定物件</li>
</ul>
<h3 id="發現">發現</h3>
<ul>
<li>DFVI和VINet都是使用optical flow來對齊不同的frame以及整合其資訊，高度依賴optical flow的準確度</li>
<li>CPNet採用簡單的全局運動估計來對齊frame，因此在複雜場景的表現會不如預期</li>
<li>VINet的temporal window只有5個frame，因此</li>
</ul>
<h2 id="f-quantitative-evaluation">F. Quantitative Evaluation</h2>
<h2 id="g-ablation-study">G. Ablation Study</h2>
<h1 id="5-conclusion">5. Conclusion</h1>
</section>

  
  

  
  
  
  
  <nav class="mt-24 flex rounded-lg bg-black/[3%] text-lg dark:bg-white/[8%]">
    
    <a
      class="flex w-1/2 items-center rounded-l-md p-6 pr-3 font-semibold no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]"
      href="https://muxi1998.github.io/Paper-Blog/paper_notes/zits/"
      ><span class="mr-1.5">←</span><span>Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding</span></a
    >
    
    
  </nav>
  
  

  
  

  
  

  

  
</article>


    </main>

    <footer
  class="opaco mx-auto flex h-[4.5rem] max-w-3xl items-center px-8 text-[0.9em] opacity-60"
>
  <div class="mr-auto">
    &copy; 2024
    <a class="link" href="https://muxi1998.github.io/Paper-Blog/">My Paper note site</a>
  </div>
  <a class="link mx-6" href="https://gohugo.io/" rel="noopener" target="_blank"
    >Powered by Hugo️️</a
  >️
  <a
    class="link"
    href="https://github.com/nanxiaobei/hugo-paper"
    rel="noopener"
    target="_blank"
    >✎ Paper</a
  >
</footer>

  </body>
</html>
