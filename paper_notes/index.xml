<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Paper_notes on My Paper note site</title>
    <link>https://muxi1998.github.io/Paper-Blog/paper_notes/</link>
    <description>Recent content in Paper_notes on My Paper note site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Jan 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://muxi1998.github.io/Paper-Blog/paper_notes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields</title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/giraffe-representing-scenes-as-compositional-generative-neural-feature-fields/</link>
      <pubDate>Tue, 09 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/giraffe-representing-scenes-as-compositional-generative-neural-feature-fields/</guid>
      <description>GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields Abstract å•é¡Œæƒ…å¢ƒ&#xA;é™¤äº†èƒ½æ›´çœŸå¯¦çš„åˆæˆ(ç”Ÿæˆ)åœ–ç‰‡ï¼Œé‚„è¦èƒ½è®“åœ–ç‰‡å…§å®¹æ˜¯å¯ä»¥æ§åˆ¶çš„&#xA;éå»è§£æ³•&#xA;ç ”ç©¶latent codeçš„è§£è€¦åˆï¼Œæƒ³è¾¦æ³•é€éä¸åŒçš„factorä¾†æ§åˆ¶dataçš„è®Šç•°æ€§ å•é¡Œç™¼ç¾&#xA;å¤šæ•¸ç ”ç©¶éƒ½å°ˆæ³¨æ–¼2Dç©ºé–“ï¼Œè€Œå¿˜è¨˜æˆ‘å€‘çš„ä¸–ç•Œæ˜¯3Dç©ºé–“ åªæœ‰å°‘æ•¸çš„ç ”ç©¶è‘—é‡æ–¼å ´æ™¯çš„æ§‹æˆæ€§è³ª(c nature of scenes) ç›®å‰åƒ…èƒ½åœ¨å–®å€‹ç‰©ä»¶å ´æ™¯ä¸‹æ‰èƒ½æœ‰è¼ƒå¥½çµæœ èƒŒæ™¯éæ–¼è¤‡é›œæˆ–é€¼çœŸï¼Œæœƒå°è‡´æ¨¡å‹çš„æ•ˆæœä¸ç©©å®š æå‡ºæ–¹æ³•&#xA;è®“æ¨¡å‹åŸºæ–¼ä¸€å€‹çµ„åˆå¼çš„3Då ´æ™¯è¡¨ç¤ºä¾†ç”Ÿæˆç‰©ä»¶ï¼Œä¾†é”åˆ°åœ–ç‰‡åˆæˆçš„å¯æ§æ€§ã€‚ ç”¨ç¥ç¶“ç‰¹å¾µåŸŸä¾†æè¿°å ´æ™¯å¯ä»¥å¹«åŠ©å°å¤šç‰©ä»¶çš„è§£è€¦åˆ(ç‰©ä»¶çš„å½¢ç‹€æˆ–å¤–è§€) ä¸éœ€è¦é¡å¤–çš„ç›£ç£å³å¯é€éunstructuredå’Œunposedçš„åœ–ç‰‡é›†åˆä¾†è¨“ç·´æ•´å€‹ç¶²è·¯ è¬›æ­¤3Då ´æ™¯è¡¨ç¤ºæ³•èˆ‡ç¥ç¶“æ¸²æŸ“ç®¡é“çµåˆï¼Œå¯ä»¥ç”Ÿæˆå¿«é€Ÿä¸”é€¼çœŸçš„åœ–åƒåˆæˆæ¨¡å‹ é”åˆ°æˆæœ&#xA;èƒ½å¤ å°‡åœ–ç‰‡ä¸­çš„ç‰©ä»¶å€‹åˆ¥çš„è§£è€¦åˆå‡ºä¾†ï¼Œä¸¦å°è©²ç‰©ä»¶é€²è¡Œå¹³ç§»æˆ–æ—‹è½‰ï¼Œæ”¹è®Šç›¸æ©Ÿè¦–è§’&#xA;1. Introduction å‰æƒ…æè¦ ç”Ÿæˆå’Œæ“æ§é€¼çœŸçš„åœ–åƒå…§å®¹æ˜¯é›»è…¦è¦–è¦ºé ˜åŸŸä¸€ç›´åœ¨åŠªåŠ›çš„ç›®æ¨™ è‹¥è¦è™•ç†3Dè³‡è¨Šï¼ŒæœƒèŠ±è²»å¤§é‡ç¡¬é«”è³‡æºï¼ŒåŠäººåŠ›å‰µå»º GANåœ¨è¿‘æœŸä¿ƒé€²é«˜åº¦é€¼çœŸåœ–åƒåˆæˆçš„é€²æ­¥ï¼Œå¯ä»¥åˆæˆ1024*1024é«˜ç•«è³ªçš„åœ–ç‰‡åˆæˆ åˆæˆé€¼çœŸçš„2Dåœ–ç‰‡ä¸¦éå”¯ä¸€ç›®æ¨™ï¼Œå°æ–¼åˆæˆéç¨‹ä¹Ÿè¦æœ‰ç°¡å–®ã€ä¸€è‡´çš„æ§åˆ¶æ–¹æ³• è¨±å¤šæ–¹æ³•ç ”ç©¶å¦‚ä½•åœ¨æœªæ˜ç¢ºç›£ç£ä¸‹ï¼Œå¾æ•¸æ“šä¸­çš„å­¸ç¿’è§£è€¦è¡¨ç¤º è§£è€¦åˆçš„è§£é‡‹å„å¼å„æ¨£ï¼Œé€šå¸¸æ˜¯æŒ‡æ§åˆ¶æ„Ÿèˆˆè¶£çš„å±¬æ€§(ç‰©ä»¶å½¢ç‹€ã€å¤§å°ã€å§¿å‹¢)ï¼Œä¸”ä¸å½±éŸ¿å…¶ä»–å±¬æ€§ å¤§å¤šæ•¸çš„æ–¹æ³•éƒ½æ²’æœ‰è€ƒæ…®åˆ°å ´æ™¯çš„çµ„åˆæ€§è³ªï¼Œéƒ½åœ¨2Dåœ–ç‰‡ä¸­æ“ä½œï¼Œè€Œå¿˜è¨˜æˆ‘å€‘çš„ä¸–ç•Œæ˜¯3Dç©ºé–“ æœƒå°è‡´ç³¾çºçš„æ§åˆ¶æ©Ÿåˆ¶ä¸æ˜¯å…§å»ºçš„ï¼Œè€Œæ˜¯å¾Œä¾†æ‰åœ¨æ½›åœ¨ç©ºé–“ä¸­ç™¼ç¾çš„ åœ¨è¨±å¤šæ‡‰ç”¨å ´æ™¯ä¸­ï¼Œ3Dçš„åˆæˆæ˜¯å¾ˆé—œéµçš„ï¼Œèƒ½é”åˆ°æ›´ç´°ç·»çš„åˆæˆçµæœï¼Œå› æ­¤æœ‰äº›ç ”ç©¶é–‹å§‹è‘—é‡æ–¼åœ¨3Dç©ºé–“ä¸Šçš„æ“ä½œ voxels primitives radiance fields Contribution æå‡ºGIRAFFEï¼Œå¾åŸå§‹éçµæ§‹åŒ–åœ–åƒé›†è¨“ç·´å‡ºä¸€å€‹å¯æ§ä¸”é€¼çœŸçš„å ´æ™¯ç”Ÿæˆæ–¹æ³•&#xA;æå‡ºå…©å€‹ä¸»è¦è¦‹è§£ï¼š&#xA;å°‡3Då ´æ™¯è¡¨ç¤ºç›´æ¥åˆä½µåˆ°ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œå¯ä»¥å¾—åˆ°ä¸€å€‹æ›´å¯æ§çš„åœ–åƒåˆæˆ å°‡3Dè¡¨ç¤ºæ³•èˆ‡ç¥ç¶“æ¸²æŸ“ç®¡é“ç›¸çµåˆï¼Œå¯ä»¥å¯¦ç¾æ›´å¿«çš„æ¨ç†ï¼Œèˆ‡ç”Ÿæˆæ›´é€¼çœŸçš„åœ–åƒ å°‡å ´æ™¯è¦–ç‚ºä¸€é€£ä¸²çš„ç¥ç¶“ç‰¹å¾µå ´çš„çµ„åˆ&#xA;å°‡å ´æ™¯é«”ç©æ¸²æŸ“åˆ°ä¸€å€‹è¼ƒä½ç•«ç´ çš„ç‰¹å¾µåœ–ç‰‡ é€éç¥ç¶“æ¸²æŸ“éç¨‹ä¾†è™•ç†ç‰¹å¾µåœ–ï¼Œæœ€å¾Œè¼¸å‡ºæœ€çµ‚æ¸²æŸ“åœ–&#xA;2. Related work GAN-based Image Synthesis ç›®å‰æƒ…æ³&#xA;å·²è¢«è­‰å¯¦å¯ä»¥æˆåŠŸåœ¨é«˜è§£æåº¦(1024$\times$1024)åœ–ç‰‡ä¸‹é€²è¡Œåˆæˆ å¤šå€‹ç ”ç©¶å¦‚ä½•åœ¨ä¸çµ¦äºˆé¡å¤–ç›£ç£ä¸‹ï¼Œä¹Ÿèƒ½å°ä¸åŒçš„factoré€²è¡Œè§£è€¦åˆ ä¿®æ”¹è¨“ç·´ç›®æ¨™(training objective) æ”¹è®Šç¶²è·¯æ¶æ§‹ ç ”ç©¶æ½›åœ¨ç©ºé–“ é‡åˆ°æŒ‘æˆ°&#xA;å‚³çµ±çš„GANæ–¹æ³•éƒ½æ²’æœ‰æ˜ç¢ºçš„æ¨¡æ“¬å ´æ™¯çš„æ§‹æˆæ€§è³ªï¼Œç›®å‰é–‹å§‹ç ”ç©¶å¦‚ä½•å°object-levelé€²è¡Œæ§åˆ¶ å‰µæ–°æ”¹è®Š</description>
    </item>
    <item>
      <title>BLIP</title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/blip/</link>
      <pubDate>Sat, 06 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/blip/</guid>
      <description>Abstract ç›®å‰çš„VLPæ¨¡å‹æ€§èƒ½æå‡çš„æ–¹å¼ä¸»è¦æ˜¯é æ“´å¤§è³‡æ–™é›†â‡’ ç›®å‰æ˜¯ä»¥ç¶²è·¯ä¸Šçš„image-text pair è¨“ç·´ ç¶²è·¯ä¸Šçš„dataå¾ˆå¤§çš„å¯èƒ½å­˜åœ¨noise æ­¤è«–æ–‡æå‡ºä¸€å€‹æ¨™é¡Œç”¢ç”Ÿå™¨å’Œå»å™ªå™¨ä¾†è™•ç†ç¶²è·¯è³‡æ–™é›†æ‰€å­˜åœ¨çš„å™ªéŸ³å•é¡Œ 1. Introduction ğŸ¯ ç›®æ¨™ï¼š æå‡ºä¸€å€‹æ›´å¼·å¤§çš„VLPæ¶æ§‹&#xA;ğŸ•°ï¸ éå»æ–¹æ³•&#xA;åœ¨éå»çš„VLP(Vision -anguage Pretraining) æ–¹æ³•ä¸­æœ‰å…©å¤§å±¤é¢çš„é™åˆ¶ æ¨¡å‹å±¤é¢ é‡å°ä¸åŒçš„ä¸‹æ¸¸æ‡‰ç”¨ç›®å‰é‚„æ˜¯æœ‰å„è‡ªé©åˆçš„æ¶æ§‹ï¼Œé‚„æ²’æœ‰ä¸€å€‹èƒ½å®Œå…¨çµ±ä¸€å€‹çš„æ¨¡å‹ ç”Ÿæˆä»»å‹™ï¼ˆe.g. æ–‡å­—ç”Ÿæˆï¼‰â‡’ Encoder-Decoder ç†è§£ä»»å‹™ï¼ˆe.g. æª¢ç´¢ï¼‰â‡’ Encoder è³‡æ–™å±¤é¢ éå»æ–¹æ³•çš„è¨“ç·´è³‡æ–™å¤šä¾†è‡ªç¶²è·¯çˆ¬èŸ²æ‰€å¾—ï¼Œå­˜åœ¨noiseè³‡æ–™æœªè¢«æ¸…ç†ä¹¾æ·¨ï¼Œä¸”noiseå¸¶ä¾†çš„è² é¢å½±éŸ¿å°šæœªè¢«é©ç•¶è§£æ±º ğŸ’¡ æœ¬ç¯‡æ–¹æ³•&#xA;é‡å°ä¸Šè¿°æåˆ°çš„å…©å¤§å±¤é¢å•é¡Œé€²è¡Œç ”ç©¶ æ¨¡å‹å±¤é¢ æå‡ºä¸€å€‹å¤šæ¨¡æ…‹æ··åˆï¼ˆMultimodal mixtureï¼‰çš„Encoder-Decoderæ¶æ§‹ (MED) å¯ä»¥åœ¨å¾ŒçºŒæ‡‰ç”¨åœ¨æ›´å¤šçš„ä¸‹æ¸¸ä»»å‹™ä¸­ ä¿æŒé è¨“ç·´æ™‚çš„æ•ˆç‡ è³‡æ–™å±¤é¢ æå‡ºä¸€å€‹å¼•å°ï¼ˆBootstrappingï¼‰æ–¹æ³•ä¾†é¿å…noisy image-text pair Finetuneä¸€å€‹pre-trained MEDæˆå…©å€‹å­æ¨¡çµ„ Cap (Captioner) â‡’ ç”Ÿæˆåˆæˆå­—å¹• Filt (Filter)â‡’ éæ¿¾æ‰noisyå­—å¹• ğŸ”¥ ç ”ç©¶æˆæœ&#xA;å¼•å°å­—å¹•å¯ä»¥æå‡ä¸‹æ¸¸ä»»å‹™çš„æ•ˆèƒ½ï¼Œä¸”å­—å¹•å¤šæ¨£æ€§è¶Šé«˜è¶Šå¥½ BLIPä¸åƒ…åœ¨Vision-language tasksä¸­æœ‰SOTAçš„æ•ˆèƒ½ï¼Œåœ¨è½‰ç§»è‡³Video-language tasksä¸­ä¹Ÿé”åˆ°çš„SOTAä¸”zero-shotçš„æ•ˆèƒ½ 2. Related Work 2.1 Vision-language Pre-training éå»æ–¹æ³•çš„datasetä¾†æºå¤šæ˜¯ç¶²è·¯çˆ¬èŸ²ï¼Œå› æ­¤å­˜åœ¨å™ªéŸ³(noisy)å•é¡Œï¼Œä¸”å™ªéŸ³å•é¡Œè¢«æ¨¡å‹å¸¶ä¾†çš„æ•ˆæœæ©è”½ â‡’ å› æ­¤æå‡ºCapFilt ä¸åŒæ€§è³ªçš„ä»»å‹™èƒŒå¾Œçš„backboneæœƒä¸åŒ â‡’ æå‡ºå¤šæ¨¡æ…‹æ··åˆencoder-decoder understanding-base tasks â‡’ encoder generation-base tasks â‡’ encoder-decoder 2.</description>
    </item>
    <item>
      <title>HistoGAN: Controlling Colors of GAN-Generated and Real Images via Color Histograms</title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/histogan/</link>
      <pubDate>Wed, 06 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/histogan/</guid>
      <description>HistoGAN: Controlling Colors of GAN-Generated and Real Images via Color Histograms æ–¹æ³•ä¸å±€é™æ–¼åŒå€‹domainä¸‹çš„é¡è‰²è½‰æ›ï¼Œè€Œæ˜¯å–®ç´”å°±color histogramä¾†è½‰æ›é¡è‰²ï¼Œè€Œå…§å®¹ä¿ç•™&#xA;Main idea Scheme åœ¨åšåœ–ç‰‡çš„é¢¨æ ¼è½‰æ›æ™‚ï¼Œç‰¹åˆ¥é—œæ³¨æ–¼å°é¡è‰²çš„æ§åˆ¶ï¼ˆå¯ä»¥è¦–ç‚ºStyle transferçš„ä¸€å€‹sub-categoryï¼‰&#xA;Previous work Problems (motivation) åŸºæ–¼ä¸€å¼µé¢¨æ ¼ç›®æ¨™åœ–åšè½‰æ› å¯èƒ½æœƒå½±éŸ¿åˆ°è¢«è½‰æ›çš„å…§å®¹ç´°ç¯€ï¼ˆç´‹ç†ã€è‰²èª¿ï¼‰ è½‰æ›çš„å“è³ªå¥½å£éå¸¸ä¾è³´inputå’Œtargetåœ–ç‰‡é–“èªæ„ç›¸ä¼¼æ€§ï¼ˆæ˜¯å¦åœ¨åŒä¸€å€‹domainï¼‰ æƒ³è¦æœ‰å¥½çš„é¢¨æ ¼è½‰æ›æ•ˆæœå°±å¿…é ˆåœ¨åŒä¸€å€‹domainä¸‹ Method åªè—‰ç”±color histogramä¾†å”åŠ©deep networkçš„æ•ˆæœ ç”±æ­¤å°±å¯ä»¥å¾ä»»æ„çš„domainä¸­æå–è‰²å½© Result Abstract HistoGAN based on StyleGAN architecture control the GAN-generated images specified by a target color histogram feature ReHistoGAN expend HistoGAN to recolor the generated image encoder unsupervised keep the original images content while changing the colors based on the given target histogram 1.</description>
    </item>
    <item>
      <title></title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/anomaly-detection-neural-network-with-dual-auto-encoders-gan-and-its-industrial-inspection-applications/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/anomaly-detection-neural-network-with-dual-auto-encoders-gan-and-its-industrial-inspection-applications/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/deep-image-compositing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/deep-image-compositing/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/fuseformer-fusing-fine-grained-information-in-transformers-for-video-inpainting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/fuseformer-fusing-fine-grained-information-in-transformers-for-video-inpainting/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/selective-unsupervised-learning-based-wi-fi-fingerprint-system-using-autoencoder-and-gan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/selective-unsupervised-learning-based-wi-fi-fingerprint-system-using-autoencoder-and-gan/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/self-attention-temporal-convolutional-network-for-long-term-daily-living-activity-detection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/self-attention-temporal-convolutional-network-for-long-term-daily-living-activity-detection/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/step/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/step/</guid>
      <description>Style-based Encoder Pre-training for Multi-modal Image Synthesis Main idea Scheme å°åœ–ç‰‡ä½œä¸€å°å¤šçš„æ¨£æ…‹è½‰æ›&#xA;Problems (motivation) ç‚ºäº†è®“æ¨¡å‹èƒ½å°‡è¼¸å…¥è½‰æˆä¸åŒæ¨£æ…‹ï¼Œå¯èƒ½æ¡ç”¨è¤‡é›œçš„æ¨¡å‹ä¾†è¨˜ä½è¼¸å…¥èˆ‡ä¸åŒæ¨£æ…‹é–“çš„é—œè¯æ€§ ï¼ˆæš´åŠ›æ³•ï¼‰ Mode collapseå•é¡Œ é‡åˆ°æ²’çœ‹éçš„é¢¨æ ¼å¯èƒ½è½‰æ›çš„æ•ˆæœå°±ä¸å¦‚é æœŸï¼ˆæ¨¡å‹è½‰æ›ä¸åŒæ¨£æ…‹çš„æˆæœå¥½å£å–æ±ºæ–¼è¨“ç·´æ™‚çš„è¨“ç·´è³‡æ–™é›†ï¼‰ Relative work VAE BicycleGAN MUNIT-p Method é è¨“ç·´ä¸€å€‹ä¸éŒ¯çš„é¢¨æ ¼encoderä¾†å°‡è¼¸å…¥çš„é¢¨æ ¼encodeæˆlatent code èª¿æ•´ä¸¦å°‹æ‰¾é©åˆçš„loss function (loss termsè¶Šå°‘è¶Šå¥½) Result åˆæˆçš„çµæœæœ€å¥½ æ¨¡å‹æˆæ•ˆä¸ä¾è³´ç›®æ¨™è¨“ç·´è³‡æ–™é›†ï¼ˆæ³›ç”¨åº¦æ›´å»£ï¼‰ æ›´æœ‰æ•ˆæ›´æœ‰èƒ½åŠ›è¡¨é”é¢¨æ ¼ç‰¹å¾µï¼ˆlatent codeï¼‰ï¼Œå°é¢¨æ ¼çš„ä¿çœŸåº¦æé«˜ ç°¡åŒ–è¨“ç·´ç›®æ¨™ä¸¦åŠ å¿«è¨“ç·´é€Ÿåº¦ Abstract éå»I2Ié‡åˆ°çš„å•é¡Œ è¼¸å…¥è¼¸å‡ºçš„ä¸€å°å¤šå°æ‡‰é—œä¿‚ Mode collapses problem éå»è§£æ±ºæ–¹æ³• é‡å°å¤šç¨®æ¨£æ…‹çš„è¼¸å‡ºä½¿ç”¨è¤‡é›œçš„æ¨¡å‹é€²è¡Œè¨“ç·´ï¼Œä¾†å› æ‡‰å¤šæ¨£çš„è¼¸å‡ºç¯„ç–‡ çœ‹è‘—çµæœä¾†è¨“ç·´ æš´åŠ›æ³• æœ¬è«–æ–‡çš„è§£æ±ºæ–¹æ³• å¼·åŒ–åœ–åƒEncoderçš„èƒ½åŠ›ï¼ˆå°é¢¨æ ¼ä½œæ›´é€²éšçš„åˆ†æï¼‰ä¾†å­¸ç¿’æ›´æ½›åœ¨çš„ç©ºé–“ç‰¹å¾µ å°è¼¸å…¥æœ¬è³ªè¨“ç·´ æœ¬è«–æ–‡çš„æ–¹æ³•æ¦‚å¿µ å°‡ I2I è½‰æ›çš„è¡Œç‚ºåˆ†æˆå…©å€‹å·¥ä½œ pre-trained generic style encoder ( proxy task ) å­¸ç¿’åœ–ç‰‡çš„åµŒå…¥è³‡è¨Š ä»»æ„domainçš„image â†’ ä½ç¶­åº¦çš„é¢¨æ ¼ç©ºé–“è³‡è¨Š åœ–ç‰‡åˆæˆ æœ¬è«–æ–‡çš„å„ªå‹¢ æ¨¡å‹ä¸ä¾è³´ç›®æ¨™è¨“ç·´è³‡æ–™é›†ï¼ˆæ³›ç”¨åº¦æ›´å»£ï¼‰ æ›´æœ‰æ•ˆæ›´æœ‰èƒ½åŠ›è¡¨é”ç©ºé–“ç‰¹å¾µï¼Œå°é¢¨æ ¼çš„ä¿çœŸåº¦æé«˜ ç°¡åŒ–è¨“ç·´ç›®æ¨™ä¸¦åŠ å¿«è¨“ç·´é€Ÿåº¦ ä¸åŒloss termå°multi-modal I2Içš„å½±éŸ¿ æå‡ºVAEsçš„æ›¿ä»£æ–¹æ¡ˆä¾†å°æœªå—é™åˆ¶çš„ç©ºé–“ç‰¹å¾µé€²è¡Œæ¡æ¨£ è·Ÿå…­å€‹å°ç…§çµ„ç›¸æ¯”çµæœæœ€å¥½ 1.</description>
    </item>
    <item>
      <title></title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/sttn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/sttn/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/stylegan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/stylegan/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/stylegan3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/stylegan3/</guid>
      <description>Alias-Free Generative Adversarial Networks Abstract å•é¡Œç™¼ç¾ åœ–ç‰‡åˆæˆçš„éç¨‹éåº¦ä¾è³´åƒç´ åº§æ¨™ï¼Œå°è‡´åœ–ç‰‡ç´°ç¯€å¯èƒ½é»åœ¨åº§æ¨™ä¸Šè€Œéæç¹ªå°è±¡çš„è¡¨é¢&#xA;è§£æ±ºæ¦‚å¿µ&#xA;å°‡ç¶²è·¯ä¸­çš„ä¿¡æ¯éƒ½æ˜¯ç‚ºé€£çºŒçš„&#xA;è§£æ±ºæ–¹æ³•&#xA;å°å¹…åº¦ä¿®æ”¹åŸæœ¬æ¨¡å‹æ¶æ§‹ï¼Œé”åˆ°æ™®éé©ç”¨æ€§é«˜ä¸”èƒ½é¿å…ä¸éœ€è¦çŸ¥è³‡è¨Šæ´©æ¼è‡³åˆ†å±¤èåˆçš„éç¨‹&#xA;çµæœæˆæ•ˆ&#xA;å°å‹•æ…‹å½±åƒçš„åˆæˆæœ‰å¾ˆå¤§çš„å¹«åŠ©&#xA;1 Introduction ç›®å‰æœ€å¤§å•é¡Œ åœ¨ç¾å¯¦ä¸­ä¸åŒå°ºåº¦çš„ç´°ç¯€å¾€å¾€æ˜¯åˆ†å±¤è½‰æ›çš„ï¼Œè€ŒGANä¸¦æ²’æœ‰åšåˆ°è‡ªç„¶çš„åˆ†å±¤ ç´°ç¯€ç‰¹å¾µçš„ä½ç½®æ‡‰è©²ç¹¼æ‰¿è¼ƒä¸Šå±¤ç²—ç•¥çš„ç‰¹å¾µï¼Œå½¼æ­¤æ‡‰è©²æœ‰ç¹¼æ‰¿é—œä¿‚&#xA;çŒœæ¸¬èˆ‡ç™¼ç¾ ç›®å‰çš„ç¶²è·¯æœƒåƒè€ƒéš±è—å±¤ä¸­æ´©æ¼å‡ºçš„ä½ç½®è¨Šæ¯ä¾†ç•«ä¸Šç´°ç¯€ï¼Œä½†å¦‚æ­¤çš„åˆ†å±¤çµæ§‹ä¸¦ä¸è‡ªç„¶ image border&#xA;åœ¨é€²è¡Œå·ç©é‹ç®—æ™‚å¸¸å¸¸æœƒä½¿ç”¨åˆ°zero-paddingï¼Œè€Œæ­¤èˆ‰å‹•æœƒä¸ç¶“æ„çš„æ´©æ¼å‡ºçµ•å°ä½ç½®çš„è¨Šæ¯&#xA;åƒè€ƒè³‡è¨Š HOW MUCH POSITION INFORMATION DO CONVOLUTIONAL NEURAL NETWORKS ENCODE?&#xA;çŒœæ¸¬åœ¨CNNå„å±¤å·ç©ä¸­éš±å«è‘—çµ•å°ä½ç½®çš„è³‡è¨Š zero-paddingé€ æˆæ¨¡å‹åœ¨ç„¡æ„é–“å­¸ç¿’çš„ç‰¹å¾µçš„çµ•å°ä½ç½® per-pixel noise inputs&#xA;positional encoding&#xA;aliasing&#xA;å¯èƒ½é€ æˆæ··ç–Šå•é¡Œç™¼ç”Ÿçš„åŸå›  ä½¿ç”¨éç†æƒ³æ¡æ¨£æ¿¾æ³¢å™¨é€ æˆçš„ï¼Œåƒç´ ç¶²æ ¼å¼±åŒ–ï¼ˆpixelä½ç½®è³‡è¨Šæ··æ·†ï¼Ÿï¼‰ Ex: nearest, bilinear, stridedå·ç© é‡å°å„å€‹pixelä½¿ç”¨éç·šæ€§è½‰æ› Ex: ReLU, swish å› ç‚ºä»¥ä¸Šå…©ç¨®å¯èƒ½å°è‡´ç¶²è·¯æœƒä¸è‡ªè¦ºå¾—å°‡ä¸åŒå°ºåº¦çš„è³‡è¨Šéƒ½ç•«åœ¨basisä¸Šï¼Œè€Œæ­¤basiså°±æ˜¯è¢å¹•ä¸­çœ‹åˆ°çš„å¹³é¢åº§æ¨™ è§£æ±ºæ–¹æ³• å»é™¤æ‰€æœ‰å¯èƒ½çš„çµ•å°ä½ç½®åƒè€ƒä¾†æº ä½¿å¾—å„å±¤å·ç©çš„ç”Ÿæˆèƒ½å¤ æ›´å¹³ç­‰ï¼Œè€Œéå—é™æ–¼æ´©æ¼çš„çµ•å°ä½ç½®è³‡è¨Š ç›®å‰çš„ä¸Šæ¡æ¨£filterä¸¦æ²’æœ‰å¾ˆå¥½çš„æŠ‘åˆ¶æ··ç–Š ä½¿ç”¨low-pass filterä¾†å°åŸæœ¬çš„resultå†é€²ä¸€æ­¥è™•ç†ä¾†ä¿®æ­£pointwiseéç·šæ€§é€ æˆçš„æ··ç–Šå•é¡Œ å…¨é¢æª¢ä¿®StyleGAN2çš„ä¿¡æ¯è™•ç† æå‡ºäº†ä¸€å€‹æ›´æ ¹æœ¬ä¸åŒçš„åœ–ç‰‡ç”Ÿæˆæ¦‚å¿µ Equivariance vs. Inquivariance Equivariance ç­‰è®Šæ€§ å°æ–¼è¼¸å…¥æ–½åŠ ä¸€äº›æ”¹è®Šæœƒåæ˜ åœ¨è¼¸å‡ºä¸Š $f(g(x))=g(f(x))$ $f(\cdot):$ ç‰¹å¾µå‡½æ•¸ $g(\cdot):$ è®Šæ›å‡½æ•¸ Inquivariance ä¸è®Šæ€§ å°è¼¸å…¥æ–½åŠ ä¸€äº›æ”¹è®Šä¸æœƒå½±éŸ¿è¼¸å‡º CNNä¸­çš„equivariant vs.</description>
    </item>
    <item>
      <title></title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/temporal_group_fusion_network_dvi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/temporal_group_fusion_network_dvi/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>https://muxi1998.github.io/Paper-Blog/paper_notes/zits/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://muxi1998.github.io/Paper-Blog/paper_notes/zits/</guid>
      <description>Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding Abstract 1. Introduction Inpaintingçš„é‡é» coherent texture visually reasonable structures éå»çš„Inpaintingæ–¹æ³•æœ‰4å¤§å•é¡Œ æ„Ÿå—é‡éå°ï¼šCNN kernelçš„é—œä¿‚ï¼Œå°±ç®—æ˜¯dilated CNNä¹Ÿæœƒåœ¨åœ¨éå¤§çš„æ¯€æå€åŸŸæˆ–é«˜è§£æåº¦çš„æƒ…å¢ƒä¸‹é™ä½æ•ˆèƒ½ éºå¤±æ•´é«”çš„çµæ§‹ï¼šè‹¥æ¨¡å‹ä¸äº†è§£æ•´é«”çš„çµæ§‹ï¼Œå‰‡å¾ˆé›£é‚„åŸç´°ç¯€ è¨ˆç®—é‡é‡ï¼šè¨“ç·´GANå°å¤§å½±åƒçš„é‚„åŸæ˜¯å›°é›£ä¸”è€—è²»è³‡æºçš„ï¼Œä¸”æ•ˆèƒ½æœƒéš¨è‘—å½±åƒè§£æåº¦æé«˜è€Œé™ä½ é®ç½©å€åŸŸæ²’æœ‰ä½ç½®è³‡è¨Šï¼šæ¨¡å‹å®¹æ˜“åœ¨éºå¤±å€åŸŸç”¢ç”Ÿå½å½± 2. Related work 3. Method ç°¡å¯«èªªæ˜ ZITS : ZeroRA based Incremental Transformer Structure MPE : Masking Positional Encoding TSR : Transformer Structure Restorer FTR : Fourier CNN Texture Restoration Overview Structure restoration è¼¸å…¥ç‚º$TSR(I_m, I_e, I_l, M)$ $I_m:$ è¢«é®ç½©çš„è¼¸å…¥å½±åƒ $I_e:$ é‚Šç·£è³‡è¨Š $I_l:$ ç·šæ¢è³‡è¨Š $M:$ äºŒå…ƒmask è¼¸å‡ºç‚ºè‰åœ–åŸŸ $[\tilde{I_e}, \tilde{I_l}]=TSR(I_m, I_e, I_l, M)$ Simple Structure upsample åœ¨æ¨è«–æœŸé–“æœƒä½¿ç”¨SSUä¾†å°‡ç°éšçš„è‰åœ–ä¸Šæ¡æ¨£è‡³ä»»æ„å¤§å° Extract structure feature åœ¨ä½¿ç”¨ä¸€å€‹gated convolution based SFEä¾†æå–å¤šå°ºåº¦çš„ç‰¹å¾µ $S_k=SFE(\tilde{I_e}, \tilde{I_l},M), {k=0,1,2,3}$ Add to to the fourier CNN texture restoration å°‡$S_k$è³‡è¨Šé€æ­¥åŠ å…¥(ZeroRA)ç›¸å°æ‡‰çš„FTR layerä¸­ 3.</description>
    </item>
  </channel>
</rss>
